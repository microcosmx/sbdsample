2016-03-02 15:19:48,173 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-02 15:19:56,017 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-02 15:19:56,038 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-02 15:19:56,053 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-02 15:19:59,376 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-02 15:19:59,617 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-02 15:20:00,695 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.3.139:53962]
2016-03-02 15:20:00,720 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 53962.
2016-03-02 15:20:00,887 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-02 15:20:01,078 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-02 15:20:01,230 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa\blockmgr-a96403b4-a9c7-4fd8-b9f0-8e6f2c2443b1
2016-03-02 15:20:01,272 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-02 15:20:01,435 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa\httpd-6104e565-799f-4d37-b803-e78d7e6755fb
2016-03-02 15:20:01,513 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-02 15:20:02,036 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-02 15:20:02,073 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:53963
2016-03-02 15:20:02,089 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 53963.
2016-03-02 15:20:02,188 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-02 15:20:02,611 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-02 15:20:02,628 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-02 15:20:02,629 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-02 15:20:02,633 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://192.168.3.139:4040
2016-03-02 15:20:02,897 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://192.168.3.139:53963/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456903202896
2016-03-02 15:20:03,406 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:20:03,408 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:20:03,409 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:20:03,543 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-02 15:20:05,373 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53982.
2016-03-02 15:20:05,374 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 53982
2016-03-02 15:20:05,422 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-02 15:20:05,435 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:53982 with 265.4 MB RAM, BlockManagerId(driver, localhost, 53982)
2016-03-02 15:20:05,472 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-02 15:20:09,183 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-02 15:20:10,650 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-02 15:20:10,690 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-02 15:20:11,290 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-02 15:20:11,297 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-02 15:20:11,639 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:20:12,564 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:20:24,465 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-02 15:20:24,675 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-02 15:20:26,553 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:26,555 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:34,432 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:34,434 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:36,405 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-02 15:20:37,087 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-02 15:20:38,508 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-02 15:20:38,512 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-02 15:20:38,955 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-02 15:20:39,171 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-02 15:20:39,378 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-02 15:20:40,656 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-02 15:20:40,715 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-02 15:20:40,749 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-02 15:20:40,965 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-02 15:20:40,966 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-02 15:20:41,073 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:20:41,305 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:20:49,676 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-02 15:20:49,736 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-02 15:20:51,060 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:51,067 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:56,759 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:56,760 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:20:58,606 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-02 15:20:58,903 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-02 15:20:59,601 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-02 15:20:59,604 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-02 15:20:59,945 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-02 15:21:00,085 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-02 15:21:01,155 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:53
2016-03-02 15:21:01,324 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:53) with 10 output partitions (allowLocal=false)
2016-03-02 15:21:01,325 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:53)
2016-03-02 15:21:01,326 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:21:01,331 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:21:01,436 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-02 15:21:02,138 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-02 15:21:02,168 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-02 15:21:02,267 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-02 15:21:02,268 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-02 15:21:02,292 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:53982 (size: 804.0 B, free: 265.4 MB)
2016-03-02 15:21:02,296 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-02 15:21:02,383 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-02 15:21:02,385 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-02 15:21:02,415 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-02 15:21:02,535 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:21:02,540 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:21:02,541 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:21:02,542 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:21:02,600 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-02 15:21:02,600 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-02 15:21:02,600 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-02 15:21:02,600 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-02 15:21:02,650 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://192.168.3.139:53963/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456903202896
2016-03-02 15:21:03,048 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://192.168.3.139:53963/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa\userFiles-8ef1a71b-c574-4bf2-be8c-5b93a7059e0d\fetchFileTemp2809627281275215094.tmp
2016-03-02 15:21:11,918 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa/userFiles-8ef1a71b-c574-4bf2-be8c-5b93a7059e0d/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-02 15:21:12,208 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-02 15:21:12,208 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-02 15:21:12,208 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-02 15:21:12,208 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-02 15:21:12,211 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:21:12,217 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-02 15:21:12,219 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:21:12,221 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-02 15:21:12,223 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:21:12,224 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-02 15:21:12,226 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:21:12,227 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-02 15:21:12,254 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-02 15:21:12,257 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-02 15:21:12,290 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-02 15:21:12,290 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 9716 ms on localhost (1/10)
2016-03-02 15:21:12,290 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-02 15:21:12,293 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 9753 ms on localhost (2/10)
2016-03-02 15:21:12,296 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 9842 ms on localhost (3/10)
2016-03-02 15:21:12,297 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:21:12,298 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-02 15:21:12,298 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 88 ms on localhost (4/10)
2016-03-02 15:21:12,300 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 9759 ms on localhost (5/10)
2016-03-02 15:21:12,302 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:21:12,304 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-02 15:21:12,326 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-02 15:21:12,368 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on localhost (6/10)
2016-03-02 15:21:12,369 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 72 ms on localhost (7/10)
2016-03-02 15:21:12,370 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 148 ms on localhost (8/10)
2016-03-02 15:21:12,372 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 153 ms on localhost (9/10)
2016-03-02 15:21:12,412 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-02 15:21:12,414 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 111 ms on localhost (10/10)
2016-03-02 15:21:12,448 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-02 15:21:12,449 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:53) finished in 9.998 s
2016-03-02 15:21:12,458 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:53, took 11.302284 s
2016-03-02 15:21:12,663 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-02 15:21:12,664 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-02 15:21:12,883 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-02 15:21:12,884 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-02 15:21:12,885 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:53982 (size: 19.3 KB, free: 265.4 MB)
2016-03-02 15:21:12,887 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:55
2016-03-02 15:21:13,096 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-02 15:21:13,168 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:56
2016-03-02 15:21:13,170 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:56) with 2 output partitions (allowLocal=false)
2016-03-02 15:21:13,170 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:56)
2016-03-02 15:21:13,171 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:21:13,173 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:21:13,174 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:55), which has no missing parents
2016-03-02 15:21:13,201 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-02 15:21:13,202 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-02 15:21:13,206 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-02 15:21:13,207 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-02 15:21:13,210 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:53982 (size: 1755.0 B, free: 265.4 MB)
2016-03-02 15:21:13,212 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-02 15:21:13,228 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:55)
2016-03-02 15:21:13,228 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-02 15:21:13,230 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-02 15:21:13,233 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:21:13,234 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:21:13,235 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-02 15:21:13,235 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-02 15:21:13,280 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-02 15:21:13,280 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:21:13,368 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-02 15:21:13,368 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-02 15:21:13,370 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-02 15:21:13,369 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-02 15:21:13,372 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-02 15:21:13,517 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-02 15:21:13,518 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-02 15:21:13,520 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 287 ms on localhost (1/2)
2016-03-02 15:21:13,523 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 292 ms on localhost (2/2)
2016-03-02 15:21:13,523 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:56) finished in 0.292 s
2016-03-02 15:21:13,526 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-02 15:21:13,527 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:56, took 0.358270 s
2016-03-02 15:21:13,564 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:57
2016-03-02 15:21:13,565 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:57) with 1 output partitions (allowLocal=true)
2016-03-02 15:21:13,566 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:57)
2016-03-02 15:21:13,567 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:21:13,569 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:21:13,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:55), which has no missing parents
2016-03-02 15:21:13,572 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=257005, maxMem=278302556
2016-03-02 15:21:13,573 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-02 15:21:13,575 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=260141, maxMem=278302556
2016-03-02 15:21:13,577 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-02 15:21:13,578 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:53982 (size: 1815.0 B, free: 265.4 MB)
2016-03-02 15:21:13,579 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-02 15:21:13,581 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:55)
2016-03-02 15:21:13,581 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-02 15:21:13,582 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-02 15:21:13,583 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:21:13,584 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-02 15:21:13,589 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:21:13,593 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1821 bytes result sent to driver
2016-03-02 15:21:13,595 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:57) finished in 0.013 s
2016-03-02 15:21:13,595 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 13 ms on localhost (1/1)
2016-03-02 15:21:13,598 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-02 15:21:13,598 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:57, took 0.032861 s
2016-03-02 15:21:13,697 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:59
2016-03-02 15:21:13,704 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:58)
2016-03-02 15:21:13,705 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:59) with 10 output partitions (allowLocal=false)
2016-03-02 15:21:13,706 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:59)
2016-03-02 15:21:13,706 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-02 15:21:13,708 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-02 15:21:13,712 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at map at Main.scala:58), which has no missing parents
2016-03-02 15:21:13,717 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=261956, maxMem=278302556
2016-03-02 15:21:13,718 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-02 15:21:13,721 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=265908, maxMem=278302556
2016-03-02 15:21:13,722 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-02 15:21:13,725 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:53982 (size: 2.2 KB, free: 265.4 MB)
2016-03-02 15:21:13,727 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-02 15:21:13,738 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at map at Main.scala:58)
2016-03-02 15:21:13,738 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-02 15:21:13,740 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-02 15:21:13,744 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1522 bytes)
2016-03-02 15:21:13,745 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1522 bytes)
2016-03-02 15:21:13,746 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-02 15:21:13,746 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-02 15:21:13,752 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:21:13,752 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-02 15:21:13,994 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-02 15:21:13,994 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-02 15:21:14,000 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 256 ms on localhost (1/2)
2016-03-02 15:21:14,002 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 259 ms on localhost (2/2)
2016-03-02 15:21:14,007 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-02 15:21:14,005 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:58) finished in 0.264 s
2016-03-02 15:21:14,009 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-02 15:21:14,013 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-02 15:21:14,014 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-02 15:21:14,015 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-02 15:21:14,020 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-02 15:21:14,024 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[5] at reduceByKey at Main.scala:58), which is now runnable
2016-03-02 15:21:14,026 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=268181, maxMem=278302556
2016-03-02 15:21:14,027 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-02 15:21:14,030 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=270421, maxMem=278302556
2016-03-02 15:21:14,031 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-02 15:21:14,034 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:53982 (size: 1361.0 B, free: 265.4 MB)
2016-03-02 15:21:14,037 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-02 15:21:14,039 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[5] at reduceByKey at Main.scala:58)
2016-03-02 15:21:14,040 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-02 15:21:14,041 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-02 15:21:14,044 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,045 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,046 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,048 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,049 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-02 15:21:14,049 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-02 15:21:14,049 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-02 15:21:14,049 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-02 15:21:14,072 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,076 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,072 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,080 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 13 ms
2016-03-02 15:21:14,072 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,079 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 12 ms
2016-03-02 15:21:14,078 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 11 ms
2016-03-02 15:21:14,082 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 15 ms
2016-03-02 15:21:14,110 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-02 15:21:14,110 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1280 bytes result sent to driver
2016-03-02 15:21:14,110 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1293 bytes result sent to driver
2016-03-02 15:21:14,110 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-02 15:21:14,112 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,114 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-02 15:21:14,115 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,116 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-02 15:21:14,117 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,118 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-02 15:21:14,121 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,120 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,119 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,123 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-02 15:21:14,123 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-02 15:21:14,127 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,125 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 83 ms on localhost (1/10)
2016-03-02 15:21:14,125 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-02 15:21:14,130 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 82 ms on localhost (2/10)
2016-03-02 15:21:14,128 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-02 15:21:14,130 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-02 15:21:14,132 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 86 ms on localhost (3/10)
2016-03-02 15:21:14,134 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-02 15:21:14,134 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,137 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 92 ms on localhost (4/10)
2016-03-02 15:21:14,138 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1275 bytes result sent to driver
2016-03-02 15:21:14,137 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-02 15:21:14,139 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:21:14,142 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-02 15:21:14,143 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 29 ms on localhost (5/10)
2016-03-02 15:21:14,146 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,147 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-02 15:21:14,149 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 37 ms on localhost (6/10)
2016-03-02 15:21:14,148 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,151 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 34 ms on localhost (7/10)
2016-03-02 15:21:14,150 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:21:14,154 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-02 15:21:14,152 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-02 15:21:14,156 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-02 15:21:14,160 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 26 ms on localhost (8/10)
2016-03-02 15:21:14,162 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-02 15:21:14,162 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1334 bytes result sent to driver
2016-03-02 15:21:14,164 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 46 ms on localhost (9/10)
2016-03-02 15:21:14,166 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:59) finished in 0.124 s
2016-03-02 15:21:14,167 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 27 ms on localhost (10/10)
2016-03-02 15:21:14,169 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-02 15:21:14,168 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:59, took 0.469469 s
2016-03-02 15:21:27,097 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-02 15:21:27,177 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-02 15:21:27,178 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-02 15:21:27,179 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-02 15:21:27,180 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-02 15:21:27,181 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-02 15:21:27,182 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-02 15:21:27,184 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-02 15:21:27,185 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-02 15:21:27,185 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-02 15:21:27,187 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-02 15:21:27,188 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-02 15:21:27,189 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-02 15:21:27,190 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-02 15:21:27,191 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-02 15:21:27,192 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-02 15:21:27,193 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-02 15:21:27,195 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-02 15:21:27,195 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-02 15:21:27,198 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-02 15:21:27,201 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-02 15:21:27,201 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-02 15:21:27,202 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-02 15:21:27,202 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-02 15:21:27,202 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-02 15:21:27,203 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-02 15:21:27,258 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://192.168.3.139:4040
2016-03-02 15:21:27,292 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-02 15:21:27,405 INFO  [sparkDriver-akka.actor.default-dispatcher-13][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-02 15:21:27,488 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa\blockmgr-a96403b4-a9c7-4fd8-b9f0-8e6f2c2443b1, already present as root for deletion.
2016-03-02 15:21:27,489 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-02 15:21:27,490 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-02 15:21:27,508 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-02 15:21:27,511 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-02 15:21:27,513 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-02 15:21:27,516 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-02 15:21:27,518 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-02 15:21:27,534 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-02 15:21:27,544 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c4db7bd0-0a78-42ff-b9d9-bc5c52e158aa
2016-03-02 15:21:27,593 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-12738156-1436-4416-8bf9-6a2dd2298ee1
2016-03-02 15:21:27,641 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-02 15:21:27,871 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-12738156-1436-4416-8bf9-6a2dd2298ee1
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-12738156-1436-4416-8bf9-6a2dd2298ee1
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-02 15:25:32,648 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-02 15:25:34,915 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-02 15:25:34,917 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-02 15:25:34,919 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-02 15:25:35,815 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-02 15:25:35,871 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-02 15:25:36,074 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.3.139:63488]
2016-03-02 15:25:36,081 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 63488.
2016-03-02 15:25:36,105 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-02 15:25:36,124 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-02 15:25:36,160 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7a3e5f12-1cca-49de-8c15-496ad132b034\blockmgr-1d2e31c4-d4a3-4cf2-9c62-5a2ce4067542
2016-03-02 15:25:36,169 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-02 15:25:36,267 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7a3e5f12-1cca-49de-8c15-496ad132b034\httpd-63dd903f-f7bd-4070-a02a-586857d151f1
2016-03-02 15:25:36,274 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-02 15:25:36,352 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-02 15:25:36,377 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:63489
2016-03-02 15:25:36,379 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 63489.
2016-03-02 15:25:36,403 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-02 15:25:36,584 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-02 15:25:36,644 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-02 15:25:36,645 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-02 15:25:36,648 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://192.168.3.139:4040
2016-03-02 15:25:36,835 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://192.168.3.139:63489/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456903536834
2016-03-02 15:25:36,908 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:25:36,909 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:25:36,910 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-02 15:25:36,930 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-02 15:25:38,394 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63508.
2016-03-02 15:25:38,396 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 63508
2016-03-02 15:25:38,397 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-02 15:25:38,402 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:63508 with 265.4 MB RAM, BlockManagerId(driver, localhost, 63508)
2016-03-02 15:25:38,407 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-02 15:25:39,440 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-02 15:25:39,908 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-02 15:25:39,936 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-02 15:25:40,154 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-02 15:25:40,156 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-02 15:25:40,301 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:25:40,641 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:25:48,973 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-02 15:25:49,052 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-02 15:25:50,388 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:25:50,390 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:25:57,026 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:25:57,027 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:25:59,123 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-02 15:25:59,523 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-02 15:26:00,223 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-02 15:26:00,228 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-02 15:26:00,665 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-02 15:26:00,794 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-02 15:26:00,881 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-02 15:26:02,389 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-02 15:26:02,430 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-02 15:26:02,458 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-02 15:26:02,672 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-02 15:26:02,673 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-02 15:26:02,791 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:26:03,027 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-02 15:26:08,437 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-02 15:26:08,479 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-02 15:26:10,450 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:26:10,451 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:26:10,879 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:26:10,880 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-02 15:26:11,018 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-02 15:26:11,020 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-02 15:26:11,380 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-02 15:26:11,383 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-02 15:26:11,478 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-02 15:26:11,693 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-02 15:26:12,292 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-02 15:26:12,316 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-02 15:26:12,318 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-02 15:26:12,320 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:26:12,327 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:26:12,339 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-02 15:26:12,503 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-02 15:26:12,508 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-02 15:26:12,526 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-02 15:26:12,528 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-02 15:26:12,548 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:63508 (size: 804.0 B, free: 265.4 MB)
2016-03-02 15:26:12,552 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-02 15:26:12,561 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-02 15:26:12,564 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-02 15:26:12,577 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-02 15:26:12,618 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:26:12,622 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:26:12,624 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:26:12,626 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:26:12,632 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-02 15:26:12,632 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-02 15:26:12,632 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-02 15:26:12,632 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-02 15:26:12,639 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://192.168.3.139:63489/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456903536834
2016-03-02 15:26:12,743 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://192.168.3.139:63489/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7a3e5f12-1cca-49de-8c15-496ad132b034\userFiles-4ad31a35-50e8-48cb-9bb8-7c6a01992a0f\fetchFileTemp7161409871705567261.tmp
2016-03-02 15:26:17,066 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-7a3e5f12-1cca-49de-8c15-496ad132b034/userFiles-4ad31a35-50e8-48cb-9bb8-7c6a01992a0f/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-02 15:26:34,987 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-02 15:26:34,991 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-02 15:26:34,994 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:26:34,996 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-02 15:26:34,998 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:26:35,005 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-02 15:26:35,001 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-02 15:26:35,001 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-02 15:26:35,008 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 22384 ms on localhost (1/10)
2016-03-02 15:26:35,010 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 22387 ms on localhost (2/10)
2016-03-02 15:26:35,006 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-02 15:26:35,017 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-02 15:26:35,020 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:26:35,022 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-02 15:26:35,033 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-02 15:26:35,029 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:26:35,036 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1386 bytes)
2016-03-02 15:26:35,037 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-02 15:26:35,037 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-02 15:26:35,041 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-02 15:26:35,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1390 bytes)
2016-03-02 15:26:35,043 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-02 15:26:35,046 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-02 15:26:35,049 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-02 15:26:35,045 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 22450 ms on localhost (3/10)
2016-03-02 15:26:35,051 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 22427 ms on localhost (4/10)
2016-03-02 15:26:35,052 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 58 ms on localhost (5/10)
2016-03-02 15:26:35,058 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 39 ms on localhost (6/10)
2016-03-02 15:26:35,060 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 22 ms on localhost (7/10)
2016-03-02 15:26:35,066 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 30 ms on localhost (8/10)
2016-03-02 15:26:35,067 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 70 ms on localhost (9/10)
2016-03-02 15:26:35,070 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 22.490 s
2016-03-02 15:26:35,081 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 45 ms on localhost (10/10)
2016-03-02 15:26:35,083 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-02 15:26:35,084 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 22.790957 s
2016-03-02 15:26:35,126 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-02 15:26:35,127 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-02 15:26:35,165 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-02 15:26:35,166 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-02 15:26:35,168 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:63508 (size: 19.3 KB, free: 265.4 MB)
2016-03-02 15:26:35,171 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:56
2016-03-02 15:26:35,227 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-02 15:26:35,238 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-02 15:26:35,239 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-02 15:26:35,240 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-02 15:26:35,241 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:26:35,244 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:26:35,245 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-02 15:26:35,252 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-02 15:26:35,253 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-02 15:26:35,256 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-02 15:26:35,257 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-02 15:26:35,260 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:63508 (size: 1755.0 B, free: 265.4 MB)
2016-03-02 15:26:35,261 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-02 15:26:35,264 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-02 15:26:35,265 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-02 15:26:35,266 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-02 15:26:35,268 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:26:35,269 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:26:35,271 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-02 15:26:35,271 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-02 15:26:35,282 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:26:35,282 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-02 15:26:35,293 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-02 15:26:35,294 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-02 15:26:35,293 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-02 15:26:35,296 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-02 15:26:35,297 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-02 15:26:35,323 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-02 15:26:35,323 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-02 15:26:35,328 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 62 ms on localhost (1/2)
2016-03-02 15:26:35,329 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 60 ms on localhost (2/2)
2016-03-02 15:26:35,330 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-02 15:26:35,330 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.064 s
2016-03-02 15:26:35,332 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.093918 s
2016-03-02 15:26:35,354 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:59
2016-03-02 15:26:35,355 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:59) with 1 output partitions (allowLocal=true)
2016-03-02 15:26:35,356 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:59)
2016-03-02 15:26:35,357 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-02 15:26:35,360 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-02 15:26:35,360 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-02 15:26:35,363 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=257005, maxMem=278302556
2016-03-02 15:26:35,364 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-02 15:26:35,435 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=260141, maxMem=278302556
2016-03-02 15:26:35,437 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-02 15:26:35,440 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:63508 (size: 1815.0 B, free: 265.4 MB)
2016-03-02 15:26:35,441 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-02 15:26:35,442 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-02 15:26:35,443 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-02 15:26:35,445 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-02 15:26:35,446 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1533 bytes)
2016-03-02 15:26:35,447 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-02 15:26:35,461 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:26:35,467 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1821 bytes result sent to driver
2016-03-02 15:26:35,471 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 25 ms on localhost (1/1)
2016-03-02 15:26:35,473 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-02 15:26:35,473 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:59) finished in 0.027 s
2016-03-02 15:26:35,479 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:59, took 0.124790 s
2016-03-02 15:26:35,514 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:63508 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-02 15:26:35,537 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:63508 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-02 15:26:35,576 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:61
2016-03-02 15:26:35,581 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:60)
2016-03-02 15:26:35,585 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:61) with 10 output partitions (allowLocal=false)
2016-03-02 15:26:35,586 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:61)
2016-03-02 15:26:35,588 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-02 15:26:35,591 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-02 15:26:35,597 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at map at Main.scala:60), which has no missing parents
2016-03-02 15:26:35,608 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=255333, maxMem=278302556
2016-03-02 15:26:35,610 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-02 15:26:35,625 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=259285, maxMem=278302556
2016-03-02 15:26:35,626 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-02 15:26:35,630 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:63508 (size: 2.2 KB, free: 265.4 MB)
2016-03-02 15:26:35,631 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-02 15:26:35,634 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at map at Main.scala:60)
2016-03-02 15:26:35,635 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-02 15:26:35,636 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-02 15:26:35,639 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1522 bytes)
2016-03-02 15:26:35,640 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1522 bytes)
2016-03-02 15:26:35,641 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-02 15:26:35,641 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-02 15:26:35,647 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-02 15:26:35,647 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-02 15:26:35,811 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-02 15:26:35,816 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 176 ms on localhost (1/2)
2016-03-02 15:26:35,827 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-02 15:26:35,833 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 196 ms on localhost (2/2)
2016-03-02 15:26:35,834 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-02 15:26:35,835 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:60) finished in 0.198 s
2016-03-02 15:26:35,851 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-02 15:26:35,853 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-02 15:26:35,855 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-02 15:26:35,856 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-02 15:26:35,863 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-02 15:26:35,869 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[5] at reduceByKey at Main.scala:60), which is now runnable
2016-03-02 15:26:35,871 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=261558, maxMem=278302556
2016-03-02 15:26:35,873 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-02 15:26:35,896 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=263798, maxMem=278302556
2016-03-02 15:26:35,899 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-02 15:26:35,903 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:63508 (size: 1361.0 B, free: 265.4 MB)
2016-03-02 15:26:35,906 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-02 15:26:35,909 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[5] at reduceByKey at Main.scala:60)
2016-03-02 15:26:35,911 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-02 15:26:35,914 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-02 15:26:35,918 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:35,920 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:35,921 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:35,923 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:35,924 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-02 15:26:35,932 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-02 15:26:35,932 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-02 15:26:35,945 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:35,945 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:35,945 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-02 15:26:35,945 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:35,950 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 11 ms
2016-03-02 15:26:35,955 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:35,956 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-02 15:26:35,949 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-02 15:26:35,949 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-02 15:26:35,984 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1293 bytes result sent to driver
2016-03-02 15:26:35,983 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-02 15:26:35,998 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1280 bytes result sent to driver
2016-03-02 15:26:35,995 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:36,014 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:35,992 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-02 15:26:36,017 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:36,020 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-02 15:26:36,021 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:36,024 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-02 15:26:36,025 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,027 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-02 15:26:36,028 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,028 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-02 15:26:36,035 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-02 15:26:36,036 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1275 bytes result sent to driver
2016-03-02 15:26:36,015 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-02 15:26:36,027 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 107 ms on localhost (1/10)
2016-03-02 15:26:36,060 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 142 ms on localhost (2/10)
2016-03-02 15:26:36,062 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,066 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-02 15:26:36,071 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-02 15:26:36,018 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-02 15:26:36,083 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,064 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 143 ms on localhost (3/10)
2016-03-02 15:26:36,085 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-02 15:26:36,093 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 171 ms on localhost (4/10)
2016-03-02 15:26:36,094 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-02 15:26:36,096 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:36,100 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1242 bytes)
2016-03-02 15:26:36,113 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-02 15:26:36,119 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,123 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-02 15:26:36,130 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1334 bytes result sent to driver
2016-03-02 15:26:36,101 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-02 15:26:36,115 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 99 ms on localhost (5/10)
2016-03-02 15:26:36,136 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 117 ms on localhost (6/10)
2016-03-02 15:26:36,138 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 123 ms on localhost (7/10)
2016-03-02 15:26:36,141 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-02 15:26:36,143 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-02 15:26:36,141 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 42 ms on localhost (8/10)
2016-03-02 15:26:36,147 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 151 ms on localhost (9/10)
2016-03-02 15:26:36,164 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-02 15:26:36,167 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 72 ms on localhost (10/10)
2016-03-02 15:26:36,168 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-02 15:26:36,168 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:61) finished in 0.252 s
2016-03-02 15:26:36,171 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:61, took 0.595026 s
2016-03-02 15:27:11,845 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-02 15:27:11,858 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-02 15:27:11,859 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-02 15:27:11,859 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-02 15:27:11,860 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-02 15:27:11,862 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-02 15:27:11,863 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-02 15:27:11,863 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-02 15:27:11,864 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-02 15:27:11,864 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-02 15:27:11,865 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-02 15:27:11,865 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-02 15:27:11,867 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-02 15:27:11,868 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-02 15:27:11,869 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-02 15:27:11,870 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-02 15:27:11,871 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-02 15:27:11,872 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-02 15:27:11,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-02 15:27:11,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-02 15:27:11,876 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-02 15:27:11,877 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-02 15:27:11,878 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-02 15:27:11,879 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-02 15:27:11,880 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-02 15:27:11,881 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-02 15:27:11,934 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://192.168.3.139:4040
2016-03-02 15:27:11,937 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-02 15:27:12,009 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-02 15:27:12,017 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7a3e5f12-1cca-49de-8c15-496ad132b034\blockmgr-1d2e31c4-d4a3-4cf2-9c62-5a2ce4067542, already present as root for deletion.
2016-03-02 15:27:12,019 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-02 15:27:12,020 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-02 15:27:12,022 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-02 15:27:12,027 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-02 15:27:12,036 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-02 15:27:12,038 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-02 15:27:12,039 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-02 15:27:12,040 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c08d37c7-601f-4623-84e5-2ad343a6f5e3
2016-03-02 15:27:12,042 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-02 15:27:12,078 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-02 15:27:12,290 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c08d37c7-601f-4623-84e5-2ad343a6f5e3
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c08d37c7-601f-4623-84e5-2ad343a6f5e3
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-02 15:27:12,293 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7a3e5f12-1cca-49de-8c15-496ad132b034
2016-03-03 11:35:25,391 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 11:35:33,496 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 11:35:33,515 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 11:35:33,518 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 11:35:37,339 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 11:35:37,650 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 11:35:38,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:3837]
2016-03-03 11:35:38,488 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 3837.
2016-03-03 11:35:38,621 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 11:35:38,859 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 11:35:39,062 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e\blockmgr-1fbb5f51-c230-4a85-8b70-5391755b504c
2016-03-03 11:35:39,090 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 11:35:39,284 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e\httpd-5a5ae066-ae7b-4028-8c35-3c667d2f4602
2016-03-03 11:35:39,356 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 11:35:39,560 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:35:39,582 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:3838
2016-03-03 11:35:39,584 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 3838.
2016-03-03 11:35:39,757 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 11:35:40,246 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:35:40,262 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 11:35:40,287 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 11:35:40,295 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 11:35:40,545 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:3838/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976140544
2016-03-03 11:35:41,554 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:35:41,556 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:35:41,557 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:35:41,722 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 11:35:43,456 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3857.
2016-03-03 11:35:43,458 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 3857
2016-03-03 11:35:43,489 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 11:35:43,495 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:3857 with 265.4 MB RAM, BlockManagerId(driver, localhost, 3857)
2016-03-03 11:35:43,530 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 11:35:47,957 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 11:35:51,274 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:35:51,678 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:35:53,241 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:35:53,244 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:35:54,589 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:35:59,236 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:36:22,083 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:36:23,126 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:36:25,549 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:25,550 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:32,914 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:32,915 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:34,626 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:36:35,243 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 11:36:36,419 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:36:36,423 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:36:37,080 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:36:37,294 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:36:37,486 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 11:36:39,603 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 11:36:39,684 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:36:39,706 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:36:39,887 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:36:39,888 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:36:39,972 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:36:40,166 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:36:43,188 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:36:43,232 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:36:44,256 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:44,257 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:44,562 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:44,563 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:36:44,773 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 11:36:44,775 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:36:45,064 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:36:45,066 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:36:45,156 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:36:45,331 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:36:46,289 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:53
2016-03-03 11:36:46,528 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:53) with 10 output partitions (allowLocal=false)
2016-03-03 11:36:46,530 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:53)
2016-03-03 11:36:46,533 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:36:46,544 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:36:46,650 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-03 11:36:47,816 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 11:36:47,839 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 11:36:47,889 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 11:36:47,891 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 11:36:47,897 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:3857 (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:36:47,902 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:47,975 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-03 11:36:47,978 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 11:36:48,008 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 11:36:48,147 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:36:48,155 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:36:48,159 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:36:48,162 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:36:48,243 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 11:36:48,243 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 11:36:48,243 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 11:36:48,243 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 11:36:48,292 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:3838/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976140544
2016-03-03 11:36:48,731 INFO  [Executor task launch worker-1][org.apache.spark.util.Utils] Fetching http://169.254.236.187:3838/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e\userFiles-f90396bd-8cd1-449c-a274-406b70255847\fetchFileTemp6040245065287877167.tmp
2016-03-03 11:36:52,978 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e/userFiles-f90396bd-8cd1-449c-a274-406b70255847/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 11:36:53,197 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 11:36:53,197 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 11:36:53,197 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 11:36:53,197 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 11:36:53,200 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:36:53,202 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 11:36:53,204 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:36:53,206 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 11:36:53,209 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:36:53,210 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 11:36:53,212 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:36:53,213 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 11:36:53,214 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 11:36:53,219 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 11:36:53,219 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 11:36:53,218 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 11:36:53,224 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 5057 ms on localhost (1/10)
2016-03-03 11:36:53,226 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:36:53,227 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 11:36:53,228 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:36:53,229 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 11:36:53,244 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 11:36:53,247 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 11:36:53,258 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 5202 ms on localhost (2/10)
2016-03-03 11:36:53,259 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 5105 ms on localhost (3/10)
2016-03-03 11:36:53,261 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 52 ms on localhost (4/10)
2016-03-03 11:36:53,264 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 60 ms on localhost (5/10)
2016-03-03 11:36:53,265 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 66 ms on localhost (6/10)
2016-03-03 11:36:53,267 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 55 ms on localhost (7/10)
2016-03-03 11:36:53,268 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 5110 ms on localhost (8/10)
2016-03-03 11:36:53,270 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 44 ms on localhost (9/10)
2016-03-03 11:36:53,271 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 43 ms on localhost (10/10)
2016-03-03 11:36:53,271 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:53) finished in 5.259 s
2016-03-03 11:36:53,272 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 11:36:53,287 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:53, took 6.996948 s
2016-03-03 11:36:53,478 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 11:36:53,479 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 11:36:53,622 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 11:36:53,623 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 11:36:53,624 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:3857 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 11:36:53,626 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:56
2016-03-03 11:36:53,850 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 11:36:53,937 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:57
2016-03-03 11:36:53,940 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:57) with 2 output partitions (allowLocal=false)
2016-03-03 11:36:53,941 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:57)
2016-03-03 11:36:53,942 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:36:53,944 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:36:53,945 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:36:53,978 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 11:36:53,979 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:36:53,983 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 11:36:53,985 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:36:53,988 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:3857 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:36:53,990 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:53,995 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:36:53,996 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 11:36:53,999 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 11:36:54,003 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:36:54,006 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:36:54,007 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 11:36:54,007 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 11:36:54,043 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:36:54,043 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:36:54,121 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 11:36:54,121 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 11:36:54,123 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 11:36:54,124 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 11:36:54,129 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 11:36:54,488 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 11:36:54,488 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 11:36:54,498 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 496 ms on localhost (1/2)
2016-03-03 11:36:54,505 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 500 ms on localhost (2/2)
2016-03-03 11:36:54,507 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 11:36:54,509 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:57) finished in 0.508 s
2016-03-03 11:36:54,511 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:57, took 0.573259 s
2016-03-03 11:36:54,529 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 11:36:54,531 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 11:36:54,531 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:58)
2016-03-03 11:36:54,532 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:36:54,534 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:36:54,535 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:36:54,538 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 11:36:54,538 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:36:54,541 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 11:36:54,542 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:36:54,543 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:3857 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:36:54,544 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:54,545 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:36:54,546 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 11:36:54,547 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 11:36:54,549 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:36:54,551 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:36:54,552 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 11:36:54,552 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 11:36:54,556 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:36:54,557 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:36:54,562 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 11:36:54,562 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 11:36:54,566 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 15 ms on localhost (1/2)
2016-03-03 11:36:54,568 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 19 ms on localhost (2/2)
2016-03-03 11:36:54,568 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 11:36:54,568 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:58) finished in 0.019 s
2016-03-03 11:36:54,570 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:58, took 0.039955 s
2016-03-03 11:36:54,646 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:59
2016-03-03 11:36:54,648 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:59) with 1 output partitions (allowLocal=true)
2016-03-03 11:36:54,649 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:59)
2016-03-03 11:36:54,651 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:36:54,654 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:36:54,656 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:36:54,660 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 11:36:54,661 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 11:36:54,667 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 11:36:54,669 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 11:36:54,672 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:3857 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:36:54,674 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:54,675 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:36:54,676 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 11:36:54,677 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 11:36:54,679 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:36:54,680 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 11:36:54,685 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:36:54,689 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 11:36:54,692 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 14 ms on localhost (1/1)
2016-03-03 11:36:54,693 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 11:36:54,692 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:59) finished in 0.014 s
2016-03-03 11:36:54,695 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:59, took 0.047987 s
2016-03-03 11:36:54,876 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:61
2016-03-03 11:36:54,883 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:60)
2016-03-03 11:36:54,885 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:61) with 10 output partitions (allowLocal=false)
2016-03-03 11:36:54,885 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:61)
2016-03-03 11:36:54,886 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 11:36:54,887 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 11:36:54,891 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60), which has no missing parents
2016-03-03 11:36:54,895 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 11:36:54,896 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 11:36:54,899 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 11:36:54,900 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 11:36:54,902 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:3857 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 11:36:54,904 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:54,931 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60)
2016-03-03 11:36:54,932 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 11:36:54,934 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 11:36:54,936 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:36:54,937 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:36:54,938 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 11:36:54,938 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 11:36:54,945 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:36:54,945 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:36:55,227 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:3857 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:36:55,266 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:3857 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:36:55,272 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:3857 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:36:55,280 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:3857 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:36:55,327 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 11:36:55,327 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 11:36:55,332 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 397 ms on localhost (1/2)
2016-03-03 11:36:55,332 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 395 ms on localhost (2/2)
2016-03-03 11:36:55,333 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:60) finished in 0.398 s
2016-03-03 11:36:55,333 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 11:36:55,335 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 11:36:55,336 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 11:36:55,337 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 11:36:55,338 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 11:36:55,341 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 11:36:55,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60), which is now runnable
2016-03-03 11:36:55,346 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 11:36:55,347 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 11:36:55,349 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 11:36:55,350 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 11:36:55,353 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:3857 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 11:36:55,354 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 11:36:55,355 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60)
2016-03-03 11:36:55,356 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 11:36:55,357 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 11:36:55,359 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,361 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,362 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,363 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,364 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 11:36:55,364 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 11:36:55,364 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 11:36:55,364 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 11:36:55,384 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,384 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,384 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,384 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,386 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 11:36:55,386 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 11:36:55,386 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 11:36:55,387 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 11:36:55,427 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 11:36:55,427 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 11:36:55,427 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 11:36:55,427 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 11:36:55,429 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,432 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 11:36:55,433 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,434 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 11:36:55,435 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,436 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,437 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:36:55,436 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 11:36:55,438 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,441 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 11:36:55,440 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,447 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,446 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,444 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 80 ms on localhost (1/10)
2016-03-03 11:36:55,442 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 11:36:55,449 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 11:36:55,448 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:36:55,447 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 11:36:55,451 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 88 ms on localhost (2/10)
2016-03-03 11:36:55,457 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 11:36:55,456 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 11:36:55,457 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,459 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 11:36:55,459 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 99 ms on localhost (3/10)
2016-03-03 11:36:55,459 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 11:36:55,461 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 103 ms on localhost (4/10)
2016-03-03 11:36:55,463 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:36:55,464 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 11:36:55,466 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 37 ms on localhost (5/10)
2016-03-03 11:36:55,466 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,467 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 33 ms on localhost (6/10)
2016-03-03 11:36:55,469 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:36:55,467 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:36:55,469 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 11:36:55,469 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 37 ms on localhost (7/10)
2016-03-03 11:36:55,473 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 35 ms on localhost (8/10)
2016-03-03 11:36:55,475 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 11:36:55,475 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 11:36:55,477 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 21 ms on localhost (9/10)
2016-03-03 11:36:55,478 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 15 ms on localhost (10/10)
2016-03-03 11:36:55,478 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:61) finished in 0.120 s
2016-03-03 11:36:55,479 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 11:36:55,480 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:61, took 0.603640 s
2016-03-03 11:39:40,842 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 11:39:40,905 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 11:39:40,907 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 11:39:40,908 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 11:39:40,911 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 11:39:40,913 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 11:39:40,915 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 11:39:40,921 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 11:39:40,924 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 11:39:40,933 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 11:39:40,935 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 11:39:40,936 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 11:39:40,938 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 11:39:40,939 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 11:39:40,940 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 11:39:40,941 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 11:39:40,942 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 11:39:40,943 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 11:39:40,945 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 11:39:40,946 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 11:39:40,947 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 11:39:40,948 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 11:39:40,949 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 11:39:40,950 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 11:39:40,953 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 11:39:40,954 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 11:39:41,012 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 11:39:41,038 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 11:39:41,165 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 11:39:41,206 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e\blockmgr-1fbb5f51-c230-4a85-8b70-5391755b504c, already present as root for deletion.
2016-03-03 11:39:41,208 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 11:39:41,210 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 11:39:41,213 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 11:39:41,218 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 11:39:41,228 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 11:39:41,232 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 11:39:41,252 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 11:39:41,254 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 11:39:41,270 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-87f3f6cd-e9ea-4c04-b32b-fa0f41e5f79e
2016-03-03 11:39:41,320 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8e1a49ad-2a0e-4ef6-97d9-ac37d7f3f7fe
2016-03-03 11:39:41,360 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 11:39:41,486 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8e1a49ad-2a0e-4ef6-97d9-ac37d7f3f7fe
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8e1a49ad-2a0e-4ef6-97d9-ac37d7f3f7fe
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 11:39:45,787 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 11:39:49,866 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 11:39:49,867 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 11:39:49,869 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 11:39:50,684 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 11:39:50,742 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 11:39:50,919 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:4024]
2016-03-03 11:39:50,926 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 4024.
2016-03-03 11:39:50,943 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 11:39:50,958 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 11:39:50,983 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-0f1042e4-31b2-430a-abac-ba2d53923269\blockmgr-9822062b-132f-44c1-85f8-d99b132b0e22
2016-03-03 11:39:50,990 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 11:39:51,054 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-0f1042e4-31b2-430a-abac-ba2d53923269\httpd-fbb43205-06c5-496e-b408-8bd2585dab1e
2016-03-03 11:39:51,062 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 11:39:51,122 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:39:51,139 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:4025
2016-03-03 11:39:51,141 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 4025.
2016-03-03 11:39:51,159 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 11:39:51,287 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:39:51,334 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 11:39:51,336 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 11:39:51,340 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 11:39:51,475 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:4025/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976391474
2016-03-03 11:39:51,537 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:39:51,538 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:39:51,539 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:39:51,554 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 11:39:52,682 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4045.
2016-03-03 11:39:52,683 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 4045
2016-03-03 11:39:52,684 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 11:39:52,689 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:4045 with 265.4 MB RAM, BlockManagerId(driver, localhost, 4045)
2016-03-03 11:39:52,692 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 11:39:53,517 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 11:39:53,875 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:39:53,896 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:39:54,077 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:39:54,079 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:39:54,189 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:39:54,446 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:40:03,140 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:40:03,191 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:40:04,469 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:04,470 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:11,469 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:11,470 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:13,449 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:40:13,697 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 11:40:14,778 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:40:14,782 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:40:15,117 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:40:15,220 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:40:15,315 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 11:40:17,493 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 11:40:17,523 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:40:17,545 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:40:17,714 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:40:17,716 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:40:17,808 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:40:18,005 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:40:19,576 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:40:19,610 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:40:20,389 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:20,390 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:20,758 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:20,759 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:40:20,900 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 11:40:20,902 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:40:21,136 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:40:21,138 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:40:21,228 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:40:21,394 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:40:21,841 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:53
2016-03-03 11:40:21,859 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:53) with 10 output partitions (allowLocal=false)
2016-03-03 11:40:21,860 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:53)
2016-03-03 11:40:21,861 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:40:21,866 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:40:21,873 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-03 11:40:21,998 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 11:40:22,001 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 11:40:22,011 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 11:40:22,012 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 11:40:22,015 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:4045 (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:40:22,017 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:22,023 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-03 11:40:22,025 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 11:40:22,034 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 11:40:22,062 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:40:22,066 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:40:22,067 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:40:22,069 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:40:22,074 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 11:40:22,074 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 11:40:22,074 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 11:40:22,074 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 11:40:22,081 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:4025/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976391474
2016-03-03 11:40:22,162 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:4025/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-0f1042e4-31b2-430a-abac-ba2d53923269\userFiles-0c2ec3d1-c87f-4605-8564-d9975286fb7a\fetchFileTemp6696399562026630808.tmp
2016-03-03 11:40:26,191 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-0f1042e4-31b2-430a-abac-ba2d53923269/userFiles-0c2ec3d1-c87f-4605-8564-d9975286fb7a/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 11:40:26,212 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 11:40:26,213 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 11:40:26,216 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:40:26,213 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 11:40:26,212 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 11:40:26,223 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:40:26,225 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 11:40:26,226 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:40:26,228 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:40:26,229 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 11:40:26,227 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 11:40:26,232 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4184 ms on localhost (1/10)
2016-03-03 11:40:26,231 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 11:40:26,229 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 11:40:26,240 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 11:40:26,236 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4171 ms on localhost (2/10)
2016-03-03 11:40:26,235 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 11:40:26,242 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4173 ms on localhost (3/10)
2016-03-03 11:40:26,240 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 11:40:26,244 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4177 ms on localhost (4/10)
2016-03-03 11:40:26,246 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:40:26,247 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:40:26,248 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 11:40:26,252 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 11:40:26,254 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 11:40:26,258 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 27 ms on localhost (5/10)
2016-03-03 11:40:26,258 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 11:40:26,260 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 45 ms on localhost (6/10)
2016-03-03 11:40:26,262 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 33 ms on localhost (7/10)
2016-03-03 11:40:26,263 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 45 ms on localhost (8/10)
2016-03-03 11:40:26,265 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 19 ms on localhost (9/10)
2016-03-03 11:40:26,266 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 18 ms on localhost (10/10)
2016-03-03 11:40:26,267 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 11:40:26,267 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:53) finished in 4.230 s
2016-03-03 11:40:26,273 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:53, took 4.431108 s
2016-03-03 11:40:26,307 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 11:40:26,308 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 11:40:26,341 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 11:40:26,342 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 11:40:26,344 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:4045 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 11:40:26,346 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:56
2016-03-03 11:40:26,389 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 11:40:26,398 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:57
2016-03-03 11:40:26,400 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:57) with 2 output partitions (allowLocal=false)
2016-03-03 11:40:26,400 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:57)
2016-03-03 11:40:26,401 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:40:26,403 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:40:26,404 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:40:26,411 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 11:40:26,412 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:40:26,414 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 11:40:26,415 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:40:26,417 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:4045 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:40:26,418 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:26,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:40:26,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 11:40:26,422 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 11:40:26,425 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:40:26,426 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:40:26,426 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 11:40:26,427 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 11:40:26,436 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:40:26,436 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:40:26,444 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 11:40:26,444 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 11:40:26,445 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 11:40:26,446 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 11:40:26,446 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 11:40:26,468 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 11:40:26,468 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 11:40:26,471 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 46 ms on localhost (1/2)
2016-03-03 11:40:26,473 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 50 ms on localhost (2/2)
2016-03-03 11:40:26,474 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 11:40:26,474 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:57) finished in 0.050 s
2016-03-03 11:40:26,476 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:57, took 0.077225 s
2016-03-03 11:40:26,481 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 11:40:26,482 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 11:40:26,483 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:58)
2016-03-03 11:40:26,483 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:40:26,486 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:40:26,487 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:40:26,489 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 11:40:26,490 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:40:26,560 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 11:40:26,562 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:40:26,568 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:4045 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:40:26,571 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:26,575 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:40:26,576 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 11:40:26,580 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 11:40:26,585 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:40:26,591 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:40:26,592 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 11:40:26,594 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 11:40:26,600 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:40:26,610 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 11:40:26,614 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:40:26,616 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 33 ms on localhost (1/2)
2016-03-03 11:40:26,622 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 11:40:26,624 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 37 ms on localhost (2/2)
2016-03-03 11:40:26,624 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:58) finished in 0.042 s
2016-03-03 11:40:26,626 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:58, took 0.144510 s
2016-03-03 11:40:26,625 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 11:40:26,641 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:59
2016-03-03 11:40:26,642 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:59) with 1 output partitions (allowLocal=true)
2016-03-03 11:40:26,643 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:59)
2016-03-03 11:40:26,644 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:40:26,645 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:40:26,646 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:40:26,649 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 11:40:26,650 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 11:40:26,652 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 11:40:26,653 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 11:40:26,655 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:4045 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:40:26,656 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:26,657 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:40:26,657 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 11:40:26,659 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 11:40:26,661 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:40:26,662 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 11:40:26,666 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:40:26,672 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 11:40:26,675 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 14 ms on localhost (1/1)
2016-03-03 11:40:26,675 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:59) finished in 0.015 s
2016-03-03 11:40:26,676 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:59, took 0.034691 s
2016-03-03 11:40:26,675 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 11:40:26,710 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:61
2016-03-03 11:40:26,715 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:60)
2016-03-03 11:40:26,717 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:61) with 10 output partitions (allowLocal=false)
2016-03-03 11:40:26,717 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:61)
2016-03-03 11:40:26,718 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 11:40:26,719 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 11:40:26,722 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60), which has no missing parents
2016-03-03 11:40:26,727 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 11:40:26,728 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 11:40:26,780 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 11:40:26,782 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 11:40:26,785 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:4045 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 11:40:26,787 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:26,789 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60)
2016-03-03 11:40:26,790 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 11:40:26,791 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 11:40:26,794 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:40:26,795 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:4045 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:40:26,796 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:40:26,797 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 11:40:26,797 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 11:40:26,805 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:4045 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:40:26,805 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:40:26,805 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:40:26,808 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:4045 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:40:26,811 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:4045 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:40:26,925 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 11:40:26,928 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 11:40:26,931 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 135 ms on localhost (1/2)
2016-03-03 11:40:26,932 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 139 ms on localhost (2/2)
2016-03-03 11:40:26,932 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:60) finished in 0.139 s
2016-03-03 11:40:26,932 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 11:40:26,933 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 11:40:26,935 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 11:40:26,936 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 11:40:26,937 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 11:40:26,940 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 11:40:26,943 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60), which is now runnable
2016-03-03 11:40:26,945 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 11:40:26,945 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 11:40:26,948 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 11:40:26,948 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 11:40:26,951 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:4045 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 11:40:26,953 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 11:40:26,954 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60)
2016-03-03 11:40:26,954 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 11:40:26,956 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 11:40:26,958 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:26,959 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:26,960 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:26,961 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:26,962 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 11:40:26,962 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 11:40:26,962 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 11:40:26,962 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 11:40:26,978 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:26,978 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:26,978 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:26,978 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:26,981 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 11:40:26,980 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 11:40:26,980 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 11:40:26,981 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 11:40:27,004 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 11:40:27,004 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 11:40:27,004 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 11:40:27,004 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 11:40:27,006 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,008 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 11:40:27,009 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,010 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 11:40:27,011 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,013 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,013 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,016 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 11:40:27,016 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,019 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 11:40:27,014 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:40:27,023 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,026 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 11:40:27,027 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 11:40:27,018 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 11:40:27,018 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 57 ms on localhost (1/10)
2016-03-03 11:40:27,026 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 11:40:27,031 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 68 ms on localhost (2/10)
2016-03-03 11:40:27,032 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,030 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 11:40:27,033 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:40:27,033 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,036 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 11:40:27,036 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 79 ms on localhost (3/10)
2016-03-03 11:40:27,038 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 11:40:27,041 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,042 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:40:27,039 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:40:27,044 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 11:40:27,047 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 11:40:27,047 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 42 ms on localhost (4/10)
2016-03-03 11:40:27,050 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 37 ms on localhost (5/10)
2016-03-03 11:40:27,051 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:40:27,053 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:40:27,052 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 92 ms on localhost (6/10)
2016-03-03 11:40:27,057 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 49 ms on localhost (7/10)
2016-03-03 11:40:27,058 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 11:40:27,061 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 50 ms on localhost (8/10)
2016-03-03 11:40:27,063 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 24 ms on localhost (9/10)
2016-03-03 11:40:27,064 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 32 ms on localhost (10/10)
2016-03-03 11:40:27,065 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 11:40:27,065 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:61) finished in 0.109 s
2016-03-03 11:40:27,068 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:61, took 0.356421 s
2016-03-03 11:46:17,963 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 11:46:17,984 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 11:46:17,985 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 11:46:17,985 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 11:46:17,987 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 11:46:17,987 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 11:46:17,989 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 11:46:17,990 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 11:46:17,990 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 11:46:17,991 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 11:46:17,994 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 11:46:17,995 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 11:46:17,996 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 11:46:17,997 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 11:46:17,999 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 11:46:18,000 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 11:46:18,001 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 11:46:18,002 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 11:46:18,003 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 11:46:18,004 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 11:46:18,006 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 11:46:18,007 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 11:46:18,007 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 11:46:18,010 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 11:46:18,011 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 11:46:18,012 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 11:46:18,069 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 11:46:18,077 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 11:46:18,176 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 11:46:18,188 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-0f1042e4-31b2-430a-abac-ba2d53923269\blockmgr-9822062b-132f-44c1-85f8-d99b132b0e22, already present as root for deletion.
2016-03-03 11:46:18,190 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 11:46:18,192 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 11:46:18,194 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 11:46:18,197 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 11:46:18,204 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 11:46:18,205 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 11:46:18,204 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 11:46:18,208 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 11:46:18,210 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e4549da4-6815-4b2c-8adb-7f0429a9ab75
2016-03-03 11:46:18,240 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 11:46:18,383 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e4549da4-6815-4b2c-8adb-7f0429a9ab75
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e4549da4-6815-4b2c-8adb-7f0429a9ab75
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 11:46:18,385 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-0f1042e4-31b2-430a-abac-ba2d53923269
2016-03-03 11:46:21,103 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 11:46:25,087 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 11:46:25,088 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 11:46:25,090 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 11:46:25,792 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 11:46:25,837 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Starting remoting
2016-03-03 11:46:26,002 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:4253]
2016-03-03 11:46:26,008 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 4253.
2016-03-03 11:46:26,027 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 11:46:26,041 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 11:46:26,065 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c515d131-10ef-42a8-909a-60876806a7db\blockmgr-7f12522a-4b07-4c7d-a9de-9724381d8d6a
2016-03-03 11:46:26,071 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 11:46:26,151 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c515d131-10ef-42a8-909a-60876806a7db\httpd-c5b775bf-8170-400a-abb8-bdc34ce2d7ba
2016-03-03 11:46:26,158 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 11:46:26,227 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:46:26,248 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:4254
2016-03-03 11:46:26,251 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 4254.
2016-03-03 11:46:26,272 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 11:46:26,403 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 11:46:26,426 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 11:46:26,427 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 11:46:26,429 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 11:46:26,600 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:4254/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976786599
2016-03-03 11:46:26,664 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:46:26,665 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:46:26,666 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 11:46:26,681 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 11:46:27,815 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4284.
2016-03-03 11:46:27,816 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 4284
2016-03-03 11:46:27,817 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 11:46:27,822 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:4284 with 265.4 MB RAM, BlockManagerId(driver, localhost, 4284)
2016-03-03 11:46:27,825 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 11:46:28,639 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 11:46:29,032 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:46:29,053 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:46:29,238 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:46:29,239 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:46:29,352 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:46:29,585 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:46:38,429 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:46:38,479 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:46:39,557 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:39,559 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:45,598 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:45,599 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:47,366 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:46:47,660 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 11:46:48,349 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:46:48,354 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:46:48,742 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:46:48,832 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:46:48,898 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 11:46:51,333 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 11:46:51,379 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 11:46:51,402 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 11:46:51,565 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 11:46:51,566 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 11:46:51,655 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:46:51,834 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 11:46:53,356 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 11:46:53,400 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 11:46:54,129 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:54,130 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:54,443 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:54,444 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 11:46:54,571 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 11:46:54,573 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 11:46:54,791 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 11:46:54,794 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 11:46:54,861 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 11:46:55,038 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 11:46:55,497 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:53
2016-03-03 11:46:55,520 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:53) with 10 output partitions (allowLocal=false)
2016-03-03 11:46:55,521 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:53)
2016-03-03 11:46:55,522 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:46:55,526 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:46:55,534 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-03 11:46:55,703 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 11:46:55,706 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 11:46:55,718 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 11:46:55,719 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 11:46:55,739 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:4284 (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:46:55,742 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 11:46:55,749 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-03 11:46:55,751 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 11:46:55,759 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 11:46:55,789 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:46:55,792 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:46:55,793 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:46:55,794 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:46:55,799 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 11:46:55,799 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 11:46:55,799 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 11:46:55,799 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 11:46:55,806 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:4254/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456976786599
2016-03-03 11:46:55,883 INFO  [Executor task launch worker-2][org.apache.spark.util.Utils] Fetching http://169.254.236.187:4254/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c515d131-10ef-42a8-909a-60876806a7db\userFiles-a7e04cdb-3973-418d-b144-4a9081a98449\fetchFileTemp7727985243000827022.tmp
2016-03-03 11:46:59,706 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-c515d131-10ef-42a8-909a-60876806a7db/userFiles-a7e04cdb-3973-418d-b144-4a9081a98449/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 11:46:59,727 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 11:46:59,727 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 11:46:59,727 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 11:46:59,727 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 11:46:59,731 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:46:59,732 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 11:46:59,735 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3941 ms on localhost (1/10)
2016-03-03 11:46:59,754 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 11:46:59,756 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:46:59,759 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 11:46:59,760 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:46:59,765 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3973 ms on localhost (2/10)
2016-03-03 11:46:59,765 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 11:46:59,767 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:46:59,767 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 11:46:59,771 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 11:46:59,771 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 11:46:59,775 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 11:46:59,775 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 11:46:59,776 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 11:46:59,776 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 46 ms on localhost (3/10)
2016-03-03 11:46:59,780 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4008 ms on localhost (4/10)
2016-03-03 11:46:59,781 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 11:46:59,782 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3989 ms on localhost (5/10)
2016-03-03 11:46:59,785 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 11:46:59,787 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 11:46:59,791 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 11:46:59,792 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 37 ms on localhost (6/10)
2016-03-03 11:46:59,799 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 29 ms on localhost (7/10)
2016-03-03 11:46:59,800 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 33 ms on localhost (8/10)
2016-03-03 11:46:59,801 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 41 ms on localhost (9/10)
2016-03-03 11:46:59,802 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 18 ms on localhost (10/10)
2016-03-03 11:46:59,803 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:53) finished in 4.042 s
2016-03-03 11:46:59,803 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 11:46:59,811 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:53, took 4.312943 s
2016-03-03 11:46:59,851 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 11:46:59,852 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 11:46:59,885 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 11:46:59,886 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 11:46:59,887 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:4284 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 11:46:59,889 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:56
2016-03-03 11:46:59,929 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 11:46:59,937 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:57
2016-03-03 11:46:59,939 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:57) with 2 output partitions (allowLocal=false)
2016-03-03 11:46:59,940 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:57)
2016-03-03 11:46:59,941 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:46:59,943 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:46:59,944 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:46:59,952 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 11:46:59,952 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:46:59,954 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 11:46:59,955 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:46:59,957 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:4284 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:46:59,958 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 11:46:59,961 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:46:59,961 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 11:46:59,962 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 11:46:59,965 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:46:59,966 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:46:59,967 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 11:46:59,968 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 11:46:59,976 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:46:59,976 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:46:59,984 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 11:46:59,984 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 11:46:59,985 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 11:46:59,985 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 11:46:59,986 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 11:47:00,015 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 11:47:00,015 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 11:47:00,019 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 54 ms on localhost (1/2)
2016-03-03 11:47:00,021 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 57 ms on localhost (2/2)
2016-03-03 11:47:00,023 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 11:47:00,022 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:57) finished in 0.058 s
2016-03-03 11:47:00,031 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:57, took 0.093717 s
2016-03-03 11:47:00,036 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 11:47:00,039 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 11:47:00,039 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:58)
2016-03-03 11:47:00,041 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:47:00,043 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:47:00,043 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:47:00,047 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 11:47:00,050 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 11:47:00,052 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 11:47:00,053 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 11:47:00,054 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:4284 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:47:00,055 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 11:47:00,056 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:47:00,057 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 11:47:00,058 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 11:47:00,060 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:47:00,062 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:47:00,066 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 11:47:00,066 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 11:47:00,070 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:47:00,072 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:47:00,075 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 11:47:00,080 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 18 ms on localhost (1/2)
2016-03-03 11:47:00,081 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 11:47:00,085 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 25 ms on localhost (2/2)
2016-03-03 11:47:00,086 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 11:47:00,085 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:58) finished in 0.026 s
2016-03-03 11:47:00,088 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:58, took 0.051074 s
2016-03-03 11:47:00,107 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:59
2016-03-03 11:47:00,109 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:59) with 1 output partitions (allowLocal=true)
2016-03-03 11:47:00,109 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:59)
2016-03-03 11:47:00,110 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 11:47:00,113 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 11:47:00,114 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 11:47:00,117 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 11:47:00,118 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 11:47:00,120 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 11:47:00,121 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 11:47:00,124 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:4284 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:47:00,125 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 11:47:00,126 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 11:47:00,128 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 11:47:00,131 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 11:47:00,133 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 11:47:00,134 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 11:47:00,140 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:47:00,144 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 11:47:00,148 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 16 ms on localhost (1/1)
2016-03-03 11:47:00,148 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:59) finished in 0.017 s
2016-03-03 11:47:00,149 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 11:47:00,151 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:59, took 0.043074 s
2016-03-03 11:47:00,182 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:61
2016-03-03 11:47:00,185 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:60)
2016-03-03 11:47:00,186 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:61) with 10 output partitions (allowLocal=false)
2016-03-03 11:47:00,187 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:61)
2016-03-03 11:47:00,187 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 11:47:00,189 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 11:47:00,192 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60), which has no missing parents
2016-03-03 11:47:00,197 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 11:47:00,198 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 11:47:00,200 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 11:47:00,201 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 11:47:00,203 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:4284 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 11:47:00,205 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 11:47:00,207 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60)
2016-03-03 11:47:00,207 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 11:47:00,209 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 11:47:00,212 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:47:00,213 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 11:47:00,214 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 11:47:00,214 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 11:47:00,221 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 11:47:00,221 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 11:47:00,337 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 11:47:00,337 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 11:47:00,341 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 131 ms on localhost (1/2)
2016-03-03 11:47:00,342 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 130 ms on localhost (2/2)
2016-03-03 11:47:00,342 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:60) finished in 0.133 s
2016-03-03 11:47:00,343 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 11:47:00,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 11:47:00,346 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 11:47:00,347 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 11:47:00,348 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 11:47:00,351 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 11:47:00,353 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60), which is now runnable
2016-03-03 11:47:00,355 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=272904, maxMem=278302556
2016-03-03 11:47:00,356 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 11:47:00,404 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=275144, maxMem=278302556
2016-03-03 11:47:00,406 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.1 MB)
2016-03-03 11:47:00,411 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:4284 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 11:47:00,413 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 11:47:00,414 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60)
2016-03-03 11:47:00,414 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 11:47:00,416 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 11:47:00,418 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:4284 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 11:47:00,419 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,420 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,421 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,422 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,423 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 11:47:00,423 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 11:47:00,423 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 11:47:00,423 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 11:47:00,427 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:4284 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:47:00,437 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:4284 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 11:47:00,441 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:4284 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 11:47:00,442 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,442 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,442 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,445 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 11:47:00,445 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 11:47:00,443 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,448 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 10 ms
2016-03-03 11:47:00,445 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 11:47:00,469 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 11:47:00,469 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 11:47:00,469 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 11:47:00,469 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 11:47:00,471 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,473 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 11:47:00,474 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,475 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 11:47:00,477 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,478 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,480 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:47:00,480 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,482 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:47:00,479 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,478 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 11:47:00,485 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 66 ms on localhost (1/10)
2016-03-03 11:47:00,485 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 11:47:00,489 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,490 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 11:47:00,485 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 11:47:00,491 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,493 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:47:00,495 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 11:47:00,487 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 65 ms on localhost (2/10)
2016-03-03 11:47:00,486 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 11:47:00,498 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 11:47:00,498 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 81 ms on localhost (3/10)
2016-03-03 11:47:00,501 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,502 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 11:47:00,503 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 11:47:00,506 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,508 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 11:47:00,506 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 85 ms on localhost (4/10)
2016-03-03 11:47:00,508 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:47:00,513 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 43 ms on localhost (5/10)
2016-03-03 11:47:00,513 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 11:47:00,515 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 11:47:00,514 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 40 ms on localhost (6/10)
2016-03-03 11:47:00,520 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 11:47:00,519 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 11:47:00,521 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 44 ms on localhost (7/10)
2016-03-03 11:47:00,523 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 45 ms on localhost (8/10)
2016-03-03 11:47:00,527 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 24 ms on localhost (9/10)
2016-03-03 11:47:00,528 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 27 ms on localhost (10/10)
2016-03-03 11:47:00,529 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 11:47:00,529 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:61) finished in 0.113 s
2016-03-03 11:47:00,531 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:61, took 0.349191 s
2016-03-03 12:01:32,590 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 12:01:32,606 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 12:01:32,606 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 12:01:32,607 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 12:01:32,608 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 12:01:32,610 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 12:01:32,611 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 12:01:32,611 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 12:01:32,612 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 12:01:32,613 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 12:01:32,614 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 12:01:32,615 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 12:01:32,615 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 12:01:32,617 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 12:01:32,618 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 12:01:32,619 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 12:01:32,619 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 12:01:32,623 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 12:01:32,623 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 12:01:32,624 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 12:01:32,626 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 12:01:32,627 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 12:01:32,628 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 12:01:32,628 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 12:01:32,630 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 12:01:32,631 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 12:01:32,683 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 12:01:32,686 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 12:01:32,778 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 12:01:32,794 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c515d131-10ef-42a8-909a-60876806a7db\blockmgr-7f12522a-4b07-4c7d-a9de-9724381d8d6a, already present as root for deletion.
2016-03-03 12:01:32,797 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 12:01:32,799 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 12:01:32,803 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 12:01:32,806 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 12:01:32,809 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 12:01:32,815 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 12:01:32,814 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 12:01:32,819 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 12:01:32,821 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c515d131-10ef-42a8-909a-60876806a7db
2016-03-03 12:01:32,845 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 12:01:32,865 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8ca33282-dcad-4d8f-b858-87d9a9d64ff3
2016-03-03 12:01:33,169 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8ca33282-dcad-4d8f-b858-87d9a9d64ff3
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8ca33282-dcad-4d8f-b858-87d9a9d64ff3
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 12:14:29,590 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 12:14:32,758 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 12:14:32,760 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 12:14:32,762 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 12:14:33,752 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 12:14:33,803 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 12:14:33,994 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:5678]
2016-03-03 12:14:34,001 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 5678.
2016-03-03 12:14:34,022 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 12:14:34,037 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 12:14:34,065 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb\blockmgr-31f02334-2ed1-462d-ae43-49912ce473b7
2016-03-03 12:14:34,072 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 12:14:34,164 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb\httpd-c14c4f00-df88-4c15-a2b8-81c91d639505
2016-03-03 12:14:34,171 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 12:14:34,236 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:14:34,259 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:5679
2016-03-03 12:14:34,261 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 5679.
2016-03-03 12:14:34,285 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 12:14:34,426 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:14:34,446 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 12:14:34,447 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 12:14:34,451 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 12:14:34,609 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:5679/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456978474609
2016-03-03 12:14:34,683 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:14:34,684 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:14:34,685 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:14:34,701 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 12:14:36,006 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5698.
2016-03-03 12:14:36,008 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 5698
2016-03-03 12:14:36,009 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 12:14:36,014 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:5698 with 265.4 MB RAM, BlockManagerId(driver, localhost, 5698)
2016-03-03 12:14:36,018 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 12:14:36,943 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 12:14:37,333 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:14:37,356 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:14:37,549 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:14:37,551 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:14:37,683 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:14:37,966 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:14:46,274 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:14:46,346 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:14:47,635 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:14:47,636 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:14:54,532 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:14:54,533 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:14:56,440 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:14:56,738 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 12:14:57,617 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:14:57,621 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:14:57,935 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:14:58,135 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:14:58,230 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 12:14:59,975 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 12:15:00,024 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:15:00,067 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:15:00,301 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:15:00,302 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:15:00,429 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:15:00,713 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:15:02,178 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:15:02,233 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:15:03,234 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:15:03,238 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:15:03,675 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:15:03,676 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:15:03,824 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 12:15:03,826 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:15:04,079 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:15:04,081 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:15:04,175 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:15:04,361 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:15:04,978 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:53
2016-03-03 12:15:05,006 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:53) with 10 output partitions (allowLocal=false)
2016-03-03 12:15:05,007 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:53)
2016-03-03 12:15:05,008 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:15:05,013 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:15:05,020 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52), which has no missing parents
2016-03-03 12:15:05,163 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 12:15:05,166 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 12:15:05,178 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 12:15:05,179 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 12:15:05,183 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:5698 (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:15:05,186 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:05,192 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:52)
2016-03-03 12:15:05,194 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 12:15:05,202 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 12:15:05,279 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:15:05,283 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:15:05,284 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:15:05,286 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:15:05,291 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 12:15:05,291 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 12:15:05,291 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 12:15:05,291 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 12:15:05,296 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:5679/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456978474609
2016-03-03 12:15:05,365 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:5679/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb\userFiles-2d17e385-5d74-4365-915b-02abe34f1a60\fetchFileTemp3848745882255537216.tmp
2016-03-03 12:15:10,628 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb/userFiles-2d17e385-5d74-4365-915b-02abe34f1a60/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 12:15:10,660 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 12:15:10,660 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 12:15:10,664 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:15:10,667 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:15:10,677 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 12:15:10,660 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 12:15:10,660 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 12:15:10,679 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 12:15:10,683 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 12:15:10,678 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 5458 ms on localhost (1/10)
2016-03-03 12:15:10,686 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 12:15:10,688 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:15:10,690 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 12:15:10,691 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:15:10,693 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:15:10,693 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 12:15:10,694 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 5411 ms on localhost (2/10)
2016-03-03 12:15:10,693 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 12:15:10,697 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 12:15:10,699 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 12:15:10,693 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 12:15:10,700 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 5417 ms on localhost (3/10)
2016-03-03 12:15:10,702 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:15:10,703 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 12:15:10,708 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 12:15:10,709 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 5424 ms on localhost (4/10)
2016-03-03 12:15:10,710 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 44 ms on localhost (5/10)
2016-03-03 12:15:10,711 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 48 ms on localhost (6/10)
2016-03-03 12:15:10,712 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 25 ms on localhost (7/10)
2016-03-03 12:15:10,713 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 23 ms on localhost (8/10)
2016-03-03 12:15:10,714 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 22 ms on localhost (9/10)
2016-03-03 12:15:10,715 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 14 ms on localhost (10/10)
2016-03-03 12:15:10,716 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:53) finished in 5.511 s
2016-03-03 12:15:10,716 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 12:15:10,723 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:53, took 5.743680 s
2016-03-03 12:15:10,780 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 12:15:10,781 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 12:15:10,825 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 12:15:10,826 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 12:15:10,827 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:5698 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 12:15:10,831 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:56
2016-03-03 12:15:10,938 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 12:15:10,949 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:57
2016-03-03 12:15:10,951 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:57) with 2 output partitions (allowLocal=false)
2016-03-03 12:15:10,952 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:57)
2016-03-03 12:15:10,952 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:15:10,954 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:15:10,955 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 12:15:10,965 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 12:15:10,966 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:15:10,968 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 12:15:10,969 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:15:10,971 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:5698 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:15:10,972 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:10,974 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 12:15:10,975 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 12:15:10,975 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 12:15:10,977 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:15:10,978 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:15:10,979 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 12:15:10,979 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 12:15:10,989 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:15:10,989 INFO  [Executor task launch worker-4][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:15:11,015 INFO  [Executor task launch worker-4][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 12:15:11,016 INFO  [Executor task launch worker-4][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 12:15:11,016 INFO  [Executor task launch worker-4][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 12:15:11,017 INFO  [Executor task launch worker-4][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 12:15:11,015 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 12:15:11,089 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 12:15:11,089 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 12:15:11,093 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 115 ms on localhost (1/2)
2016-03-03 12:15:11,094 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 118 ms on localhost (2/2)
2016-03-03 12:15:11,095 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 12:15:11,094 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:57) finished in 0.118 s
2016-03-03 12:15:11,096 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:57, took 0.146798 s
2016-03-03 12:15:11,102 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 12:15:11,103 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 12:15:11,104 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:58)
2016-03-03 12:15:11,104 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:15:11,106 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:15:11,107 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 12:15:11,109 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 12:15:11,109 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:15:11,111 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 12:15:11,112 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:15:11,113 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:5698 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:15:11,115 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:11,116 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 12:15:11,116 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 12:15:11,118 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 12:15:11,120 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:15:11,121 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:15:11,123 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 12:15:11,123 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 12:15:11,127 INFO  [Executor task launch worker-4][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:15:11,136 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 12:15:11,140 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:15:11,146 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 12:15:11,150 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 30 ms on localhost (1/2)
2016-03-03 12:15:11,152 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 32 ms on localhost (2/2)
2016-03-03 12:15:11,153 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 12:15:11,153 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:58) finished in 0.035 s
2016-03-03 12:15:11,155 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:58, took 0.052074 s
2016-03-03 12:15:11,233 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:59
2016-03-03 12:15:11,234 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:59) with 1 output partitions (allowLocal=true)
2016-03-03 12:15:11,234 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:59)
2016-03-03 12:15:11,235 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:15:11,236 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:15:11,237 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56), which has no missing parents
2016-03-03 12:15:11,239 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 12:15:11,239 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 12:15:11,241 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 12:15:11,242 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 12:15:11,244 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:5698 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:15:11,246 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:11,247 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:56)
2016-03-03 12:15:11,248 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 12:15:11,249 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 12:15:11,251 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:15:11,252 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 12:15:11,256 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:15:11,261 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 12:15:11,267 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 17 ms on localhost (1/1)
2016-03-03 12:15:11,267 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:59) finished in 0.018 s
2016-03-03 12:15:11,268 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 12:15:11,269 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:59, took 0.035378 s
2016-03-03 12:15:11,307 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:61
2016-03-03 12:15:11,311 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:60)
2016-03-03 12:15:11,312 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:61) with 10 output partitions (allowLocal=false)
2016-03-03 12:15:11,312 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:61)
2016-03-03 12:15:11,313 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 12:15:11,314 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 12:15:11,317 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60), which has no missing parents
2016-03-03 12:15:11,322 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 12:15:11,323 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 12:15:11,325 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 12:15:11,325 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 12:15:11,327 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:5698 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 12:15:11,328 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:11,331 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:60)
2016-03-03 12:15:11,331 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 12:15:11,332 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 12:15:11,334 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:15:11,335 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:15:11,336 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 12:15:11,336 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 12:15:11,343 INFO  [Executor task launch worker-4][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:15:11,343 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:15:11,659 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:5698 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:15:11,702 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:5698 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:15:11,706 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 12:15:11,707 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 12:15:11,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:5698 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:15:11,714 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 379 ms on localhost (1/2)
2016-03-03 12:15:11,715 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 382 ms on localhost (2/2)
2016-03-03 12:15:11,715 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:60) finished in 0.383 s
2016-03-03 12:15:11,717 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 12:15:11,718 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 12:15:11,715 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:5698 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:15:11,719 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 12:15:11,720 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 12:15:11,716 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 12:15:11,723 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 12:15:11,727 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60), which is now runnable
2016-03-03 12:15:11,730 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 12:15:11,731 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 12:15:11,733 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 12:15:11,734 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 12:15:11,737 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:5698 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 12:15:11,744 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 12:15:11,747 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:60)
2016-03-03 12:15:11,747 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 12:15:11,748 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 12:15:11,752 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,753 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,754 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,755 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,756 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 12:15:11,756 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 12:15:11,759 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 12:15:11,759 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 12:15:11,868 INFO  [Executor task launch worker-4][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,869 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,868 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,868 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,871 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:15:11,870 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:15:11,870 INFO  [Executor task launch worker-4][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:15:11,871 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:15:11,898 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 12:15:11,899 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 12:15:11,899 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 12:15:11,898 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 12:15:11,900 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,902 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 12:15:11,903 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,905 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,906 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 12:15:11,908 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,908 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 12:15:11,906 INFO  [Executor task launch worker-4][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,914 INFO  [Executor task launch worker-4][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 12:15:11,916 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 165 ms on localhost (1/10)
2016-03-03 12:15:11,913 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,921 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 12:15:11,906 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 12:15:11,921 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 166 ms on localhost (2/10)
2016-03-03 12:15:11,919 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 12:15:11,915 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,928 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 13 ms
2016-03-03 12:15:11,926 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 173 ms on localhost (3/10)
2016-03-03 12:15:11,926 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,930 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-03 12:15:11,934 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 12:15:11,934 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 178 ms on localhost (4/10)
2016-03-03 12:15:11,937 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,937 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 12:15:11,928 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 12:15:11,941 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,943 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 12:15:11,935 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 12:15:11,942 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:15:11,947 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 12:15:11,947 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 12:15:11,951 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:15:11,956 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:15:11,961 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 12:15:11,956 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 51 ms on localhost (5/10)
2016-03-03 12:15:11,966 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 59 ms on localhost (6/10)
2016-03-03 12:15:11,989 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 89 ms on localhost (7/10)
2016-03-03 12:15:12,008 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 66 ms on localhost (8/10)
2016-03-03 12:15:12,010 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 74 ms on localhost (9/10)
2016-03-03 12:15:12,014 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 112 ms on localhost (10/10)
2016-03-03 12:15:12,015 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 12:15:12,019 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:61) finished in 0.270 s
2016-03-03 12:15:12,022 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:61, took 0.714161 s
2016-03-03 12:19:58,922 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 12:19:58,957 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 12:19:58,959 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 12:19:58,961 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 12:19:58,962 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 12:19:58,963 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 12:19:58,963 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 12:19:58,964 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 12:19:58,966 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 12:19:58,967 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 12:19:58,970 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 12:19:58,971 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 12:19:58,972 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 12:19:58,973 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 12:19:58,974 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 12:19:58,974 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 12:19:58,976 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 12:19:58,977 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 12:19:58,978 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 12:19:58,978 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 12:19:58,980 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 12:19:58,981 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 12:19:58,981 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 12:19:58,982 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 12:19:58,984 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 12:19:58,984 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 12:19:59,037 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 12:19:59,040 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 12:19:59,137 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 12:19:59,148 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb\blockmgr-31f02334-2ed1-462d-ae43-49912ce473b7, already present as root for deletion.
2016-03-03 12:19:59,150 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 12:19:59,153 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 12:19:59,155 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 12:19:59,160 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 12:19:59,166 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 12:19:59,168 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 12:19:59,170 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 12:19:59,171 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 12:19:59,173 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c15e19d8-7e0e-437f-8459-5e9289608f56
2016-03-03 12:19:59,209 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 12:19:59,380 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c15e19d8-7e0e-437f-8459-5e9289608f56
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-c15e19d8-7e0e-437f-8459-5e9289608f56
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 12:19:59,382 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-ad3227df-0059-4efa-a0ac-f4066a2a0cbb
2016-03-03 12:21:24,523 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 12:21:27,532 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 12:21:27,533 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 12:21:27,534 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 12:21:28,258 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 12:21:28,302 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 12:21:28,471 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:6010]
2016-03-03 12:21:28,479 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 6010.
2016-03-03 12:21:28,497 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 12:21:28,512 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 12:21:28,536 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec\blockmgr-26a88c7c-d624-48a3-b7af-06f25d12ffd8
2016-03-03 12:21:28,543 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 12:21:28,639 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec\httpd-50adeb62-c4ae-4220-b03d-7e7ec1c072fb
2016-03-03 12:21:28,648 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 12:21:28,707 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:21:28,729 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:6011
2016-03-03 12:21:28,732 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 6011.
2016-03-03 12:21:28,755 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 12:21:28,890 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:21:28,910 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 12:21:28,911 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 12:21:28,937 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 12:21:29,097 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:6011/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456978889097
2016-03-03 12:21:29,164 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:21:29,165 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:21:29,166 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:21:29,182 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 12:21:30,523 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6030.
2016-03-03 12:21:30,525 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 6030
2016-03-03 12:21:30,526 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 12:21:30,531 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:6030 with 265.4 MB RAM, BlockManagerId(driver, localhost, 6030)
2016-03-03 12:21:30,534 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 12:21:31,398 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 12:21:31,744 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:21:31,767 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:21:31,939 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:21:31,940 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:21:32,073 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:21:32,329 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:21:40,982 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:21:41,046 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:21:42,778 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:21:42,780 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:21:55,069 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:21:55,070 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:21:58,587 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:21:59,148 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 12:22:00,033 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:22:00,036 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:22:00,510 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:22:00,612 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:22:00,698 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 12:22:02,786 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 12:22:02,834 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:22:02,856 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:22:03,014 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:22:03,015 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:22:03,109 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:22:03,301 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:22:04,421 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:22:04,469 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:22:05,264 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:22:05,265 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:22:05,570 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:22:05,571 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:22:05,695 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 12:22:05,698 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:22:05,969 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:22:05,972 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:22:06,072 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:22:06,277 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:22:06,773 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 12:22:06,793 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 12:22:06,795 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 12:22:06,796 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:22:06,802 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:22:06,809 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 12:22:06,986 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 12:22:06,989 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 12:22:07,003 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 12:22:07,005 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 12:22:07,008 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:6030 (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:22:07,012 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:07,019 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 12:22:07,022 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 12:22:07,033 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 12:22:07,063 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:22:07,066 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:22:07,067 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:22:07,069 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:22:07,076 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 12:22:07,076 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 12:22:07,076 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 12:22:07,076 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 12:22:07,084 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:6011/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456978889097
2016-03-03 12:22:07,151 INFO  [Executor task launch worker-1][org.apache.spark.util.Utils] Fetching http://169.254.236.187:6011/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec\userFiles-7de43701-0d4b-4f90-8307-8eaa838a595d\fetchFileTemp4542758677206359351.tmp
2016-03-03 12:22:10,955 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec/userFiles-7de43701-0d4b-4f90-8307-8eaa838a595d/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 12:22:10,977 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 12:22:10,977 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 12:22:10,977 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 12:22:10,977 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 12:22:10,980 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:22:10,982 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 12:22:10,985 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:22:10,988 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:22:10,990 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 12:22:10,992 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 12:22:10,992 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 12:22:10,993 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:22:10,997 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 12:22:10,997 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 12:22:10,999 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:22:11,000 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 12:22:11,001 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 12:22:11,000 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 12:22:11,004 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3933 ms on localhost (1/10)
2016-03-03 12:22:11,005 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 26 ms on localhost (2/10)
2016-03-03 12:22:11,006 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 12:22:11,007 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 22 ms on localhost (3/10)
2016-03-03 12:22:11,009 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3962 ms on localhost (4/10)
2016-03-03 12:22:11,010 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3944 ms on localhost (5/10)
2016-03-03 12:22:11,010 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3943 ms on localhost (6/10)
2016-03-03 12:22:11,012 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:22:11,014 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 12:22:11,018 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 12:22:11,021 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 33 ms on localhost (7/10)
2016-03-03 12:22:11,022 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 24 ms on localhost (8/10)
2016-03-03 12:22:11,023 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 30 ms on localhost (9/10)
2016-03-03 12:22:11,024 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 11 ms on localhost (10/10)
2016-03-03 12:22:11,024 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 3.989 s
2016-03-03 12:22:11,025 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,033 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 4.258973 s
2016-03-03 12:22:11,073 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 12:22:11,074 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 12:22:11,109 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 12:22:11,109 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 12:22:11,111 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:6030 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 12:22:11,112 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 12:22:11,159 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 12:22:11,170 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 12:22:11,173 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 12:22:11,173 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 12:22:11,174 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:22:11,175 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:22:11,176 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:22:11,185 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 12:22:11,186 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:22:11,188 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 12:22:11,189 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:22:11,191 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:6030 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:22:11,192 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:11,194 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:22:11,195 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 12:22:11,196 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 12:22:11,198 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:22:11,199 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:22:11,200 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 12:22:11,200 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 12:22:11,210 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:22:11,210 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:22:11,219 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 12:22:11,219 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 12:22:11,220 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 12:22:11,220 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 12:22:11,221 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 12:22:11,243 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 12:22:11,243 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 12:22:11,247 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 50 ms on localhost (1/2)
2016-03-03 12:22:11,248 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 49 ms on localhost (2/2)
2016-03-03 12:22:11,249 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,248 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.052 s
2016-03-03 12:22:11,251 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.080539 s
2016-03-03 12:22:11,257 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 12:22:11,258 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 12:22:11,259 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 12:22:11,259 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:22:11,262 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:22:11,262 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:22:11,265 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 12:22:11,265 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:22:11,268 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 12:22:11,270 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:22:11,271 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:6030 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:22:11,272 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:11,273 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:22:11,274 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 12:22:11,275 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 12:22:11,277 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:22:11,279 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:22:11,279 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 12:22:11,279 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 12:22:11,284 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:22:11,284 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:22:11,289 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 12:22:11,289 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 12:22:11,292 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 15 ms on localhost (1/2)
2016-03-03 12:22:11,293 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.017 s
2016-03-03 12:22:11,293 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 14 ms on localhost (2/2)
2016-03-03 12:22:11,293 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.035786 s
2016-03-03 12:22:11,294 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,309 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 12:22:11,310 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 12:22:11,311 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 12:22:11,311 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:22:11,313 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:22:11,314 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:22:11,316 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 12:22:11,317 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 12:22:11,320 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 12:22:11,320 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 12:22:11,323 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:6030 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:22:11,324 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:11,324 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:22:11,325 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 12:22:11,326 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 12:22:11,328 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:22:11,329 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 12:22:11,333 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:22:11,337 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 12:22:11,341 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 13 ms on localhost (1/1)
2016-03-03 12:22:11,341 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.013 s
2016-03-03 12:22:11,341 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,342 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.032389 s
2016-03-03 12:22:11,376 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 12:22:11,380 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 12:22:11,381 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 12:22:11,382 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 12:22:11,382 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 12:22:11,385 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 12:22:11,388 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 12:22:11,392 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 12:22:11,393 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 12:22:11,396 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 12:22:11,396 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 12:22:11,398 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:6030 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 12:22:11,400 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:11,402 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 12:22:11,403 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 12:22:11,404 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 12:22:11,406 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:22:11,408 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:22:11,408 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 12:22:11,409 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 12:22:11,414 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:22:11,414 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:22:11,546 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 12:22:11,546 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 12:22:11,550 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 143 ms on localhost (1/2)
2016-03-03 12:22:11,551 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 146 ms on localhost (2/2)
2016-03-03 12:22:11,551 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.147 s
2016-03-03 12:22:11,552 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,553 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 12:22:11,555 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 12:22:11,555 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 12:22:11,556 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 12:22:11,609 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 12:22:11,614 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 12:22:11,616 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=272904, maxMem=278302556
2016-03-03 12:22:11,617 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 12:22:11,620 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=275144, maxMem=278302556
2016-03-03 12:22:11,621 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.1 MB)
2016-03-03 12:22:11,623 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:6030 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 12:22:11,624 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 12:22:11,625 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 12:22:11,625 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:6030 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:22:11,626 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 12:22:11,628 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 12:22:11,631 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,633 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,634 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,634 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:6030 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:22:11,635 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,636 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 12:22:11,637 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 12:22:11,637 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 12:22:11,637 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 12:22:11,640 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:6030 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:22:11,653 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:6030 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:22:11,658 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,658 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,658 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,658 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,660 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:22:11,660 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:22:11,660 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:22:11,660 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 12:22:11,683 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 12:22:11,683 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 12:22:11,683 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 12:22:11,683 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 12:22:11,685 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,688 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 12:22:11,689 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,691 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,692 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,693 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:22:11,692 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 12:22:11,693 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,693 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 12:22:11,697 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 12:22:11,700 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 12:22:11,704 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,705 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:22:11,701 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 66 ms on localhost (1/10)
2016-03-03 12:22:11,709 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 76 ms on localhost (2/10)
2016-03-03 12:22:11,706 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,705 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,714 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 10 ms
2016-03-03 12:22:11,713 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 12:22:11,712 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,711 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 12:22:11,720 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 12:22:11,720 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 88 ms on localhost (3/10)
2016-03-03 12:22:11,721 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 12:22:11,721 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 12:22:11,726 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,727 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:22:11,722 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 93 ms on localhost (4/10)
2016-03-03 12:22:11,730 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 45 ms on localhost (5/10)
2016-03-03 12:22:11,732 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:22:11,732 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 12:22:11,735 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 44 ms on localhost (6/10)
2016-03-03 12:22:11,736 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 12:22:11,737 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 48 ms on localhost (7/10)
2016-03-03 12:22:11,739 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 46 ms on localhost (8/10)
2016-03-03 12:22:11,741 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:22:11,743 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 12:22:11,742 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 31 ms on localhost (9/10)
2016-03-03 12:22:11,748 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 12:22:11,752 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 21 ms on localhost (10/10)
2016-03-03 12:22:11,753 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 12:22:11,752 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.123 s
2016-03-03 12:22:11,755 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.378375 s
2016-03-03 12:29:55,435 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 12:29:55,451 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 12:29:55,452 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 12:29:55,453 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 12:29:55,454 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 12:29:55,455 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 12:29:55,457 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 12:29:55,458 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 12:29:55,459 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 12:29:55,461 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 12:29:55,462 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 12:29:55,463 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 12:29:55,464 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 12:29:55,465 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 12:29:55,468 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 12:29:55,469 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 12:29:55,472 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 12:29:55,473 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 12:29:55,474 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 12:29:55,476 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 12:29:55,477 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 12:29:55,478 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 12:29:55,479 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 12:29:55,482 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 12:29:55,484 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 12:29:55,485 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 12:29:55,539 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 12:29:55,542 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 12:29:55,614 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 12:29:55,624 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec\blockmgr-26a88c7c-d624-48a3-b7af-06f25d12ffd8, already present as root for deletion.
2016-03-03 12:29:55,625 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 12:29:55,626 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 12:29:55,628 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 12:29:55,632 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 12:29:55,639 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 12:29:55,640 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 12:29:55,640 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 12:29:55,644 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-245b2466-47a8-43f1-aa4d-c8c0f671c371
2016-03-03 12:29:55,657 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 12:29:55,694 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 12:29:55,920 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-245b2466-47a8-43f1-aa4d-c8c0f671c371
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-245b2466-47a8-43f1-aa4d-c8c0f671c371
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 12:29:55,923 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e8e307e8-3d51-4f6f-b30e-d1c5b189e3ec
2016-03-03 12:30:29,400 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 12:30:32,260 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 12:30:32,262 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 12:30:32,263 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 12:30:33,045 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 12:30:33,097 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 12:30:33,290 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:6315]
2016-03-03 12:30:33,298 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 6315.
2016-03-03 12:30:33,317 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 12:30:33,332 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 12:30:33,358 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-fc305fd5-58ef-42c6-9151-4194db4c4e06\blockmgr-c5e86230-421c-4848-aaa5-1fff0c75d9bb
2016-03-03 12:30:33,366 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 12:30:33,445 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-fc305fd5-58ef-42c6-9151-4194db4c4e06\httpd-8b3d4233-101b-441f-bc66-e3636065b5fc
2016-03-03 12:30:33,453 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 12:30:33,528 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:30:33,552 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:6316
2016-03-03 12:30:33,555 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 6316.
2016-03-03 12:30:33,579 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 12:30:33,738 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:30:33,766 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 12:30:33,821 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 12:30:33,825 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 12:30:34,010 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:6316/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456979434009
2016-03-03 12:30:34,096 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:30:34,097 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:30:34,098 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:30:34,118 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 12:30:35,325 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6336.
2016-03-03 12:30:35,327 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 6336
2016-03-03 12:30:35,328 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 12:30:35,333 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:6336 with 265.4 MB RAM, BlockManagerId(driver, localhost, 6336)
2016-03-03 12:30:35,336 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 12:30:36,184 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 12:30:36,538 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:30:36,560 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:30:36,753 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:30:36,754 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:30:36,882 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:30:37,121 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:30:49,679 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:30:49,733 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:30:51,153 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:30:51,154 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:30:58,350 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:30:58,351 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:31:01,270 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:31:01,707 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 12:31:02,562 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:31:02,567 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:31:03,102 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:31:03,236 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:31:03,329 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 12:31:05,505 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 12:31:05,551 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:31:05,573 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:31:05,738 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:31:05,739 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:31:05,825 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:31:06,016 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:31:07,597 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:31:07,646 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:31:08,541 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:31:08,542 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:31:08,867 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:31:08,868 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:31:08,996 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 12:31:08,999 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:31:09,278 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:31:09,280 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:31:09,386 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:31:09,598 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:31:10,106 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 12:31:10,128 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 12:31:10,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 12:31:10,131 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:31:10,137 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:31:10,145 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 12:31:10,318 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 12:31:10,321 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 12:31:10,331 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 12:31:10,332 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 12:31:10,335 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:6336 (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:31:10,338 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:10,346 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 12:31:10,354 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 12:31:10,362 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 12:31:10,389 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:31:10,393 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:31:10,395 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:31:10,397 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:31:10,403 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 12:31:10,403 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 12:31:10,403 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 12:31:10,403 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 12:31:10,409 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:6316/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456979434009
2016-03-03 12:31:10,485 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:6316/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-fc305fd5-58ef-42c6-9151-4194db4c4e06\userFiles-3868641c-d806-4f89-8b8f-efc1315b3a83\fetchFileTemp5595415927562217580.tmp
2016-03-03 12:31:13,829 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-fc305fd5-58ef-42c6-9151-4194db4c4e06/userFiles-3868641c-d806-4f89-8b8f-efc1315b3a83/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 12:31:13,850 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 12:31:13,850 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 12:31:13,850 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 12:31:13,850 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 12:31:13,852 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:31:13,856 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 12:31:13,858 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:31:13,869 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 12:31:13,871 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:31:13,869 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 12:31:13,875 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:31:13,878 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 12:31:13,873 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 12:31:13,878 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 12:31:13,881 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3506 ms on localhost (1/10)
2016-03-03 12:31:13,883 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:31:13,884 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 12:31:13,883 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 12:31:13,886 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:31:13,885 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 12:31:13,893 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 12:31:13,897 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 12:31:13,899 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3505 ms on localhost (2/10)
2016-03-03 12:31:13,899 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 12:31:13,902 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 45 ms on localhost (3/10)
2016-03-03 12:31:13,904 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3508 ms on localhost (4/10)
2016-03-03 12:31:13,908 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 37 ms on localhost (5/10)
2016-03-03 12:31:13,909 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 57 ms on localhost (6/10)
2016-03-03 12:31:13,912 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3520 ms on localhost (7/10)
2016-03-03 12:31:13,913 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 39 ms on localhost (8/10)
2016-03-03 12:31:13,915 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 29 ms on localhost (9/10)
2016-03-03 12:31:13,916 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 34 ms on localhost (10/10)
2016-03-03 12:31:13,917 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 3.553 s
2016-03-03 12:31:13,918 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 12:31:13,927 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 3.820304 s
2016-03-03 12:31:13,968 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 12:31:13,969 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 12:31:14,006 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 12:31:14,007 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 12:31:14,009 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:6336 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 12:31:14,011 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 12:31:14,082 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 12:31:14,093 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 12:31:14,096 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 12:31:14,096 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 12:31:14,097 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:31:14,100 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:31:14,100 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:31:14,111 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 12:31:14,111 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:31:14,114 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1754) called with curMem=255250, maxMem=278302556
2016-03-03 12:31:14,115 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1754.0 B, free 265.2 MB)
2016-03-03 12:31:14,118 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:6336 (size: 1754.0 B, free: 265.4 MB)
2016-03-03 12:31:14,119 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:14,121 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:31:14,122 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 12:31:14,123 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 12:31:14,125 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:31:14,126 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:31:14,127 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 12:31:14,127 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 12:31:14,137 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:31:14,137 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:31:14,145 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 12:31:14,145 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 12:31:14,146 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 12:31:14,146 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 12:31:14,147 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 12:31:14,168 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 12:31:14,168 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 12:31:14,172 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 47 ms on localhost (1/2)
2016-03-03 12:31:14,173 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.049 s
2016-03-03 12:31:14,173 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 46 ms on localhost (2/2)
2016-03-03 12:31:14,173 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.079506 s
2016-03-03 12:31:14,174 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 12:31:14,179 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 12:31:14,180 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 12:31:14,181 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 12:31:14,181 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:31:14,184 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:31:14,185 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:31:14,187 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257004, maxMem=278302556
2016-03-03 12:31:14,188 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:31:14,314 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1754) called with curMem=259972, maxMem=278302556
2016-03-03 12:31:14,317 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1754.0 B, free 265.2 MB)
2016-03-03 12:31:14,322 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:6336 (size: 1754.0 B, free: 265.4 MB)
2016-03-03 12:31:14,326 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:14,328 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:31:14,331 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 12:31:14,336 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 12:31:14,341 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:31:14,347 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:31:14,350 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 12:31:14,350 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 12:31:14,358 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:31:14,360 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:31:14,366 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 12:31:14,366 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 12:31:14,370 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 26 ms on localhost (1/2)
2016-03-03 12:31:14,372 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 32 ms on localhost (2/2)
2016-03-03 12:31:14,373 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 12:31:14,372 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.032 s
2016-03-03 12:31:14,375 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.195383 s
2016-03-03 12:31:14,391 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 12:31:14,392 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 12:31:14,393 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 12:31:14,394 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:31:14,395 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:31:14,396 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:31:14,398 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261726, maxMem=278302556
2016-03-03 12:31:14,399 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 12:31:14,401 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1814) called with curMem=264862, maxMem=278302556
2016-03-03 12:31:14,402 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1814.0 B, free 265.2 MB)
2016-03-03 12:31:14,405 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:6336 (size: 1814.0 B, free: 265.4 MB)
2016-03-03 12:31:14,406 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:14,406 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:31:14,407 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 12:31:14,408 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 12:31:14,410 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:31:14,410 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 12:31:14,414 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:31:14,418 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 12:31:14,421 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 12 ms on localhost (1/1)
2016-03-03 12:31:14,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.012 s
2016-03-03 12:31:14,421 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 12:31:14,422 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.030888 s
2016-03-03 12:31:14,455 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 12:31:14,459 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 12:31:14,460 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 12:31:14,460 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 12:31:14,461 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 12:31:14,462 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 12:31:14,466 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 12:31:14,470 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266676, maxMem=278302556
2016-03-03 12:31:14,471 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 12:31:14,473 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2272) called with curMem=270628, maxMem=278302556
2016-03-03 12:31:14,474 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 12:31:14,476 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:6336 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 12:31:14,477 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:14,480 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 12:31:14,480 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 12:31:14,482 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 12:31:14,484 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:31:14,485 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:31:14,486 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 12:31:14,486 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 12:31:14,494 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:31:14,494 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:31:14,659 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:6336 in memory (size: 1814.0 B, free: 265.4 MB)
2016-03-03 12:31:14,671 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:6336 in memory (size: 1754.0 B, free: 265.4 MB)
2016-03-03 12:31:14,677 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 12:31:14,678 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:6336 in memory (size: 1754.0 B, free: 265.4 MB)
2016-03-03 12:31:14,677 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 12:31:14,683 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:6336 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:31:14,683 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 197 ms on localhost (1/2)
2016-03-03 12:31:14,685 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 203 ms on localhost (2/2)
2016-03-03 12:31:14,686 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 12:31:14,685 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.203 s
2016-03-03 12:31:14,688 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 12:31:14,689 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 12:31:14,690 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 12:31:14,691 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 12:31:14,694 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 12:31:14,697 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 12:31:14,699 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256606, maxMem=278302556
2016-03-03 12:31:14,700 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 12:31:14,703 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258846, maxMem=278302556
2016-03-03 12:31:14,703 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 12:31:14,706 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:6336 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 12:31:14,707 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 12:31:14,708 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 12:31:14,709 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 12:31:14,710 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 12:31:14,712 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,713 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,714 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,716 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,716 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 12:31:14,717 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 12:31:14,716 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 12:31:14,716 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 12:31:14,734 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,734 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,734 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,734 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,736 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:31:14,736 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:31:14,736 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:31:14,736 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:31:14,759 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 12:31:14,759 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 12:31:14,759 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 12:31:14,759 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 12:31:14,761 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,764 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 12:31:14,765 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,766 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 12:31:14,767 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,770 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,772 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,773 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:31:14,770 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 12:31:14,770 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,776 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 12:31:14,779 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 12:31:14,773 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 59 ms on localhost (1/10)
2016-03-03 12:31:14,784 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 12:31:14,785 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,787 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 12:31:14,772 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 12:31:14,791 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,792 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,792 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 12:31:14,787 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:31:14,795 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 12:31:14,781 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,797 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 16 ms
2016-03-03 12:31:14,795 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 80 ms on localhost (2/10)
2016-03-03 12:31:14,792 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 12:31:14,803 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 89 ms on localhost (3/10)
2016-03-03 12:31:14,802 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 12:31:14,801 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:31:14,798 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 12:31:14,810 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 12:31:14,808 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 12:31:14,807 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 95 ms on localhost (4/10)
2016-03-03 12:31:14,814 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 53 ms on localhost (5/10)
2016-03-03 12:31:14,816 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 50 ms on localhost (6/10)
2016-03-03 12:31:14,816 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 12:31:14,818 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 53 ms on localhost (7/10)
2016-03-03 12:31:14,820 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 35 ms on localhost (8/10)
2016-03-03 12:31:14,820 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 51 ms on localhost (9/10)
2016-03-03 12:31:14,822 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 35 ms on localhost (10/10)
2016-03-03 12:31:14,822 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.112 s
2016-03-03 12:31:14,823 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 12:31:14,824 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.368135 s
2016-03-03 12:31:43,071 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 12:31:43,092 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 12:31:43,093 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 12:31:43,095 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 12:31:43,104 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 12:31:43,122 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 12:31:43,123 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 12:31:43,128 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 12:31:43,129 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 12:31:43,129 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 12:31:43,131 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 12:31:43,132 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 12:31:43,133 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 12:31:43,134 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 12:31:43,136 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 12:31:43,136 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 12:31:43,137 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 12:31:43,138 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 12:31:43,139 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 12:31:43,140 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 12:31:43,141 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 12:31:43,142 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 12:31:43,143 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 12:31:43,144 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 12:31:43,144 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 12:31:43,145 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 12:31:43,201 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 12:31:43,209 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 12:31:43,315 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 12:31:43,324 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-fc305fd5-58ef-42c6-9151-4194db4c4e06\blockmgr-c5e86230-421c-4848-aaa5-1fff0c75d9bb, already present as root for deletion.
2016-03-03 12:31:43,326 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 12:31:43,328 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 12:31:43,331 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 12:31:43,334 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 12:31:43,339 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 12:31:43,340 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 12:31:43,341 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 12:31:43,341 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 12:31:43,344 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-77e6841d-c04f-4bcf-9248-38478e7658c7
2016-03-03 12:31:43,378 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 12:31:43,644 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-77e6841d-c04f-4bcf-9248-38478e7658c7
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-77e6841d-c04f-4bcf-9248-38478e7658c7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 12:31:43,647 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-fc305fd5-58ef-42c6-9151-4194db4c4e06
2016-03-03 12:32:06,730 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 12:32:09,786 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 12:32:09,788 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 12:32:09,789 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 12:32:10,543 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 12:32:10,588 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 12:32:10,752 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:6410]
2016-03-03 12:32:10,759 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 6410.
2016-03-03 12:32:10,777 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 12:32:10,791 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 12:32:10,817 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e72174cc-ad01-4f60-892c-76529628adf6\blockmgr-4eaeb908-241d-4d27-860d-dad9599fefe5
2016-03-03 12:32:10,823 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 12:32:10,890 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e72174cc-ad01-4f60-892c-76529628adf6\httpd-489d2067-dcd7-4374-ab49-ba4e744c19dc
2016-03-03 12:32:10,896 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 12:32:10,955 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:32:10,973 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:6411
2016-03-03 12:32:10,975 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 6411.
2016-03-03 12:32:10,997 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 12:32:11,130 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 12:32:11,173 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 12:32:11,175 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 12:32:11,180 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 12:32:11,313 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:6411/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456979531313
2016-03-03 12:32:11,374 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:32:11,375 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:32:11,375 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 12:32:11,390 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 12:32:12,525 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6430.
2016-03-03 12:32:12,526 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 6430
2016-03-03 12:32:12,528 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 12:32:12,533 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:6430 with 265.4 MB RAM, BlockManagerId(driver, localhost, 6430)
2016-03-03 12:32:12,536 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 12:32:13,362 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 12:32:13,693 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:32:13,715 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:32:13,896 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:32:13,898 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:32:14,032 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:32:14,271 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:32:22,846 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:32:22,891 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:32:24,102 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:24,103 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:31,273 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:31,274 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:33,317 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:32:33,674 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 12:32:34,676 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:32:34,681 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:32:35,171 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:32:35,263 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:32:35,346 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 12:32:37,660 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 12:32:37,710 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 12:32:37,734 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 12:32:37,885 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 12:32:37,885 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 12:32:37,978 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:32:38,171 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 12:32:39,363 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 12:32:39,404 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 12:32:40,202 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:40,203 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:40,514 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:40,515 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 12:32:40,631 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 12:32:40,633 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 12:32:40,864 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 12:32:40,865 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 12:32:40,953 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 12:32:41,123 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 12:32:41,557 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 12:32:41,576 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 12:32:41,577 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 12:32:41,578 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:32:41,582 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:32:41,590 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 12:32:41,715 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 12:32:41,719 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 12:32:41,728 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 12:32:41,729 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 12:32:41,732 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:6430 (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:32:41,734 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:41,740 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 12:32:41,741 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 12:32:41,749 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 12:32:41,778 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:32:41,782 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:32:41,783 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:32:41,784 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:32:41,789 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 12:32:41,789 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 12:32:41,789 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 12:32:41,789 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 12:32:41,795 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:6411/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456979531313
2016-03-03 12:32:41,875 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:6411/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e72174cc-ad01-4f60-892c-76529628adf6\userFiles-101cf336-3cc7-47e1-be87-41acc054f9a6\fetchFileTemp1193663841095540366.tmp
2016-03-03 12:32:45,210 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-e72174cc-ad01-4f60-892c-76529628adf6/userFiles-101cf336-3cc7-47e1-be87-41acc054f9a6/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 12:32:45,232 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 12:32:45,233 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 12:32:45,232 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 12:32:45,232 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 12:32:45,235 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:32:45,236 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 12:32:45,238 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:32:45,240 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 12:32:45,241 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:32:45,242 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 12:32:45,244 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:32:45,241 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 12:32:45,246 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 12:32:45,246 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 12:32:45,247 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 12:32:45,249 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 12:32:45,249 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 12:32:45,249 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 12:32:45,250 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 12:32:45,251 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 12:32:45,253 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 12:32:45,254 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 12:32:45,256 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3470 ms on localhost (1/10)
2016-03-03 12:32:45,257 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3475 ms on localhost (2/10)
2016-03-03 12:32:45,257 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3495 ms on localhost (3/10)
2016-03-03 12:32:45,259 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 24 ms on localhost (4/10)
2016-03-03 12:32:45,260 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3477 ms on localhost (5/10)
2016-03-03 12:32:45,261 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 17 ms on localhost (6/10)
2016-03-03 12:32:45,261 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 21 ms on localhost (7/10)
2016-03-03 12:32:45,262 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 25 ms on localhost (8/10)
2016-03-03 12:32:45,263 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 15 ms on localhost (9/10)
2016-03-03 12:32:45,264 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 17 ms on localhost (10/10)
2016-03-03 12:32:45,264 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 3.514 s
2016-03-03 12:32:45,265 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,270 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 3.712464 s
2016-03-03 12:32:45,305 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 12:32:45,305 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 12:32:45,339 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 12:32:45,339 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 12:32:45,340 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:6430 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 12:32:45,342 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 12:32:45,385 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 12:32:45,394 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 12:32:45,396 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 12:32:45,396 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 12:32:45,396 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:32:45,398 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:32:45,398 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:32:45,406 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 12:32:45,407 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:32:45,408 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 12:32:45,409 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:32:45,411 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:6430 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:32:45,411 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:45,413 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:32:45,414 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 12:32:45,414 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 12:32:45,416 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:32:45,417 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:32:45,418 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 12:32:45,418 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 12:32:45,427 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:32:45,427 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:32:45,435 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 12:32:45,435 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 12:32:45,435 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 12:32:45,436 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 12:32:45,436 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 12:32:45,455 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 12:32:45,455 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 12:32:45,458 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 41 ms on localhost (1/2)
2016-03-03 12:32:45,458 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.043 s
2016-03-03 12:32:45,458 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 43 ms on localhost (2/2)
2016-03-03 12:32:45,459 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.064514 s
2016-03-03 12:32:45,459 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,463 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 12:32:45,464 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 12:32:45,464 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 12:32:45,464 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:32:45,465 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:32:45,466 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:32:45,468 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 12:32:45,469 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 12:32:45,471 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 12:32:45,472 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 12:32:45,473 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:6430 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:32:45,473 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:45,474 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:32:45,474 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 12:32:45,475 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 12:32:45,476 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:32:45,478 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:32:45,478 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 12:32:45,478 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 12:32:45,481 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:32:45,485 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 12:32:45,488 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 11 ms on localhost (1/2)
2016-03-03 12:32:45,489 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:32:45,493 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 12:32:45,496 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 20 ms on localhost (2/2)
2016-03-03 12:32:45,496 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.021 s
2016-03-03 12:32:45,497 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,497 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.033928 s
2016-03-03 12:32:45,510 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 12:32:45,512 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 12:32:45,512 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 12:32:45,512 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 12:32:45,513 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 12:32:45,514 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 12:32:45,516 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 12:32:45,516 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 12:32:45,518 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 12:32:45,519 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 12:32:45,521 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:6430 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:32:45,522 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:45,522 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 12:32:45,522 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 12:32:45,523 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 12:32:45,524 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 12:32:45,525 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 12:32:45,528 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:32:45,532 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 12:32:45,536 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.013 s
2016-03-03 12:32:45,536 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 11 ms on localhost (1/1)
2016-03-03 12:32:45,536 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,536 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.025465 s
2016-03-03 12:32:45,565 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 12:32:45,569 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 12:32:45,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 12:32:45,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 12:32:45,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 12:32:45,571 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 12:32:45,574 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 12:32:45,578 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 12:32:45,579 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 12:32:45,630 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 12:32:45,632 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 12:32:45,634 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:6430 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 12:32:45,635 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:45,638 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 12:32:45,638 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 12:32:45,639 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 12:32:45,641 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:32:45,642 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 12:32:45,642 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:6430 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 12:32:45,643 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 12:32:45,643 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 12:32:45,649 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:6430 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:32:45,652 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 12:32:45,652 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 12:32:45,652 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:6430 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 12:32:45,656 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:6430 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 12:32:45,770 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 12:32:45,773 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 12:32:45,774 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 132 ms on localhost (1/2)
2016-03-03 12:32:45,775 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 135 ms on localhost (2/2)
2016-03-03 12:32:45,775 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.136 s
2016-03-03 12:32:45,776 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,776 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 12:32:45,777 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 12:32:45,777 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 12:32:45,778 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 12:32:45,780 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 12:32:45,782 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 12:32:45,784 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 12:32:45,784 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 12:32:45,786 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 12:32:45,787 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 12:32:45,789 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:6430 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 12:32:45,790 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 12:32:45,791 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 12:32:45,791 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 12:32:45,792 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 12:32:45,794 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,795 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,795 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,796 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,797 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 12:32:45,797 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 12:32:45,797 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 12:32:45,797 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 12:32:45,812 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,812 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,812 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,812 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,814 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:32:45,814 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:32:45,814 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:32:45,814 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 12:32:45,834 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 12:32:45,834 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 12:32:45,834 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 12:32:45,834 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 12:32:45,836 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,836 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 12:32:45,837 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,838 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 12:32:45,838 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 43 ms on localhost (1/10)
2016-03-03 12:32:45,840 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,841 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,842 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:32:45,842 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,843 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 12:32:45,846 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 12:32:45,845 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,845 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 12:32:45,847 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 12:32:45,851 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,852 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:32:45,853 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 12:32:45,854 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,856 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 60 ms on localhost (2/10)
2016-03-03 12:32:45,858 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 12:32:45,851 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,859 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 64 ms on localhost (3/10)
2016-03-03 12:32:45,859 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 12:32:45,858 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 12:32:45,856 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 12:32:45,859 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 22 ms on localhost (4/10)
2016-03-03 12:32:45,862 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 70 ms on localhost (5/10)
2016-03-03 12:32:45,866 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,868 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 12:32:45,869 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 12:32:45,870 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 12:32:45,873 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 12:32:45,859 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 12:32:45,874 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 12:32:45,868 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 32 ms on localhost (6/10)
2016-03-03 12:32:45,878 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 12:32:45,881 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 27 ms on localhost (7/10)
2016-03-03 12:32:45,883 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 39 ms on localhost (8/10)
2016-03-03 12:32:45,886 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 29 ms on localhost (9/10)
2016-03-03 12:32:45,887 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.094 s
2016-03-03 12:32:45,887 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 46 ms on localhost (10/10)
2016-03-03 12:32:45,888 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 12:32:45,888 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.321775 s
2016-03-03 12:39:29,600 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 12:39:29,615 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 12:39:29,620 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 12:39:29,620 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 12:39:29,622 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 12:39:29,623 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 12:39:29,623 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 12:39:29,624 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 12:39:29,625 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 12:39:29,625 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 12:39:29,626 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 12:39:29,627 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 12:39:29,628 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 12:39:29,628 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 12:39:29,629 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 12:39:29,630 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 12:39:29,632 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 12:39:29,633 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 12:39:29,633 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 12:39:29,634 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 12:39:29,635 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 12:39:29,636 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 12:39:29,637 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 12:39:29,638 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 12:39:29,638 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 12:39:29,639 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 12:39:29,691 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 12:39:29,694 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 12:39:29,763 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 12:39:29,769 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e72174cc-ad01-4f60-892c-76529628adf6\blockmgr-4eaeb908-241d-4d27-860d-dad9599fefe5, already present as root for deletion.
2016-03-03 12:39:29,771 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 12:39:29,772 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 12:39:29,773 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 12:39:29,775 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 12:39:29,779 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 12:39:29,782 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 12:39:29,782 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 12:39:29,784 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 12:39:29,785 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-41a2915a-c475-4442-8cb8-db7b4168471e
2016-03-03 12:39:29,816 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 12:39:30,093 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-41a2915a-c475-4442-8cb8-db7b4168471e
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-41a2915a-c475-4442-8cb8-db7b4168471e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 12:39:30,096 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e72174cc-ad01-4f60-892c-76529628adf6
2016-03-03 13:58:01,424 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 13:58:07,898 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 13:58:07,899 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 13:58:07,903 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 13:58:10,140 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 13:58:10,251 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 13:58:10,925 INFO  [sparkDriver-akka.actor.default-dispatcher-5][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:8746]
2016-03-03 13:58:10,932 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 8746.
2016-03-03 13:58:11,070 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 13:58:11,145 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 13:58:11,216 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-2da4db34-966b-4951-91ca-2c13de275823\blockmgr-be588b25-f98e-4db6-ac33-95cb40ec13de
2016-03-03 13:58:11,230 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 13:58:11,399 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-2da4db34-966b-4951-91ca-2c13de275823\httpd-70ab61ca-7b0e-4629-8f18-ade0fbc5e5e6
2016-03-03 13:58:11,439 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 13:58:11,539 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 13:58:11,582 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:8747
2016-03-03 13:58:11,584 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 8747.
2016-03-03 13:58:11,663 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 13:58:11,981 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 13:58:12,034 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 13:58:12,035 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 13:58:12,039 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 13:58:14,325 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:8747/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456984694323
2016-03-03 13:58:14,683 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 13:58:14,684 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 13:58:14,685 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 13:58:15,193 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 13:58:16,749 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8766.
2016-03-03 13:58:16,762 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 8766
2016-03-03 13:58:16,764 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 13:58:16,799 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:8766 with 265.4 MB RAM, BlockManagerId(driver, localhost, 8766)
2016-03-03 13:58:16,804 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 13:58:18,570 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 13:58:19,090 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 13:58:19,111 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 13:58:19,325 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 13:58:19,327 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 13:58:19,448 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 13:58:19,719 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 13:58:30,309 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 13:58:30,372 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 13:58:31,528 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:31,529 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:40,015 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:40,016 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:42,434 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 13:58:42,914 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 13:58:43,861 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 13:58:43,865 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 13:58:44,527 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 13:58:44,698 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 13:58:44,779 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 13:58:46,651 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 13:58:46,686 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 13:58:46,712 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 13:58:46,883 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 13:58:46,883 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 13:58:46,974 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 13:58:47,158 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 13:58:48,290 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 13:58:48,322 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 13:58:49,042 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:49,045 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:49,361 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:49,362 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 13:58:49,483 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 13:58:49,485 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 13:58:49,724 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 13:58:49,726 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 13:58:49,811 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 13:58:49,990 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 13:58:50,439 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 13:58:50,457 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 13:58:50,459 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 13:58:50,459 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 13:58:50,463 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 13:58:50,469 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 13:58:50,580 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 13:58:50,583 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 13:58:50,594 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 13:58:50,594 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 13:58:50,598 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:8766 (size: 804.0 B, free: 265.4 MB)
2016-03-03 13:58:50,600 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:50,607 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 13:58:50,610 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 13:58:50,621 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 13:58:50,655 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 13:58:50,658 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 13:58:50,660 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 13:58:50,661 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 13:58:50,666 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 13:58:50,666 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 13:58:50,666 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 13:58:50,666 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 13:58:50,673 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:8747/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456984694323
2016-03-03 13:58:50,736 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:8747/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-2da4db34-966b-4951-91ca-2c13de275823\userFiles-5051efeb-c324-45f9-a168-1dcd01617c01\fetchFileTemp3885585210237477387.tmp
2016-03-03 13:58:54,542 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-2da4db34-966b-4951-91ca-2c13de275823/userFiles-5051efeb-c324-45f9-a168-1dcd01617c01/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 13:58:54,569 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 13:58:54,569 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 13:58:54,569 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 13:58:54,569 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 13:58:54,573 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 13:58:54,574 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 13:58:54,575 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 13:58:54,579 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3916 ms on localhost (1/10)
2016-03-03 13:58:54,584 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 13:58:54,585 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 13:58:54,586 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 13:58:54,586 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 13:58:54,591 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 13:58:54,589 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 13:58:54,592 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 13:58:54,594 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 13:58:54,595 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 13:58:54,596 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 13:58:54,597 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 13:58:54,598 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 13:58:54,594 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 13:58:54,600 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 13:58:54,602 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3966 ms on localhost (2/10)
2016-03-03 13:58:54,603 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 13:58:54,604 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3947 ms on localhost (3/10)
2016-03-03 13:58:54,605 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 33 ms on localhost (4/10)
2016-03-03 13:58:54,607 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 14 ms on localhost (5/10)
2016-03-03 13:58:54,609 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 13 ms on localhost (6/10)
2016-03-03 13:58:54,609 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3950 ms on localhost (7/10)
2016-03-03 13:58:54,611 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 35 ms on localhost (8/10)
2016-03-03 13:58:54,611 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 28 ms on localhost (9/10)
2016-03-03 13:58:54,612 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 24 ms on localhost (10/10)
2016-03-03 13:58:54,612 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 3.990 s
2016-03-03 13:58:54,613 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 13:58:54,618 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 4.178667 s
2016-03-03 13:58:54,655 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 13:58:54,655 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 13:58:54,694 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 13:58:54,695 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 13:58:54,696 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:8766 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 13:58:54,697 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 13:58:54,742 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 13:58:54,750 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 13:58:54,752 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 13:58:54,752 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 13:58:54,752 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 13:58:54,754 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 13:58:54,754 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 13:58:54,761 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 13:58:54,762 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 13:58:54,764 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 13:58:54,765 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 13:58:54,767 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:8766 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 13:58:54,768 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:54,770 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 13:58:54,770 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 13:58:54,771 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 13:58:54,773 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 13:58:54,774 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 13:58:54,775 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 13:58:54,775 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 13:58:54,784 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 13:58:54,784 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 13:58:54,792 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 13:58:54,792 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 13:58:54,792 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 13:58:54,793 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 13:58:54,793 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 13:58:54,812 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 13:58:54,812 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 13:58:54,816 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 44 ms on localhost (1/2)
2016-03-03 13:58:54,817 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 43 ms on localhost (2/2)
2016-03-03 13:58:54,817 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.046 s
2016-03-03 13:58:54,818 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 13:58:54,818 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.067753 s
2016-03-03 13:58:54,823 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 13:58:54,824 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 13:58:54,824 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 13:58:54,824 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 13:58:54,826 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 13:58:54,826 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 13:58:54,828 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 13:58:54,829 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 13:58:54,831 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 13:58:54,832 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 13:58:54,833 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:8766 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 13:58:54,834 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:54,835 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 13:58:54,835 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 13:58:54,836 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 13:58:54,837 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 13:58:54,838 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 13:58:54,839 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 13:58:54,839 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 13:58:54,842 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 13:58:54,842 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 13:58:54,846 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 13:58:54,846 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 13:58:54,849 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 12 ms on localhost (1/2)
2016-03-03 13:58:54,849 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 12 ms on localhost (2/2)
2016-03-03 13:58:54,850 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.013 s
2016-03-03 13:58:54,850 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 13:58:54,850 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.027073 s
2016-03-03 13:58:54,863 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 13:58:54,864 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 13:58:54,865 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 13:58:54,865 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 13:58:54,866 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 13:58:54,867 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 13:58:54,868 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 13:58:54,869 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 13:58:54,871 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 13:58:54,872 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 13:58:54,874 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:8766 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 13:58:54,875 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:54,876 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 13:58:54,876 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 13:58:54,876 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 13:58:54,878 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 13:58:54,878 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 13:58:54,881 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 13:58:54,884 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 13:58:54,887 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 10 ms on localhost (1/1)
2016-03-03 13:58:54,887 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.010 s
2016-03-03 13:58:54,887 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 13:58:54,888 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.024189 s
2016-03-03 13:58:54,917 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 13:58:54,921 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 13:58:54,923 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 13:58:54,923 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 13:58:54,923 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 13:58:54,924 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 13:58:54,927 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 13:58:54,932 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 13:58:54,933 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 13:58:54,935 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 13:58:54,936 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 13:58:54,938 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:8766 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 13:58:54,939 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:54,941 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 13:58:54,942 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 13:58:54,943 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 13:58:54,945 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 13:58:54,946 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 13:58:54,946 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 13:58:54,946 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 13:58:55,005 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 13:58:55,007 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 13:58:55,013 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:8766 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 13:58:55,018 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:8766 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 13:58:55,021 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:8766 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 13:58:55,024 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:8766 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 13:58:55,128 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 13:58:55,128 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 13:58:55,131 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 188 ms on localhost (1/2)
2016-03-03 13:58:55,132 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 187 ms on localhost (2/2)
2016-03-03 13:58:55,132 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.189 s
2016-03-03 13:58:55,133 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 13:58:55,133 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 13:58:55,134 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 13:58:55,134 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 13:58:55,135 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 13:58:55,137 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 13:58:55,140 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 13:58:55,141 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 13:58:55,142 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 13:58:55,144 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 13:58:55,145 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 13:58:55,147 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:8766 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 13:58:55,149 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 13:58:55,149 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 13:58:55,150 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 13:58:55,150 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 13:58:55,152 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,153 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,154 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,156 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,156 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 13:58:55,156 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 13:58:55,156 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 13:58:55,156 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 13:58:55,173 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,173 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,173 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,173 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,175 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 13:58:55,175 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 13:58:55,175 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 13:58:55,175 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 13:58:55,194 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 13:58:55,194 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 13:58:55,194 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 13:58:55,194 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 13:58:55,196 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,196 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 13:58:55,197 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,198 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 13:58:55,198 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 47 ms on localhost (1/10)
2016-03-03 13:58:55,200 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,200 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,201 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 13:58:55,201 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 13:58:55,201 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,202 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,204 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 13:58:55,204 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 48 ms on localhost (2/10)
2016-03-03 13:58:55,207 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 52 ms on localhost (3/10)
2016-03-03 13:58:55,203 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 13:58:55,207 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 13:58:55,205 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,210 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 56 ms on localhost (4/10)
2016-03-03 13:58:55,209 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 13:58:55,210 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 13:58:55,211 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,212 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,212 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 13:58:55,213 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 13:58:55,215 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 13:58:55,213 INFO  [sparkDriver-akka.actor.default-dispatcher-5][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 13:58:55,217 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 13:58:55,217 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 13:58:55,219 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 20 ms on localhost (5/10)
2016-03-03 13:58:55,220 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 25 ms on localhost (6/10)
2016-03-03 13:58:55,222 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,223 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 13:58:55,224 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 27 ms on localhost (7/10)
2016-03-03 13:58:55,225 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 13:58:55,226 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 13:58:55,228 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 13:58:55,230 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 28 ms on localhost (8/10)
2016-03-03 13:58:55,230 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 13:58:55,232 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 18 ms on localhost (9/10)
2016-03-03 13:58:55,233 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 22 ms on localhost (10/10)
2016-03-03 13:58:55,234 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 13:58:55,233 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.082 s
2016-03-03 13:58:55,235 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.317835 s
2016-03-03 14:03:34,427 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:03:34,444 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:03:34,445 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:03:34,446 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:03:34,446 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:03:34,447 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:03:34,447 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:03:34,448 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:03:34,448 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:03:34,452 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:03:34,456 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:03:34,462 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:03:34,464 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:03:34,465 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:03:34,465 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:03:34,466 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:03:34,467 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:03:34,468 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:03:34,469 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:03:34,470 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:03:34,471 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:03:34,471 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:03:34,472 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:03:34,472 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:03:34,473 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:03:34,474 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:03:34,526 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:03:34,529 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:03:34,599 INFO  [sparkDriver-akka.actor.default-dispatcher-16][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:03:34,604 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-2da4db34-966b-4951-91ca-2c13de275823\blockmgr-be588b25-f98e-4db6-ac33-95cb40ec13de, already present as root for deletion.
2016-03-03 14:03:34,605 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:03:34,606 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:03:34,607 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:03:34,610 INFO  [sparkDriver-akka.actor.default-dispatcher-16][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:03:34,612 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:03:34,613 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:03:34,637 INFO  [sparkDriver-akka.actor.default-dispatcher-16][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:03:34,656 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:03:34,657 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:03:34,658 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-796da036-d7d0-4a63-be75-32947bcf20bc
2016-03-03 14:03:34,953 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-796da036-d7d0-4a63-be75-32947bcf20bc
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-796da036-d7d0-4a63-be75-32947bcf20bc
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:03:34,956 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-2da4db34-966b-4951-91ca-2c13de275823
2016-03-03 14:04:16,976 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:04:20,159 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:04:20,160 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:04:20,161 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:04:20,851 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:04:20,896 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 14:04:21,064 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:8969]
2016-03-03 14:04:21,069 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 8969.
2016-03-03 14:04:21,086 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:04:21,102 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:04:21,129 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-884ce466-654a-441d-bfe0-1b7628cb0433\blockmgr-c5417997-2609-4ce4-a98d-66821749e594
2016-03-03 14:04:21,135 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:04:21,206 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-884ce466-654a-441d-bfe0-1b7628cb0433\httpd-08345ec2-3058-4a35-b469-93bf636a48bf
2016-03-03 14:04:21,212 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:04:21,268 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:04:21,288 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:8971
2016-03-03 14:04:21,290 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 8971.
2016-03-03 14:04:21,307 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:04:21,423 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:04:21,445 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:04:21,446 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:04:21,448 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:04:21,635 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:8971/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985061634
2016-03-03 14:04:21,703 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:04:21,704 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:04:21,704 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:04:21,721 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:04:22,821 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8990.
2016-03-03 14:04:22,822 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 8990
2016-03-03 14:04:22,823 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:04:22,828 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:8990 with 265.4 MB RAM, BlockManagerId(driver, localhost, 8990)
2016-03-03 14:04:22,831 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:04:23,664 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:04:24,019 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:04:24,042 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:04:24,251 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:04:24,252 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:04:24,379 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:04:24,620 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:04:34,666 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:04:34,726 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:04:36,016 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:36,018 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:43,704 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:43,705 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:45,837 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:04:46,121 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:04:47,039 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:04:47,048 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:04:47,556 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:04:47,665 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:04:47,781 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:04:50,259 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:04:50,304 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:04:50,326 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:04:50,490 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:04:50,491 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:04:50,588 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:04:50,780 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:04:51,930 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:04:51,970 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:04:52,812 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:52,814 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:53,130 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:53,130 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:04:53,249 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:04:53,251 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:04:53,471 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:04:53,473 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:04:53,563 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:04:53,733 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:04:54,135 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 14:04:54,150 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 14:04:54,151 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 14:04:54,152 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:04:54,157 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:04:54,163 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 14:04:54,289 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:04:54,292 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:04:54,304 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:04:54,304 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:04:54,308 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:8990 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:04:54,316 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:54,322 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 14:04:54,325 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:04:54,333 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:04:54,364 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:04:54,366 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:04:54,368 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:04:54,370 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:04:54,374 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:04:54,374 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:04:54,374 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:04:54,374 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:04:54,380 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:8971/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985061634
2016-03-03 14:04:54,438 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:8971/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-884ce466-654a-441d-bfe0-1b7628cb0433\userFiles-778c3569-b1e2-4a67-8edc-57ee67f4471a\fetchFileTemp2873976428539326673.tmp
2016-03-03 14:04:57,867 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-884ce466-654a-441d-bfe0-1b7628cb0433/userFiles-778c3569-b1e2-4a67-8edc-57ee67f4471a/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:04:57,887 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:04:57,887 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:04:57,887 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:04:57,891 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:04:57,891 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:04:57,893 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:04:57,899 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:04:57,900 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:04:57,901 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:04:57,904 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:04:57,905 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:04:57,906 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:04:57,908 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3539 ms on localhost (1/10)
2016-03-03 14:04:57,909 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3543 ms on localhost (2/10)
2016-03-03 14:04:57,909 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:04:57,911 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:04:57,912 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:04:57,912 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:04:57,913 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:04:57,914 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:04:57,917 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:04:57,920 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:04:57,920 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:04:57,918 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3571 ms on localhost (3/10)
2016-03-03 14:04:57,923 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 30 ms on localhost (4/10)
2016-03-03 14:04:57,924 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 12 ms on localhost (5/10)
2016-03-03 14:04:57,925 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 15 ms on localhost (6/10)
2016-03-03 14:04:57,926 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:04:57,928 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3559 ms on localhost (7/10)
2016-03-03 14:04:57,928 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 15 ms on localhost (8/10)
2016-03-03 14:04:57,929 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 39 ms on localhost (9/10)
2016-03-03 14:04:57,930 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 27 ms on localhost (10/10)
2016-03-03 14:04:57,930 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 3.595 s
2016-03-03 14:04:57,931 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:04:57,936 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 3.800220 s
2016-03-03 14:04:57,973 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:04:57,974 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:04:58,006 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:04:58,007 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:04:58,008 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:8990 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:04:58,010 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 14:04:58,049 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:04:58,057 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 14:04:58,059 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 14:04:58,059 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 14:04:58,059 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:04:58,061 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:04:58,061 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:04:58,069 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:04:58,070 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:04:58,071 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 14:04:58,072 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 14:04:58,074 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:8990 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:04:58,075 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:58,077 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:04:58,077 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:04:58,078 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:04:58,080 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:04:58,081 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:04:58,081 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:04:58,081 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:04:58,091 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:04:58,091 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:04:58,098 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:04:58,099 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:04:58,099 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:04:58,099 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:04:58,100 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:04:58,120 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:04:58,120 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:04:58,123 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 45 ms on localhost (1/2)
2016-03-03 14:04:58,124 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.046 s
2016-03-03 14:04:58,124 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 44 ms on localhost (2/2)
2016-03-03 14:04:58,125 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.066881 s
2016-03-03 14:04:58,125 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:04:58,128 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 14:04:58,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 14:04:58,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 14:04:58,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:04:58,132 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:04:58,133 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:04:58,135 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 14:04:58,135 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:04:58,201 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 14:04:58,203 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 14:04:58,207 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:8990 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:04:58,210 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:58,212 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:04:58,213 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 14:04:58,215 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:04:58,219 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:04:58,223 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:04:58,225 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:04:58,225 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 14:04:58,237 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:04:58,237 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:04:58,247 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 14:04:58,247 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 14:04:58,251 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 31 ms on localhost (1/2)
2016-03-03 14:04:58,253 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.037 s
2016-03-03 14:04:58,253 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 35 ms on localhost (2/2)
2016-03-03 14:04:58,254 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.124842 s
2016-03-03 14:04:58,254 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:04:58,274 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 14:04:58,275 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 14:04:58,275 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 14:04:58,276 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:04:58,277 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:04:58,278 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:04:58,279 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 14:04:58,280 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:04:58,282 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 14:04:58,283 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 14:04:58,285 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:8990 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 14:04:58,286 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:58,286 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:04:58,287 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 14:04:58,288 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:04:58,289 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:04:58,290 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 14:04:58,293 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:04:58,297 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 14:04:58,299 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.011 s
2016-03-03 14:04:58,299 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 10 ms on localhost (1/1)
2016-03-03 14:04:58,301 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:04:58,301 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.027050 s
2016-03-03 14:04:58,336 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 14:04:58,340 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 14:04:58,342 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 14:04:58,342 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 14:04:58,342 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 14:04:58,343 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 14:04:58,346 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 14:04:58,352 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 14:04:58,353 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:04:58,355 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 14:04:58,356 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 14:04:58,358 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:8990 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:04:58,359 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:58,361 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 14:04:58,361 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 14:04:58,362 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:04:58,365 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:04:58,367 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:04:58,368 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:04:58,368 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:04:58,375 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:04:58,375 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:04:58,512 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:8990 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 14:04:58,532 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:8990 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:04:58,535 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:8990 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:04:58,540 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:8990 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:04:58,545 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 14:04:58,545 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 14:04:58,548 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 183 ms on localhost (1/2)
2016-03-03 14:04:58,549 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 186 ms on localhost (2/2)
2016-03-03 14:04:58,549 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.187 s
2016-03-03 14:04:58,549 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:04:58,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:04:58,551 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:04:58,551 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 14:04:58,552 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:04:58,554 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 14:04:58,557 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 14:04:58,558 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 14:04:58,559 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:04:58,561 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 14:04:58,562 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 14:04:58,564 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:8990 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 14:04:58,565 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 14:04:58,566 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 14:04:58,566 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 14:04:58,567 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:04:58,569 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,570 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,571 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,572 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,572 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 14:04:58,572 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 14:04:58,572 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 14:04:58,572 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 14:04:58,619 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,621 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 14:04:58,619 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,619 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,640 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 24 ms
2016-03-03 14:04:58,639 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 14:04:58,621 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,649 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 14:04:58,648 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 76 ms on localhost (1/10)
2016-03-03 14:04:58,640 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 23 ms
2016-03-03 14:04:58,651 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 30 ms
2016-03-03 14:04:58,656 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 14:04:58,658 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 14:04:58,964 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,966 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 14:04:58,971 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,975 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:58,977 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 14:04:58,976 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 14:04:58,997 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,997 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:04:58,999 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:59,001 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:04:59,002 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:04:58,981 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:58,980 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:59,005 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:59,005 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 14:04:59,010 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:59,011 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:04:59,015 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 14:04:59,007 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 437 ms on localhost (2/10)
2016-03-03 14:04:59,016 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 34 ms
2016-03-03 14:04:59,006 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 14:04:59,019 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 14:04:59,017 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 50 ms on localhost (3/10)
2016-03-03 14:04:59,024 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:59,025 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:04:59,025 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 453 ms on localhost (4/10)
2016-03-03 14:04:59,027 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:04:59,029 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 54 ms on localhost (5/10)
2016-03-03 14:04:59,024 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 14:04:59,032 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 463 ms on localhost (6/10)
2016-03-03 14:04:59,030 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 14:04:59,029 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 14:04:59,038 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 75 ms on localhost (7/10)
2016-03-03 14:04:59,040 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 62 ms on localhost (8/10)
2016-03-03 14:04:59,040 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:04:59,041 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:04:59,041 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 37 ms on localhost (9/10)
2016-03-03 14:04:59,045 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 14:04:59,051 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 24 ms on localhost (10/10)
2016-03-03 14:04:59,052 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:04:59,052 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.485 s
2016-03-03 14:04:59,053 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.716760 s
2016-03-03 14:05:33,073 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:05:33,105 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:05:33,106 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:05:33,111 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:05:33,112 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:05:33,113 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:05:33,113 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:05:33,114 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:05:33,114 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:05:33,115 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:05:33,116 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:05:33,116 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:05:33,117 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:05:33,117 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:05:33,118 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:05:33,118 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:05:33,119 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:05:33,119 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:05:33,120 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:05:33,121 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:05:33,122 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:05:33,122 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:05:33,123 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:05:33,123 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:05:33,124 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:05:33,124 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:05:33,178 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:05:33,183 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:05:33,276 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:05:33,287 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-884ce466-654a-441d-bfe0-1b7628cb0433\blockmgr-c5417997-2609-4ce4-a98d-66821749e594, already present as root for deletion.
2016-03-03 14:05:33,289 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:05:33,290 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:05:33,292 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:05:33,297 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:05:33,301 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:05:33,303 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:05:33,308 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:05:33,310 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:05:33,311 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-884ce466-654a-441d-bfe0-1b7628cb0433
2016-03-03 14:05:33,342 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:05:33,358 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-cf51dd7d-fa5d-4154-bb69-ba71113204eb
2016-03-03 14:05:33,520 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-cf51dd7d-fa5d-4154-bb69-ba71113204eb
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-cf51dd7d-fa5d-4154-bb69-ba71113204eb
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:06:29,663 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:06:32,855 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:06:32,856 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:06:32,857 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:06:33,561 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:06:33,607 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 14:06:33,776 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:9078]
2016-03-03 14:06:33,782 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 9078.
2016-03-03 14:06:33,800 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:06:33,814 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:06:33,838 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f0569bd3-956b-4758-aee9-0233a6b2c252\blockmgr-7d9de118-a964-4a9e-a2a1-becc6fef5d8f
2016-03-03 14:06:33,844 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:06:33,917 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f0569bd3-956b-4758-aee9-0233a6b2c252\httpd-fda71528-ff5c-4b11-bf31-1be80d9aa106
2016-03-03 14:06:33,923 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:06:33,999 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:06:34,023 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:9079
2016-03-03 14:06:34,025 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 9079.
2016-03-03 14:06:34,046 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:06:34,180 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:06:34,202 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:06:34,203 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:06:34,205 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:06:34,398 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:9079/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985194397
2016-03-03 14:06:34,460 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:06:34,461 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:06:34,462 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:06:34,476 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:06:35,581 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9098.
2016-03-03 14:06:35,582 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 9098
2016-03-03 14:06:35,583 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:06:35,587 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:9098 with 265.4 MB RAM, BlockManagerId(driver, localhost, 9098)
2016-03-03 14:06:35,590 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:06:36,398 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:06:36,767 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:06:36,788 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:06:36,973 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:06:36,974 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:06:37,099 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:06:37,332 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:06:45,743 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:06:45,789 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:06:47,113 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:06:47,114 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:06:55,200 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:06:55,201 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:06:57,205 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:06:57,523 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:06:58,347 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:06:58,350 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:06:58,778 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:06:58,902 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:06:58,972 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:07:01,112 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:07:01,154 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:07:01,176 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:07:01,357 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:07:01,357 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:07:01,444 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:07:01,632 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:07:02,788 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:07:02,830 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:07:03,677 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:07:03,679 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:07:04,046 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:07:04,047 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:07:04,168 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:07:04,170 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:07:04,405 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:07:04,406 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:07:04,489 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:07:04,659 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:07:05,080 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:54
2016-03-03 14:07:05,098 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:54) with 10 output partitions (allowLocal=false)
2016-03-03 14:07:05,099 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:54)
2016-03-03 14:07:05,100 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:07:05,105 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:07:05,111 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53), which has no missing parents
2016-03-03 14:07:05,233 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:07:05,235 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:07:05,244 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:07:05,245 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:07:05,248 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:9098 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:07:05,250 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:05,256 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:53)
2016-03-03 14:07:05,258 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:07:05,266 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:07:05,295 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:07:05,298 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:07:05,300 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:07:05,301 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:07:05,306 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:07:05,306 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:07:05,306 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:07:05,306 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:07:05,312 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:9079/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985194397
2016-03-03 14:07:05,371 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:9079/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f0569bd3-956b-4758-aee9-0233a6b2c252\userFiles-bba5ccbf-d3f8-4391-a686-9b8753ebf091\fetchFileTemp4236780684804614862.tmp
2016-03-03 14:07:09,285 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-f0569bd3-956b-4758-aee9-0233a6b2c252/userFiles-bba5ccbf-d3f8-4391-a686-9b8753ebf091/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:07:09,304 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:07:09,305 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:07:09,305 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:07:09,306 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:07:09,308 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:07:09,309 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:07:09,312 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4030 ms on localhost (1/10)
2016-03-03 14:07:09,314 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:07:09,315 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4015 ms on localhost (2/10)
2016-03-03 14:07:09,317 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:07:09,318 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:07:09,320 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:07:09,323 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:07:09,324 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:07:09,324 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:07:09,325 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4024 ms on localhost (3/10)
2016-03-03 14:07:09,324 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:07:09,327 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:07:09,328 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:07:09,330 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:07:09,330 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:07:09,331 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:07:09,333 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:07:09,334 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:07:09,335 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 28 ms on localhost (4/10)
2016-03-03 14:07:09,337 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 13 ms on localhost (5/10)
2016-03-03 14:07:09,337 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:07:09,338 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 19 ms on localhost (6/10)
2016-03-03 14:07:09,339 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 22 ms on localhost (7/10)
2016-03-03 14:07:09,339 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4042 ms on localhost (8/10)
2016-03-03 14:07:09,341 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 14 ms on localhost (9/10)
2016-03-03 14:07:09,342 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 14 ms on localhost (10/10)
2016-03-03 14:07:09,343 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:54) finished in 4.075 s
2016-03-03 14:07:09,344 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,350 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:54, took 4.269614 s
2016-03-03 14:07:09,382 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:07:09,383 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:07:09,414 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:07:09,414 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:07:09,416 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:9098 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:07:09,417 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:57
2016-03-03 14:07:09,469 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:07:09,477 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:58
2016-03-03 14:07:09,479 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:58) with 2 output partitions (allowLocal=false)
2016-03-03 14:07:09,479 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:58)
2016-03-03 14:07:09,480 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:07:09,481 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:07:09,482 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:07:09,489 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:07:09,489 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:07:09,491 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=255250, maxMem=278302556
2016-03-03 14:07:09,492 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 14:07:09,494 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:9098 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:07:09,495 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:09,497 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:07:09,498 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:07:09,498 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:07:09,500 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:07:09,501 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:07:09,502 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:07:09,502 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:07:09,511 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:07:09,511 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:07:09,518 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:07:09,518 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:07:09,519 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:07:09,519 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:07:09,519 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:07:09,539 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:07:09,539 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:07:09,542 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 40 ms on localhost (1/2)
2016-03-03 14:07:09,542 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:58) finished in 0.043 s
2016-03-03 14:07:09,542 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 43 ms on localhost (2/2)
2016-03-03 14:07:09,543 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,543 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:58, took 0.065708 s
2016-03-03 14:07:09,548 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:59
2016-03-03 14:07:09,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (count at Main.scala:59) with 2 output partitions (allowLocal=false)
2016-03-03 14:07:09,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(count at Main.scala:59)
2016-03-03 14:07:09,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:07:09,552 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:07:09,553 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:07:09,554 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=257005, maxMem=278302556
2016-03-03 14:07:09,555 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:07:09,558 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1755) called with curMem=259973, maxMem=278302556
2016-03-03 14:07:09,558 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1755.0 B, free 265.2 MB)
2016-03-03 14:07:09,559 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:9098 (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:07:09,560 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:09,561 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:07:09,561 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 2 tasks
2016-03-03 14:07:09,562 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:07:09,563 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:07:09,565 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 2.0 (TID 13, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:07:09,566 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 2.0 (TID 13)
2016-03-03 14:07:09,566 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:07:09,569 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:07:09,569 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:07:09,573 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1830 bytes result sent to driver
2016-03-03 14:07:09,574 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 2.0 (TID 13). 1830 bytes result sent to driver
2016-03-03 14:07:09,576 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 13 ms on localhost (1/2)
2016-03-03 14:07:09,577 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 2.0 (TID 13) in 13 ms on localhost (2/2)
2016-03-03 14:07:09,577 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (count at Main.scala:59) finished in 0.014 s
2016-03-03 14:07:09,578 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,579 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: count at Main.scala:59, took 0.029755 s
2016-03-03 14:07:09,592 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:60
2016-03-03 14:07:09,593 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (first at Main.scala:60) with 1 output partitions (allowLocal=true)
2016-03-03 14:07:09,594 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 3(first at Main.scala:60)
2016-03-03 14:07:09,594 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:07:09,595 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:07:09,596 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57), which has no missing parents
2016-03-03 14:07:09,597 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=261728, maxMem=278302556
2016-03-03 14:07:09,598 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:07:09,600 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1815) called with curMem=264864, maxMem=278302556
2016-03-03 14:07:09,601 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1815.0 B, free 265.2 MB)
2016-03-03 14:07:09,603 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:9098 (size: 1815.0 B, free: 265.4 MB)
2016-03-03 14:07:09,604 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:09,604 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at textFile at Main.scala:57)
2016-03-03 14:07:09,604 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 1 tasks
2016-03-03 14:07:09,605 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:07:09,606 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:07:09,607 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 14)
2016-03-03 14:07:09,610 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:07:09,613 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 14). 1821 bytes result sent to driver
2016-03-03 14:07:09,616 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 14) in 10 ms on localhost (1/1)
2016-03-03 14:07:09,616 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 3 (first at Main.scala:60) finished in 0.011 s
2016-03-03 14:07:09,617 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,617 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: first at Main.scala:60, took 0.024631 s
2016-03-03 14:07:09,646 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:62
2016-03-03 14:07:09,650 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 4 (map at Main.scala:61)
2016-03-03 14:07:09,651 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:62) with 10 output partitions (allowLocal=false)
2016-03-03 14:07:09,651 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:62)
2016-03-03 14:07:09,652 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 4)
2016-03-03 14:07:09,653 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 4)
2016-03-03 14:07:09,656 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61), which has no missing parents
2016-03-03 14:07:09,661 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=266679, maxMem=278302556
2016-03-03 14:07:09,661 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:07:09,714 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2273) called with curMem=270631, maxMem=278302556
2016-03-03 14:07:09,715 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.1 MB)
2016-03-03 14:07:09,720 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:9098 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:07:09,721 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:09,723 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at map at Main.scala:61)
2016-03-03 14:07:09,723 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 2 tasks
2016-03-03 14:07:09,724 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:07:09,725 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:9098 in memory (size: 1815.0 B, free: 265.4 MB)
2016-03-03 14:07:09,726 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:07:09,728 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:07:09,728 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:07:09,728 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:07:09,730 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:9098 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:07:09,733 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:9098 in memory (size: 1755.0 B, free: 265.4 MB)
2016-03-03 14:07:09,735 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:9098 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:07:09,736 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:07:09,736 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:07:09,850 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 2010 bytes result sent to driver
2016-03-03 14:07:09,850 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 2010 bytes result sent to driver
2016-03-03 14:07:09,853 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 126 ms on localhost (1/2)
2016-03-03 14:07:09,854 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 129 ms on localhost (2/2)
2016-03-03 14:07:09,854 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 4 (map at Main.scala:61) finished in 0.130 s
2016-03-03 14:07:09,854 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,855 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:07:09,856 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:07:09,856 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 5)
2016-03-03 14:07:09,857 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:07:09,859 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 5: List()
2016-03-03 14:07:09,862 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61), which is now runnable
2016-03-03 14:07:09,863 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256607, maxMem=278302556
2016-03-03 14:07:09,864 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:07:09,866 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1361) called with curMem=258847, maxMem=278302556
2016-03-03 14:07:09,867 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 1361.0 B, free 265.2 MB)
2016-03-03 14:07:09,869 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:9098 (size: 1361.0 B, free: 265.4 MB)
2016-03-03 14:07:09,870 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 6 from broadcast at DAGScheduler.scala:874
2016-03-03 14:07:09,870 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 5 (ShuffledRDD[5] at reduceByKey at Main.scala:61)
2016-03-03 14:07:09,871 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 10 tasks
2016-03-03 14:07:09,871 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:07:09,873 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,875 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,876 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 5.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,877 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,877 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 5.0 (TID 19)
2016-03-03 14:07:09,877 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 3.0 in stage 5.0 (TID 20)
2016-03-03 14:07:09,877 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 18)
2016-03-03 14:07:09,877 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 17)
2016-03-03 14:07:09,892 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,892 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,892 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,892 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,894 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 14:07:09,894 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 14:07:09,894 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:07:09,894 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:07:09,914 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 18). 1293 bytes result sent to driver
2016-03-03 14:07:09,914 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 5.0 (TID 19). 1280 bytes result sent to driver
2016-03-03 14:07:09,914 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 17). 1296 bytes result sent to driver
2016-03-03 14:07:09,914 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 3.0 in stage 5.0 (TID 20). 1277 bytes result sent to driver
2016-03-03 14:07:09,916 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,917 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 5.0 (TID 21)
2016-03-03 14:07:09,918 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,919 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 5.0 (TID 22)
2016-03-03 14:07:09,919 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,920 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 5.0 (TID 23)
2016-03-03 14:07:09,923 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,923 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:07:09,924 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,925 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:07:09,920 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,927 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 5.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:07:09,925 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 5.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,929 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 5.0 (TID 24)
2016-03-03 14:07:09,929 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 5.0 (TID 23). 1275 bytes result sent to driver
2016-03-03 14:07:09,931 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,928 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 14:07:09,932 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,933 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 5.0 (TID 25)
2016-03-03 14:07:09,932 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:07:09,933 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:07:09,934 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 18) in 60 ms on localhost (1/10)
2016-03-03 14:07:09,937 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 5.0 (TID 22) in 20 ms on localhost (2/10)
2016-03-03 14:07:09,937 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 5.0 (TID 24). 1275 bytes result sent to driver
2016-03-03 14:07:09,935 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 5.0 (TID 26)
2016-03-03 14:07:09,935 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 5.0 (TID 21). 1296 bytes result sent to driver
2016-03-03 14:07:09,938 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 5.0 (TID 19) in 63 ms on localhost (3/10)
2016-03-03 14:07:09,942 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 5.0 (TID 23) in 22 ms on localhost (4/10)
2016-03-03 14:07:09,942 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 5.0 (TID 20) in 66 ms on localhost (5/10)
2016-03-03 14:07:09,937 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,943 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 17) in 71 ms on localhost (6/10)
2016-03-03 14:07:09,942 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:07:09,944 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:07:09,944 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 5.0 (TID 24) in 23 ms on localhost (7/10)
2016-03-03 14:07:09,943 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:07:09,945 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 5.0 (TID 21) in 29 ms on localhost (8/10)
2016-03-03 14:07:09,948 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 5.0 (TID 26). 1334 bytes result sent to driver
2016-03-03 14:07:09,950 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 5.0 (TID 25). 1319 bytes result sent to driver
2016-03-03 14:07:09,950 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 5.0 (TID 26) in 18 ms on localhost (9/10)
2016-03-03 14:07:09,952 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 5.0 (TID 25) in 22 ms on localhost (10/10)
2016-03-03 14:07:09,952 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:62) finished in 0.080 s
2016-03-03 14:07:09,953 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:07:09,953 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:62, took 0.306620 s
2016-03-03 14:18:41,603 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:18:41,635 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:18:41,638 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:18:41,641 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:18:41,643 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:18:41,645 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:18:41,646 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:18:41,647 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:18:41,648 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:18:41,649 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:18:41,650 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:18:41,651 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:18:41,652 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:18:41,652 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:18:41,653 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:18:41,654 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:18:41,655 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:18:41,656 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:18:41,657 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:18:41,658 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:18:41,659 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:18:41,660 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:18:41,660 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:18:41,661 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:18:41,661 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:18:41,661 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:18:41,718 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:18:41,725 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:18:41,824 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:18:41,836 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f0569bd3-956b-4758-aee9-0233a6b2c252\blockmgr-7d9de118-a964-4a9e-a2a1-becc6fef5d8f, already present as root for deletion.
2016-03-03 14:18:41,838 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:18:41,839 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:18:41,841 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:18:41,845 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:18:41,848 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:18:41,852 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:18:41,856 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:18:41,857 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:18:41,860 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-35d01d2a-2fd2-4be6-acc1-9cac6002c68c
2016-03-03 14:18:41,894 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:18:42,090 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-35d01d2a-2fd2-4be6-acc1-9cac6002c68c
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-35d01d2a-2fd2-4be6-acc1-9cac6002c68c
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:18:42,092 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f0569bd3-956b-4758-aee9-0233a6b2c252
2016-03-03 14:18:47,013 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:18:50,097 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:18:50,099 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:18:50,100 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:18:50,830 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:18:50,875 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 14:18:51,047 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:9460]
2016-03-03 14:18:51,054 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 9460.
2016-03-03 14:18:51,071 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:18:51,086 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:18:51,110 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-beacbfb4-5b77-48c7-bc73-784762f014c0\blockmgr-8affea95-373e-4e6b-ae56-db0cc6b6ee6d
2016-03-03 14:18:51,116 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:18:51,189 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-beacbfb4-5b77-48c7-bc73-784762f014c0\httpd-64b3b767-5fb2-47fc-ad3b-8f4e612e0825
2016-03-03 14:18:51,195 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:18:51,260 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:18:51,283 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:9461
2016-03-03 14:18:51,285 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 9461.
2016-03-03 14:18:51,301 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:18:51,429 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:18:51,473 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:18:51,474 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:18:51,478 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:18:51,647 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:9461/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985931647
2016-03-03 14:18:51,743 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:18:51,744 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:18:51,745 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:18:51,764 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:18:52,929 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9480.
2016-03-03 14:18:52,930 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 9480
2016-03-03 14:18:52,931 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:18:52,935 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:9480 with 265.4 MB RAM, BlockManagerId(driver, localhost, 9480)
2016-03-03 14:18:52,938 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:18:53,800 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:18:54,159 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:18:54,181 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:18:54,362 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:18:54,363 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:18:54,488 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:18:54,728 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:19:06,343 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:19:06,405 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:19:07,758 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:07,759 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:15,397 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:15,398 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:17,745 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:19:18,003 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:19:19,147 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:19:19,152 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:19:19,674 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:19:19,771 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:19:19,861 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:19:23,341 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:19:23,390 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:19:23,415 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:19:23,558 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:19:23,559 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:19:23,661 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:19:23,861 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:19:25,119 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:19:25,153 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:19:25,904 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:25,905 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:26,260 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:26,260 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:19:26,385 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:19:26,387 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:19:26,611 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:19:26,613 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:19:26,683 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:19:26,847 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:19:27,246 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:55
2016-03-03 14:19:27,262 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:55) with 10 output partitions (allowLocal=false)
2016-03-03 14:19:27,263 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:55)
2016-03-03 14:19:27,264 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:19:27,267 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:19:27,274 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:54), which has no missing parents
2016-03-03 14:19:27,397 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:19:27,400 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:19:27,411 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:19:27,412 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:19:27,416 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:9480 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:19:27,419 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:19:27,426 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:54)
2016-03-03 14:19:27,428 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:19:27,437 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:19:27,467 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:19:27,470 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:19:27,471 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:19:27,472 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:19:27,478 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:19:27,478 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:19:27,478 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:19:27,478 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:19:27,486 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:9461/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456985931647
2016-03-03 14:19:27,575 INFO  [Executor task launch worker-2][org.apache.spark.util.Utils] Fetching http://169.254.236.187:9461/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-beacbfb4-5b77-48c7-bc73-784762f014c0\userFiles-41708ff5-cc59-4c7c-a5a8-dfa4c1403e56\fetchFileTemp8032822209030070461.tmp
2016-03-03 14:19:31,261 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-beacbfb4-5b77-48c7-bc73-784762f014c0/userFiles-41708ff5-cc59-4c7c-a5a8-dfa4c1403e56/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:19:31,281 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:19:31,281 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:19:31,282 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:19:31,282 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:19:31,285 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:19:31,286 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:19:31,289 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:19:31,289 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3817 ms on localhost (1/10)
2016-03-03 14:19:31,297 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3825 ms on localhost (2/10)
2016-03-03 14:19:31,299 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:19:31,300 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:19:31,302 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:19:31,303 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:19:31,303 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:19:31,305 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:19:31,306 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:19:31,306 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3837 ms on localhost (3/10)
2016-03-03 14:19:31,307 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 22 ms on localhost (4/10)
2016-03-03 14:19:31,308 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:19:31,309 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:19:31,309 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:19:31,310 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:19:31,311 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3860 ms on localhost (5/10)
2016-03-03 14:19:31,312 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:19:31,314 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:19:31,314 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:19:31,314 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 16 ms on localhost (6/10)
2016-03-03 14:19:31,318 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:19:31,321 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 21 ms on localhost (7/10)
2016-03-03 14:19:31,323 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 15 ms on localhost (8/10)
2016-03-03 14:19:31,324 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 20 ms on localhost (9/10)
2016-03-03 14:19:31,325 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 12 ms on localhost (10/10)
2016-03-03 14:19:31,325 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:55) finished in 3.888 s
2016-03-03 14:19:31,326 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:19:31,332 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:55, took 4.085142 s
2016-03-03 14:19:32,649 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:19:32,650 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:19:32,685 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:19:32,686 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:19:32,687 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:9480 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:19:32,689 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:60
2016-03-03 14:19:32,730 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:19:32,740 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:61
2016-03-03 14:19:32,742 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:61) with 2 output partitions (allowLocal=false)
2016-03-03 14:19:32,743 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:61)
2016-03-03 14:19:32,743 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:19:32,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:19:32,745 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:60), which has no missing parents
2016-03-03 14:19:32,751 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:19:32,752 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:19:32,754 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1758) called with curMem=255250, maxMem=278302556
2016-03-03 14:19:32,755 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1758.0 B, free 265.2 MB)
2016-03-03 14:19:32,758 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:9480 (size: 1758.0 B, free: 265.4 MB)
2016-03-03 14:19:32,759 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:19:32,761 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:60)
2016-03-03 14:19:32,762 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:19:32,762 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:19:32,765 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:19:32,766 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:19:32,767 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:19:32,767 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:19:32,779 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:19:32,779 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:19:32,789 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:19:32,789 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:19:32,789 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:19:32,790 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:19:32,790 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:19:32,886 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:9480 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:19:32,886 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:19:32,886 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:19:32,890 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 127 ms on localhost (1/2)
2016-03-03 14:19:32,891 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 125 ms on localhost (2/2)
2016-03-03 14:19:32,892 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:19:32,892 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:61) finished in 0.128 s
2016-03-03 14:19:32,893 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:61, took 0.152157 s
2016-03-03 14:19:32,907 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:62
2016-03-03 14:19:32,909 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:62) with 1 output partitions (allowLocal=true)
2016-03-03 14:19:32,910 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:62)
2016-03-03 14:19:32,910 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:19:32,911 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:19:32,912 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:60), which has no missing parents
2016-03-03 14:19:32,914 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255108, maxMem=278302556
2016-03-03 14:19:32,915 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:19:32,925 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1818) called with curMem=258244, maxMem=278302556
2016-03-03 14:19:32,926 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1818.0 B, free 265.2 MB)
2016-03-03 14:19:32,927 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:9480 (size: 1818.0 B, free: 265.4 MB)
2016-03-03 14:19:32,928 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:19:32,929 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:60)
2016-03-03 14:19:32,929 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:19:32,930 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:19:32,931 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:19:32,932 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:19:32,937 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:19:32,942 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1821 bytes result sent to driver
2016-03-03 14:19:32,944 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:62) finished in 0.014 s
2016-03-03 14:19:32,944 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 14 ms on localhost (1/1)
2016-03-03 14:19:32,945 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:62, took 0.037318 s
2016-03-03 14:19:32,946 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:19:32,995 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:64
2016-03-03 14:19:33,001 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:63)
2016-03-03 14:19:33,002 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:64) with 10 output partitions (allowLocal=false)
2016-03-03 14:19:33,003 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:64)
2016-03-03 14:19:33,003 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:19:33,005 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:19:33,009 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:63), which has no missing parents
2016-03-03 14:19:33,015 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260062, maxMem=278302556
2016-03-03 14:19:33,016 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:19:33,019 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2277) called with curMem=264014, maxMem=278302556
2016-03-03 14:19:33,019 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:19:33,022 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:9480 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:19:33,023 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:19:33,026 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:63)
2016-03-03 14:19:33,026 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:19:33,027 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:19:33,029 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:19:33,031 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:19:33,031 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:19:33,031 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:19:33,039 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1511+1511
2016-03-03 14:19:33,039 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1511
2016-03-03 14:19:33,197 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:19:33,197 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:19:33,201 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 173 ms on localhost (1/2)
2016-03-03 14:19:33,203 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 173 ms on localhost (2/2)
2016-03-03 14:19:33,203 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:63) finished in 0.176 s
2016-03-03 14:19:33,204 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:19:33,205 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:19:33,206 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:19:33,206 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:19:33,207 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:19:33,210 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:19:33,214 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:63), which is now runnable
2016-03-03 14:19:33,216 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266291, maxMem=278302556
2016-03-03 14:19:33,216 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:19:33,219 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268531, maxMem=278302556
2016-03-03 14:19:33,220 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:19:33,224 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:9480 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:19:33,225 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:19:33,227 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:63)
2016-03-03 14:19:33,227 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:19:33,228 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:19:33,231 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,233 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,234 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,236 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,237 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:19:33,237 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:19:33,237 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:19:33,237 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:19:33,256 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,256 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,256 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,256 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,259 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:19:33,259 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:19:33,259 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:19:33,259 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:19:33,280 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1293 bytes result sent to driver
2016-03-03 14:19:33,280 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 14:19:33,282 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,280 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1280 bytes result sent to driver
2016-03-03 14:19:33,280 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:19:33,284 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:19:33,284 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,286 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:19:33,289 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,290 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:19:33,290 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,290 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 58 ms on localhost (1/10)
2016-03-03 14:19:33,291 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:19:33,294 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 14:19:33,294 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 14:19:33,296 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,297 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:19:33,298 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,299 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:19:33,299 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 70 ms on localhost (2/10)
2016-03-03 14:19:33,303 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,304 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,305 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,306 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:19:33,305 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:19:33,307 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:19:33,311 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:19:33,307 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:19:33,313 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:19:33,313 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,314 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:19:33,317 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:19:33,317 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:19:33,320 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:19:33,324 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1334 bytes result sent to driver
2016-03-03 14:19:33,326 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1275 bytes result sent to driver
2016-03-03 14:19:33,327 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 94 ms on localhost (3/10)
2016-03-03 14:19:33,329 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 48 ms on localhost (4/10)
2016-03-03 14:19:33,330 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 95 ms on localhost (5/10)
2016-03-03 14:19:33,331 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 48 ms on localhost (6/10)
2016-03-03 14:19:33,333 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 28 ms on localhost (7/10)
2016-03-03 14:19:33,333 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 35 ms on localhost (8/10)
2016-03-03 14:19:33,334 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 39 ms on localhost (9/10)
2016-03-03 14:19:33,335 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 29 ms on localhost (10/10)
2016-03-03 14:19:33,336 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:19:33,336 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:64) finished in 0.106 s
2016-03-03 14:19:33,338 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:64, took 0.341461 s
2016-03-03 14:20:30,793 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:20:30,813 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:20:30,815 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:20:30,817 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:20:30,818 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:20:30,820 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:20:30,821 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:20:30,823 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:20:30,824 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:20:30,827 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:20:30,829 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:20:30,830 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:20:30,832 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:20:30,833 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:20:30,834 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:20:30,836 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:20:30,838 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:20:30,839 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:20:30,840 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:20:30,843 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:20:30,846 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:20:30,847 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:20:30,848 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:20:30,850 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:20:30,851 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:20:30,852 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:20:30,906 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:20:30,910 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:20:30,988 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:20:30,999 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-beacbfb4-5b77-48c7-bc73-784762f014c0\blockmgr-8affea95-373e-4e6b-ae56-db0cc6b6ee6d, already present as root for deletion.
2016-03-03 14:20:31,000 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:20:31,002 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:20:31,003 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:20:31,006 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:20:31,011 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:20:31,012 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:20:31,013 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-beacbfb4-5b77-48c7-bc73-784762f014c0
2016-03-03 14:20:31,030 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:20:31,036 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:20:31,045 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f5e59b69-5abd-4424-bc83-de6e1f721ad9
2016-03-03 14:20:31,054 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:20:31,212 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f5e59b69-5abd-4424-bc83-de6e1f721ad9
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f5e59b69-5abd-4424-bc83-de6e1f721ad9
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:33:14,211 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:33:17,410 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:33:17,411 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:33:17,412 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:33:18,134 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:33:18,182 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 14:33:18,353 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:9967]
2016-03-03 14:33:18,359 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 9967.
2016-03-03 14:33:18,376 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:33:18,390 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:33:18,415 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9114bee6-ebab-4cfc-8906-a0bce88006c1\blockmgr-8942c1a8-5bf6-48fd-bed5-b42cf8e5f853
2016-03-03 14:33:18,421 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:33:18,494 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9114bee6-ebab-4cfc-8906-a0bce88006c1\httpd-b799725c-5582-410d-897e-bee75732aeac
2016-03-03 14:33:18,499 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:33:18,560 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:33:18,581 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:9968
2016-03-03 14:33:18,583 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 9968.
2016-03-03 14:33:18,605 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:33:18,734 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:33:18,752 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:33:18,753 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:33:18,792 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:33:18,936 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:9968/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456986798935
2016-03-03 14:33:19,000 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:33:19,001 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:33:19,002 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:33:19,017 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:33:20,141 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9988.
2016-03-03 14:33:20,142 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 9988
2016-03-03 14:33:20,143 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:33:20,147 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:9988 with 265.4 MB RAM, BlockManagerId(driver, localhost, 9988)
2016-03-03 14:33:20,150 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:33:21,003 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:33:21,395 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:33:21,424 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:33:21,639 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:33:21,640 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:33:21,773 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:33:22,011 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:33:31,508 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:33:31,565 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:33:32,683 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:32,684 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:38,929 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:38,930 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:41,252 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:33:41,633 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:33:42,605 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:33:42,610 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:33:43,092 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:33:43,195 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:33:43,271 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:33:45,498 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:33:45,555 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:33:45,577 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:33:45,744 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:33:45,745 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:33:45,842 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:33:46,038 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:33:47,300 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:33:47,344 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:33:48,159 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:48,160 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:48,464 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:48,465 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:33:48,585 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:33:48,587 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:33:48,830 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:33:48,831 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:33:48,909 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:33:49,075 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:33:49,556 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 14:33:49,573 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 14:33:49,574 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 14:33:49,575 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:33:49,579 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:33:49,585 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 14:33:49,736 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:33:49,739 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:33:49,752 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:33:49,752 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:33:49,756 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:9988 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:33:49,759 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:49,765 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 14:33:49,767 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:33:49,777 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:33:49,814 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:33:49,817 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:33:49,818 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:33:49,819 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:33:49,824 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:33:49,824 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:33:49,824 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:33:49,824 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:33:49,830 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:9968/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456986798935
2016-03-03 14:33:49,908 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:9968/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9114bee6-ebab-4cfc-8906-a0bce88006c1\userFiles-5a382959-929d-4a16-9075-8aa42d4eca70\fetchFileTemp6683604873896052363.tmp
2016-03-03 14:33:54,350 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-9114bee6-ebab-4cfc-8906-a0bce88006c1/userFiles-5a382959-929d-4a16-9075-8aa42d4eca70/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:33:54,369 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:33:54,370 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:33:54,370 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:33:54,369 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:33:54,373 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:33:54,374 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:33:54,376 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:33:54,379 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:33:54,378 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:33:54,380 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:33:54,383 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:33:54,384 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:33:54,385 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1387 bytes)
2016-03-03 14:33:54,386 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:33:54,388 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:33:54,388 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4593 ms on localhost (1/10)
2016-03-03 14:33:54,390 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 17 ms on localhost (2/10)
2016-03-03 14:33:54,389 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:33:54,391 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:33:54,391 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4572 ms on localhost (3/10)
2016-03-03 14:33:54,392 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4575 ms on localhost (4/10)
2016-03-03 14:33:54,390 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:33:54,393 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:33:54,393 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4577 ms on localhost (5/10)
2016-03-03 14:33:54,395 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1391 bytes)
2016-03-03 14:33:54,396 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:33:54,396 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 16 ms on localhost (6/10)
2016-03-03 14:33:54,401 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:33:54,403 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 27 ms on localhost (7/10)
2016-03-03 14:33:54,404 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 9 ms on localhost (8/10)
2016-03-03 14:33:54,405 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 20 ms on localhost (9/10)
2016-03-03 14:33:54,406 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 22 ms on localhost (10/10)
2016-03-03 14:33:54,406 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 4.627 s
2016-03-03 14:33:54,406 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:33:54,413 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.856711 s
2016-03-03 14:33:54,775 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:33:54,776 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:33:54,819 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:33:54,820 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:33:54,821 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:9988 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:33:54,822 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 14:33:54,872 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:33:54,882 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 14:33:54,884 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 14:33:54,885 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 14:33:54,885 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:33:54,886 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:33:54,887 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:33:54,893 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:33:54,893 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:33:54,967 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 14:33:54,968 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 14:33:54,973 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:9988 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:33:54,975 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:54,977 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:33:54,978 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:33:54,978 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:33:54,979 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:9988 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:33:54,981 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:33:54,982 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:33:54,983 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:33:54,983 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:33:54,996 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1526+1527
2016-03-03 14:33:54,996 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1526
2016-03-03 14:33:55,007 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:33:55,007 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:33:55,007 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:33:55,007 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:33:55,008 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:33:55,034 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:33:55,034 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:33:55,038 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 59 ms on localhost (1/2)
2016-03-03 14:33:55,039 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 57 ms on localhost (2/2)
2016-03-03 14:33:55,040 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.061 s
2016-03-03 14:33:55,040 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:33:55,041 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.157746 s
2016-03-03 14:33:55,054 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 14:33:55,055 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 14:33:55,055 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 14:33:55,056 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:33:55,057 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:33:55,057 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:33:55,060 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 14:33:55,060 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:33:55,146 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 14:33:55,146 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 14:33:55,148 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:9988 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:33:55,149 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:55,149 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:33:55,149 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:33:55,150 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:33:55,152 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:33:55,155 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:33:55,159 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1526
2016-03-03 14:33:55,162 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1824 bytes result sent to driver
2016-03-03 14:33:55,170 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 18 ms on localhost (1/1)
2016-03-03 14:33:55,170 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:33:55,175 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.025 s
2016-03-03 14:33:55,176 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.121246 s
2016-03-03 14:33:55,225 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 14:33:55,231 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 14:33:55,232 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 14:33:55,232 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 14:33:55,232 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:33:55,234 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:33:55,238 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 14:33:55,243 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 14:33:55,244 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:33:55,246 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264012, maxMem=278302556
2016-03-03 14:33:55,247 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:33:55,249 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:9988 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:33:55,251 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:55,254 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 14:33:55,255 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:33:55,256 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:33:55,258 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:33:55,259 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1523 bytes)
2016-03-03 14:33:55,260 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:33:55,260 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:33:55,269 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1526
2016-03-03 14:33:55,277 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1526+1527
2016-03-03 14:33:55,420 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:33:55,420 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:33:55,425 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 167 ms on localhost (1/2)
2016-03-03 14:33:55,426 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 169 ms on localhost (2/2)
2016-03-03 14:33:55,426 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:33:55,426 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.170 s
2016-03-03 14:33:55,428 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:33:55,428 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:33:55,429 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:33:55,430 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:33:55,432 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:33:55,436 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 14:33:55,439 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266292, maxMem=278302556
2016-03-03 14:33:55,440 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:33:55,443 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268532, maxMem=278302556
2016-03-03 14:33:55,443 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:33:55,446 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:9988 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:33:55,448 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:55,449 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 14:33:55,450 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:33:55,451 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:33:55,453 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,455 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,458 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,458 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:33:55,459 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:33:55,459 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:33:55,462 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:33:55,482 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,482 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,483 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,482 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,485 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:33:55,485 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:33:55,485 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:33:55,485 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:33:55,509 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1280 bytes result sent to driver
2016-03-03 14:33:55,509 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1316 bytes result sent to driver
2016-03-03 14:33:55,509 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:33:55,509 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1293 bytes result sent to driver
2016-03-03 14:33:55,511 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,511 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:33:55,512 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,515 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,516 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:33:55,519 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,520 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:33:55,522 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 65 ms on localhost (1/10)
2016-03-03 14:33:55,517 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,525 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 14:33:55,525 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 73 ms on localhost (2/10)
2016-03-03 14:33:55,525 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,527 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:33:55,523 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,528 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:33:55,530 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1315 bytes result sent to driver
2016-03-03 14:33:55,531 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1294 bytes result sent to driver
2016-03-03 14:33:55,519 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:33:55,535 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1294 bytes result sent to driver
2016-03-03 14:33:55,529 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 74 ms on localhost (3/10)
2016-03-03 14:33:55,536 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,538 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:33:55,538 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,540 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1243 bytes)
2016-03-03 14:33:55,540 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:33:55,542 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1275 bytes result sent to driver
2016-03-03 14:33:55,545 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:33:55,544 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,546 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:33:55,549 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:33:55,551 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:33:55,544 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 25 ms on localhost (4/10)
2016-03-03 14:33:55,551 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1352 bytes result sent to driver
2016-03-03 14:33:55,552 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 95 ms on localhost (5/10)
2016-03-03 14:33:55,556 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 45 ms on localhost (6/10)
2016-03-03 14:33:55,557 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 45 ms on localhost (7/10)
2016-03-03 14:33:55,558 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:33:55,559 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 45 ms on localhost (8/10)
2016-03-03 14:33:55,561 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 24 ms on localhost (9/10)
2016-03-03 14:33:55,562 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.111 s
2016-03-03 14:33:55,563 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.337650 s
2016-03-03 14:33:55,562 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 23 ms on localhost (10/10)
2016-03-03 14:33:55,565 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:33:55,589 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269894, maxMem=278302556
2016-03-03 14:33:55,590 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 14:33:55,630 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500582, maxMem=278302556
2016-03-03 14:33:55,631 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 14:33:55,634 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:9988 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:33:55,636 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 14:33:55,655 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:33:55,673 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 14:33:55,675 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 14:33:55,677 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 14:33:55,677 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:33:55,680 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:33:55,681 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 14:33:55,684 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520332, maxMem=278302556
2016-03-03 14:33:55,685 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 14:33:55,688 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523580, maxMem=278302556
2016-03-03 14:33:55,689 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 14:33:55,692 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:9988 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:33:55,694 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 14:33:55,695 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 14:33:55,695 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 14:33:55,698 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:33:55,700 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:33:55,701 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1534 bytes)
2016-03-03 14:33:55,702 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 14:33:55,702 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 14:33:55,707 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1526+1527
2016-03-03 14:33:55,707 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1526
2016-03-03 14:33:55,717 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4204 bytes result sent to driver
2016-03-03 14:33:55,717 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4158 bytes result sent to driver
2016-03-03 14:33:55,723 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 24 ms on localhost (1/2)
2016-03-03 14:33:55,727 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.029 s
2016-03-03 14:33:55,727 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 26 ms on localhost (2/2)
2016-03-03 14:33:55,728 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:33:55,728 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.054238 s
2016-03-03 14:36:51,861 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:36:51,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:36:51,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:36:51,876 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:36:51,876 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:36:51,877 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:36:51,878 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:36:51,878 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:36:51,879 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:36:51,880 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:36:51,880 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:36:51,881 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:36:51,881 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:36:51,881 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:36:51,882 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:36:51,882 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:36:51,883 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:36:51,884 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:36:51,884 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:36:51,885 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:36:51,886 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:36:51,886 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:36:51,887 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:36:51,887 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:36:51,888 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:36:51,888 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:36:51,941 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:36:51,944 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:36:52,012 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:36:52,020 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9114bee6-ebab-4cfc-8906-a0bce88006c1\blockmgr-8942c1a8-5bf6-48fd-bed5-b42cf8e5f853, already present as root for deletion.
2016-03-03 14:36:52,021 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:36:52,022 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:36:52,023 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:36:52,027 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:36:52,034 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:36:52,035 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:36:52,036 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9114bee6-ebab-4cfc-8906-a0bce88006c1
2016-03-03 14:36:52,039 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:36:52,041 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:36:52,085 INFO  [sparkDriver-akka.actor.default-dispatcher-14][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:36:52,097 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-4977f08c-96bd-4ae1-bca0-ce7cf89c1d80
2016-03-03 14:36:52,371 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-4977f08c-96bd-4ae1-bca0-ce7cf89c1d80
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-4977f08c-96bd-4ae1-bca0-ce7cf89c1d80
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:37:04,221 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:37:07,127 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:37:29,982 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:37:33,233 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:37:33,234 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:37:33,234 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:37:33,957 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:37:34,001 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Starting remoting
2016-03-03 14:37:34,162 INFO  [sparkDriver-akka.actor.default-dispatcher-3][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:10256]
2016-03-03 14:37:34,168 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 10256.
2016-03-03 14:37:34,184 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:37:34,198 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:37:34,222 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a\blockmgr-fc1f3171-ec8f-412f-ab77-c8f2428345bf
2016-03-03 14:37:34,229 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:37:34,292 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a\httpd-23aa8850-1c43-4392-869c-4c4ebd0560db
2016-03-03 14:37:34,298 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:37:34,357 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:37:34,375 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:10257
2016-03-03 14:37:34,378 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 10257.
2016-03-03 14:37:34,399 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:37:34,529 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:37:34,548 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:37:34,548 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:37:34,551 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:37:34,733 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:10257/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987054732
2016-03-03 14:37:34,803 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:37:34,804 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:37:34,804 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:37:34,819 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:37:35,978 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10276.
2016-03-03 14:37:35,979 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 10276
2016-03-03 14:37:35,980 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:37:35,984 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:10276 with 265.4 MB RAM, BlockManagerId(driver, localhost, 10276)
2016-03-03 14:37:35,987 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:37:36,822 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:37:37,210 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:37:37,234 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:37:37,460 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:37:37,461 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:37:37,585 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:37:37,831 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:37:46,387 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:37:46,445 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:37:47,545 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:37:47,546 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:37:54,434 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:37:54,435 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:37:56,130 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:37:56,598 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:37:57,802 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:37:57,805 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:37:58,316 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:37:58,451 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:37:58,540 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:38:00,463 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:38:00,499 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:38:00,520 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:38:00,681 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:38:00,682 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:38:00,779 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:38:00,961 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:38:02,022 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:38:02,066 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:38:02,829 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:38:02,830 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:38:03,195 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:38:03,196 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:38:03,320 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:38:03,322 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:38:03,547 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:38:03,549 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:38:03,624 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:38:03,783 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:38:04,227 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 14:38:04,243 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 14:38:04,244 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 14:38:04,245 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:38:04,250 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:38:04,255 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 14:38:04,380 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:38:04,382 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:38:04,391 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:38:04,392 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:38:04,395 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:10276 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:38:04,397 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:04,402 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 14:38:04,404 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:38:04,412 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:38:04,440 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:38:04,443 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:38:04,444 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:38:04,445 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:38:04,450 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:38:04,450 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:38:04,450 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:38:04,450 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:38:04,455 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:10257/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987054732
2016-03-03 14:38:04,514 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:10257/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a\userFiles-7b7997a4-29c8-49d0-b88f-19a868d95b86\fetchFileTemp1116052599279924121.tmp
2016-03-03 14:38:08,146 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a/userFiles-7b7997a4-29c8-49d0-b88f-19a868d95b86/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:38:08,167 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:38:08,168 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:38:08,167 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:38:08,167 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:38:08,170 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:38:08,171 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:38:08,173 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:38:08,175 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:38:08,175 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:38:08,177 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:38:08,178 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:38:08,176 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:38:08,181 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:38:08,182 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:38:08,183 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:38:08,184 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:38:08,185 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:38:08,186 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:38:08,186 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3742 ms on localhost (1/10)
2016-03-03 14:38:08,187 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3744 ms on localhost (2/10)
2016-03-03 14:38:08,188 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:38:08,188 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3765 ms on localhost (3/10)
2016-03-03 14:38:08,190 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:38:08,191 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:38:08,191 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3747 ms on localhost (4/10)
2016-03-03 14:38:08,195 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:38:08,195 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 26 ms on localhost (5/10)
2016-03-03 14:38:08,196 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 24 ms on localhost (6/10)
2016-03-03 14:38:08,198 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 8 ms on localhost (7/10)
2016-03-03 14:38:08,200 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 22 ms on localhost (8/10)
2016-03-03 14:38:08,201 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 21 ms on localhost (9/10)
2016-03-03 14:38:08,202 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 27 ms on localhost (10/10)
2016-03-03 14:38:08,203 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 3.789 s
2016-03-03 14:38:08,203 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:38:08,209 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 3.981242 s
2016-03-03 14:38:08,550 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:38:08,551 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:38:08,585 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:38:08,586 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:38:08,588 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:10276 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:38:08,589 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 14:38:08,631 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:38:08,640 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 14:38:08,642 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 14:38:08,643 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 14:38:08,643 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:38:08,645 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:38:08,646 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:38:08,653 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:38:08,654 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:38:08,657 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 14:38:08,658 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 14:38:08,665 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:10276 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:38:08,666 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:08,668 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:38:08,669 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:38:08,670 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:38:08,673 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:38:08,674 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:38:08,675 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:38:08,675 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:38:08,687 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:38:08,687 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:38:08,697 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:38:08,697 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:38:08,697 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:38:08,698 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:38:08,698 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:38:08,726 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:38:08,726 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:38:08,729 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 56 ms on localhost (1/2)
2016-03-03 14:38:08,730 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 58 ms on localhost (2/2)
2016-03-03 14:38:08,730 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.060 s
2016-03-03 14:38:08,730 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:38:08,731 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.090260 s
2016-03-03 14:38:08,742 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 14:38:08,743 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 14:38:08,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 14:38:08,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:38:08,746 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:38:08,746 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:38:08,749 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=257007, maxMem=278302556
2016-03-03 14:38:08,750 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:38:08,752 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=260143, maxMem=278302556
2016-03-03 14:38:08,753 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 14:38:08,754 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:10276 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:38:08,755 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:08,756 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:38:08,756 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:38:08,757 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:38:08,759 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:38:08,760 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:38:08,764 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:38:08,768 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 14:38:08,770 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.013 s
2016-03-03 14:38:08,770 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 12 ms on localhost (1/1)
2016-03-03 14:38:08,771 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.028416 s
2016-03-03 14:38:08,771 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:38:08,807 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 14:38:08,811 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 14:38:08,812 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 14:38:08,812 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 14:38:08,813 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:38:08,813 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:38:08,817 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 14:38:08,823 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=261960, maxMem=278302556
2016-03-03 14:38:08,824 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:38:08,827 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=265912, maxMem=278302556
2016-03-03 14:38:08,828 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:38:08,830 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:10276 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:38:08,831 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:08,834 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 14:38:08,834 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:38:08,835 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:38:08,838 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:38:08,839 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:38:08,839 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:38:08,839 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:38:08,903 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:38:08,903 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:38:08,920 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:10276 in memory (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:38:08,930 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:10276 in memory (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:38:08,936 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:10276 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:38:09,063 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:38:09,064 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:38:09,070 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 233 ms on localhost (1/2)
2016-03-03 14:38:09,071 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 233 ms on localhost (2/2)
2016-03-03 14:38:09,072 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:38:09,073 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.236 s
2016-03-03 14:38:09,074 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:38:09,075 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:38:09,077 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:38:09,078 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:38:09,082 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:38:09,086 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 14:38:09,088 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256614, maxMem=278302556
2016-03-03 14:38:09,089 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:38:09,092 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=258854, maxMem=278302556
2016-03-03 14:38:09,093 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:38:09,096 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:10276 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:38:09,097 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:09,098 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 14:38:09,099 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:38:09,100 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:38:09,102 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,104 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,105 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,106 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,107 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:38:09,108 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:38:09,107 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:38:09,107 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:38:09,128 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,128 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,128 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,128 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,130 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:38:09,130 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:38:09,130 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:38:09,130 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:38:09,153 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:38:09,153 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 14:38:09,153 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 14:38:09,153 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 14:38:09,155 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,155 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:38:09,156 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,158 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,160 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,160 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:38:09,160 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:38:09,165 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,165 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:38:09,160 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,165 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,161 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:38:09,169 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-03 14:38:09,171 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 14:38:09,173 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 14:38:09,173 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:38:09,169 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 62 ms on localhost (1/10)
2016-03-03 14:38:09,173 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 14:38:09,175 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 74 ms on localhost (2/10)
2016-03-03 14:38:09,177 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 74 ms on localhost (3/10)
2016-03-03 14:38:09,177 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,178 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:38:09,179 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,179 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:38:09,179 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 75 ms on localhost (4/10)
2016-03-03 14:38:09,182 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:38:09,183 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,183 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:38:09,184 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:38:09,187 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 33 ms on localhost (5/10)
2016-03-03 14:38:09,189 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 33 ms on localhost (6/10)
2016-03-03 14:38:09,190 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:38:09,191 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 32 ms on localhost (7/10)
2016-03-03 14:38:09,192 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 35 ms on localhost (8/10)
2016-03-03 14:38:09,192 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:38:09,194 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 16 ms on localhost (9/10)
2016-03-03 14:38:09,197 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:38:09,198 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:38:09,203 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 14:38:09,205 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 23 ms on localhost (10/10)
2016-03-03 14:38:09,205 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.104 s
2016-03-03 14:38:09,205 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:38:09,206 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.398359 s
2016-03-03 14:38:09,227 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=260216, maxMem=278302556
2016-03-03 14:38:09,228 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 14:38:09,269 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=490904, maxMem=278302556
2016-03-03 14:38:09,271 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 14:38:09,275 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:10276 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:38:09,279 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 14:38:09,299 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:38:09,309 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 14:38:09,311 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 14:38:09,311 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 14:38:09,312 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:38:09,313 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:38:09,314 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 14:38:09,316 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=510654, maxMem=278302556
2016-03-03 14:38:09,317 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 14:38:09,319 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=513902, maxMem=278302556
2016-03-03 14:38:09,319 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 14:38:09,321 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:10276 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:38:09,322 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 14:38:09,323 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 14:38:09,324 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 14:38:09,325 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:38:09,327 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:38:09,328 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:38:09,329 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 14:38:09,329 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 14:38:09,333 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:38:09,334 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:38:09,343 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 14:38:09,344 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 14:38:09,346 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 20 ms on localhost (1/2)
2016-03-03 14:38:09,348 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 20 ms on localhost (2/2)
2016-03-03 14:38:09,348 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.022 s
2016-03-03 14:38:09,349 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:38:09,349 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.039520 s
2016-03-03 14:40:36,318 INFO  [sbd-akka.actor.default-dispatcher-6][hive.ql.parse.ParseDriver] Parsing command: select * from pltable
2016-03-03 14:40:36,578 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_7_piece0 on localhost:10276 in memory (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:40:36,581 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_6_piece0 on localhost:10276 in memory (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:40:36,583 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_5_piece0 on localhost:10276 in memory (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:40:36,585 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:10276 in memory (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:40:36,781 INFO  [sbd-akka.actor.default-dispatcher-6][hive.ql.parse.ParseDriver] Parse Completed
2016-03-03 14:40:37,224 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,225 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 5 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,226 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 6(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,226 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,229 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,229 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 6 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,234 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=250382, maxMem=278302556
2016-03-03 14:40:37,235 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_8 stored as values in memory (estimated size 11.7 KB, free 265.2 MB)
2016-03-03 14:40:37,237 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=262406, maxMem=278302556
2016-03-03 14:40:37,238 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.2 MB)
2016-03-03 14:40:37,242 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_8_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,244 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 8 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,244 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,245 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 6.0 with 1 tasks
2016-03-03 14:40:37,246 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_6 tasks to pool default
2016-03-03 14:40:37,250 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 6.0 (TID 27, localhost, PROCESS_LOCAL, 2241 bytes)
2016-03-03 14:40:37,251 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 6.0 (TID 27)
2016-03-03 14:40:37,280 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_0 not found, computing it
2016-03-03 14:40:37,341 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(992) called with curMem=267673, maxMem=278302556
2016-03-03 14:40:37,341 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_0 stored as values in memory (estimated size 992.0 B, free 265.2 MB)
2016-03-03 14:40:37,343 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_0 in memory on localhost:10276 (size: 992.0 B, free: 265.4 MB)
2016-03-03 14:40:37,362 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 6.0 (TID 27). 2648 bytes result sent to driver
2016-03-03 14:40:37,364 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 6.0 (TID 27) in 116 ms on localhost (1/1)
2016-03-03 14:40:37,364 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 6.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,380 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 6 (hasNext at Streamer.scala:177) finished in 0.134 s
2016-03-03 14:40:37,381 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 5 finished: hasNext at Streamer.scala:177, took 0.156400 s
2016-03-03 14:40:37,415 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,416 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 6 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,416 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 7(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,417 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,423 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 7 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,430 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=268665, maxMem=278302556
2016-03-03 14:40:37,431 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_9 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,433 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=280689, maxMem=278302556
2016-03-03 14:40:37,434 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,436 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_9_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,437 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 9 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,438 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,438 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 7.0 with 1 tasks
2016-03-03 14:40:37,439 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_7 tasks to pool default
2016-03-03 14:40:37,440 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,441 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 7.0 (TID 28)
2016-03-03 14:40:37,448 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_1 not found, computing it
2016-03-03 14:40:37,450 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=285956, maxMem=278302556
2016-03-03 14:40:37,451 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_1 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,453 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_1 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,456 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 7.0 (TID 28). 2714 bytes result sent to driver
2016-03-03 14:40:37,458 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 7.0 (TID 28) in 19 ms on localhost (1/1)
2016-03-03 14:40:37,458 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 7 (hasNext at Streamer.scala:177) finished in 0.019 s
2016-03-03 14:40:37,458 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 7.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,459 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 6 finished: hasNext at Streamer.scala:177, took 0.043266 s
2016-03-03 14:40:37,473 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,475 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 7 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,475 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 8(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,475 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,479 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,480 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 8 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,483 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=286972, maxMem=278302556
2016-03-03 14:40:37,484 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_10 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,486 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=298996, maxMem=278302556
2016-03-03 14:40:37,487 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,489 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_10_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,490 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 10 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,491 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,491 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 8.0 with 1 tasks
2016-03-03 14:40:37,492 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_8 tasks to pool default
2016-03-03 14:40:37,493 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 8.0 (TID 29, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,493 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 8.0 (TID 29)
2016-03-03 14:40:37,499 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_2 not found, computing it
2016-03-03 14:40:37,503 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=304263, maxMem=278302556
2016-03-03 14:40:37,504 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_2 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,506 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_2 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,510 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 8.0 (TID 29). 2714 bytes result sent to driver
2016-03-03 14:40:37,512 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 8.0 (TID 29) in 19 ms on localhost (1/1)
2016-03-03 14:40:37,512 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 8.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,512 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 8 (hasNext at Streamer.scala:177) finished in 0.020 s
2016-03-03 14:40:37,513 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 7 finished: hasNext at Streamer.scala:177, took 0.039454 s
2016-03-03 14:40:37,525 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,527 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 8 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,527 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 9(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,528 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,533 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,533 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 9 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,537 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=305279, maxMem=278302556
2016-03-03 14:40:37,538 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_11 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,541 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=317303, maxMem=278302556
2016-03-03 14:40:37,542 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,543 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_11_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,543 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 11 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,544 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,544 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 9.0 with 1 tasks
2016-03-03 14:40:37,545 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_9 tasks to pool default
2016-03-03 14:40:37,547 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 9.0 (TID 30, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,547 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 9.0 (TID 30)
2016-03-03 14:40:37,553 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_3 not found, computing it
2016-03-03 14:40:37,556 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=322570, maxMem=278302556
2016-03-03 14:40:37,556 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_3 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,558 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_3 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,561 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 9.0 (TID 30). 2714 bytes result sent to driver
2016-03-03 14:40:37,562 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 9.0 (TID 30) in 16 ms on localhost (1/1)
2016-03-03 14:40:37,562 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 9 (hasNext at Streamer.scala:177) finished in 0.017 s
2016-03-03 14:40:37,562 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 9.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,563 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 8 finished: hasNext at Streamer.scala:177, took 0.036794 s
2016-03-03 14:40:37,568 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 9 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 10(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,570 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,573 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,574 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 10 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,576 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=323586, maxMem=278302556
2016-03-03 14:40:37,576 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_12 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,579 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=335610, maxMem=278302556
2016-03-03 14:40:37,580 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,582 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_12_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,583 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 12 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,584 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,584 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 10.0 with 1 tasks
2016-03-03 14:40:37,586 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_10 tasks to pool default
2016-03-03 14:40:37,587 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 10.0 (TID 31, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,588 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 10.0 (TID 31)
2016-03-03 14:40:37,594 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_4 not found, computing it
2016-03-03 14:40:37,596 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=340877, maxMem=278302556
2016-03-03 14:40:37,597 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_4 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,598 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_4 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,601 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 10.0 (TID 31). 2714 bytes result sent to driver
2016-03-03 14:40:37,602 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 10.0 (TID 31) in 16 ms on localhost (1/1)
2016-03-03 14:40:37,602 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 10 (hasNext at Streamer.scala:177) finished in 0.016 s
2016-03-03 14:40:37,602 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 10.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,603 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 9 finished: hasNext at Streamer.scala:177, took 0.033825 s
2016-03-03 14:40:37,611 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,613 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 10 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,613 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 11(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,614 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,618 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,619 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 11 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,621 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=341893, maxMem=278302556
2016-03-03 14:40:37,622 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_13 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,625 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=353917, maxMem=278302556
2016-03-03 14:40:37,626 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,628 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_13_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,629 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 13 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,630 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,630 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 11.0 with 1 tasks
2016-03-03 14:40:37,631 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_11 tasks to pool default
2016-03-03 14:40:37,632 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 11.0 (TID 32, localhost, PROCESS_LOCAL, 2241 bytes)
2016-03-03 14:40:37,633 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 11.0 (TID 32)
2016-03-03 14:40:37,640 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_5 not found, computing it
2016-03-03 14:40:37,644 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(984) called with curMem=359184, maxMem=278302556
2016-03-03 14:40:37,645 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_5 stored as values in memory (estimated size 984.0 B, free 265.1 MB)
2016-03-03 14:40:37,647 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_5 in memory on localhost:10276 (size: 984.0 B, free: 265.4 MB)
2016-03-03 14:40:37,651 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 11.0 (TID 32). 2648 bytes result sent to driver
2016-03-03 14:40:37,653 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 11.0 (TID 32) in 21 ms on localhost (1/1)
2016-03-03 14:40:37,653 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 11 (hasNext at Streamer.scala:177) finished in 0.022 s
2016-03-03 14:40:37,653 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 11.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,654 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 10 finished: hasNext at Streamer.scala:177, took 0.042374 s
2016-03-03 14:40:37,660 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,661 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 11 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,662 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 12(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,662 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,665 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,665 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 12 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,667 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=360168, maxMem=278302556
2016-03-03 14:40:37,667 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_14 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,669 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=372192, maxMem=278302556
2016-03-03 14:40:37,670 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,670 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_14_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,671 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 14 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,671 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,672 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 12.0 with 1 tasks
2016-03-03 14:40:37,672 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_12 tasks to pool default
2016-03-03 14:40:37,673 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 12.0 (TID 33, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,674 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 12.0 (TID 33)
2016-03-03 14:40:37,679 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_6 not found, computing it
2016-03-03 14:40:37,682 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=377459, maxMem=278302556
2016-03-03 14:40:37,683 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_6 stored as values in memory (estimated size 1016.0 B, free 265.0 MB)
2016-03-03 14:40:37,684 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_6 in memory on localhost:10276 (size: 1016.0 B, free: 265.3 MB)
2016-03-03 14:40:37,687 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 12.0 (TID 33). 2714 bytes result sent to driver
2016-03-03 14:40:37,688 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 12.0 (TID 33) in 15 ms on localhost (1/1)
2016-03-03 14:40:37,688 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 12 (hasNext at Streamer.scala:177) finished in 0.015 s
2016-03-03 14:40:37,689 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 12.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,689 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 11 finished: hasNext at Streamer.scala:177, took 0.028071 s
2016-03-03 14:40:37,695 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,696 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 12 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,696 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 13(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,696 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,699 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,700 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 13 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,702 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=378475, maxMem=278302556
2016-03-03 14:40:37,703 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_15 stored as values in memory (estimated size 11.7 KB, free 265.0 MB)
2016-03-03 14:40:37,742 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=390499, maxMem=278302556
2016-03-03 14:40:37,743 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.0 MB)
2016-03-03 14:40:37,744 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_14_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,745 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_15_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.3 MB)
2016-03-03 14:40:37,746 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 15 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,746 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,746 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 13.0 with 1 tasks
2016-03-03 14:40:37,746 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_13_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,747 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_13 tasks to pool default
2016-03-03 14:40:37,748 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 13.0 (TID 34, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,748 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 13.0 (TID 34)
2016-03-03 14:40:37,749 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_12_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,750 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_11_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,752 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_10_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,754 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_7 not found, computing it
2016-03-03 14:40:37,755 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_9_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,758 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=292020, maxMem=278302556
2016-03-03 14:40:37,759 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_7 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,760 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_8_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,761 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_7 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,764 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 13.0 (TID 34). 2714 bytes result sent to driver
2016-03-03 14:40:37,765 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 13.0 (TID 34) in 18 ms on localhost (1/1)
2016-03-03 14:40:37,766 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 13 (hasNext at Streamer.scala:177) finished in 0.018 s
2016-03-03 14:40:37,766 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 13.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,766 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 12 finished: hasNext at Streamer.scala:177, took 0.070956 s
2016-03-03 14:40:37,772 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,773 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 13 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,774 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 14(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,774 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,776 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,777 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 14 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,778 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=275745, maxMem=278302556
2016-03-03 14:40:37,779 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_16 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,780 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=287769, maxMem=278302556
2016-03-03 14:40:37,781 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,781 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_16_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,782 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 16 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,782 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,783 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 14.0 with 1 tasks
2016-03-03 14:40:37,783 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_14 tasks to pool default
2016-03-03 14:40:37,784 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 14.0 (TID 35, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,784 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 14.0 (TID 35)
2016-03-03 14:40:37,789 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_8 not found, computing it
2016-03-03 14:40:37,791 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=293036, maxMem=278302556
2016-03-03 14:40:37,792 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_8 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,793 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_8 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,795 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 14.0 (TID 35). 2714 bytes result sent to driver
2016-03-03 14:40:37,796 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 14.0 (TID 35) in 13 ms on localhost (1/1)
2016-03-03 14:40:37,796 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 14 (hasNext at Streamer.scala:177) finished in 0.013 s
2016-03-03 14:40:37,797 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 14.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,797 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 13 finished: hasNext at Streamer.scala:177, took 0.024397 s
2016-03-03 14:40:37,802 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:37,803 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 14 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:37,803 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 15(hasNext at Streamer.scala:177)
2016-03-03 14:40:37,803 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:37,807 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:37,807 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 15 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:37,809 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(12024) called with curMem=294052, maxMem=278302556
2016-03-03 14:40:37,809 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_17 stored as values in memory (estimated size 11.7 KB, free 265.1 MB)
2016-03-03 14:40:37,810 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5267) called with curMem=306076, maxMem=278302556
2016-03-03 14:40:37,810 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 14:40:37,811 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_17_piece0 in memory on localhost:10276 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:37,811 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 17 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:37,812 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[18] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:37,812 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 15.0 with 1 tasks
2016-03-03 14:40:37,813 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_15 tasks to pool default
2016-03-03 14:40:37,814 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 15.0 (TID 36, localhost, PROCESS_LOCAL, 2297 bytes)
2016-03-03 14:40:37,814 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 15.0 (TID 36)
2016-03-03 14:40:37,819 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_15_9 not found, computing it
2016-03-03 14:40:37,821 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1016) called with curMem=311343, maxMem=278302556
2016-03-03 14:40:37,822 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_15_9 stored as values in memory (estimated size 1016.0 B, free 265.1 MB)
2016-03-03 14:40:37,824 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_15_9 in memory on localhost:10276 (size: 1016.0 B, free: 265.4 MB)
2016-03-03 14:40:37,826 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 15.0 (TID 36). 2714 bytes result sent to driver
2016-03-03 14:40:37,828 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 15.0 (TID 36) in 15 ms on localhost (1/1)
2016-03-03 14:40:37,829 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 15 (hasNext at Streamer.scala:177) finished in 0.016 s
2016-03-03 14:40:37,830 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 15.0, whose tasks have all completed, from pool default
2016-03-03 14:40:37,831 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 14 finished: hasNext at Streamer.scala:177, took 0.028269 s
2016-03-03 14:40:37,840 INFO  [sbd-akka.actor.default-dispatcher-6][org.apache.spark.scheduler.DAGScheduler] Asked to cancel job group streamer-$a
2016-03-03 14:40:58,371 INFO  [sbd-akka.actor.default-dispatcher-9][hive.ql.parse.ParseDriver] Parsing command: select * from ntable
2016-03-03 14:40:58,379 INFO  [sbd-akka.actor.default-dispatcher-9][hive.ql.parse.ParseDriver] Parse Completed
2016-03-03 14:40:58,429 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,431 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 15 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,431 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 16(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,431 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,436 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,437 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 16 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,439 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=312359, maxMem=278302556
2016-03-03 14:40:58,440 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_18 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,442 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=321223, maxMem=278302556
2016-03-03 14:40:58,442 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,444 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_18_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,444 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 18 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,445 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,445 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 16.0 with 1 tasks
2016-03-03 14:40:58,446 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_16 tasks to pool default
2016-03-03 14:40:58,447 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 16.0 (TID 37, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:40:58,448 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 16.0 (TID 37)
2016-03-03 14:40:58,452 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_0 not found, computing it
2016-03-03 14:40:58,491 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(16) called with curMem=325526, maxMem=278302556
2016-03-03 14:40:58,492 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_0 stored as values in memory (estimated size 16.0 B, free 265.1 MB)
2016-03-03 14:40:58,493 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_0 in memory on localhost:10276 (size: 16.0 B, free: 265.4 MB)
2016-03-03 14:40:58,494 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 16.0 (TID 37). 1427 bytes result sent to driver
2016-03-03 14:40:58,495 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 16.0 (TID 37) in 48 ms on localhost (1/1)
2016-03-03 14:40:58,495 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 16 (hasNext at Streamer.scala:177) finished in 0.048 s
2016-03-03 14:40:58,495 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 16.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,496 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 15 finished: hasNext at Streamer.scala:177, took 0.066454 s
2016-03-03 14:40:58,501 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,503 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 16 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,503 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 17(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,503 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,507 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,508 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 17 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,509 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=325542, maxMem=278302556
2016-03-03 14:40:58,510 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_19 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,511 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=334406, maxMem=278302556
2016-03-03 14:40:58,512 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_19_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,513 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 19 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,514 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,514 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 17.0 with 1 tasks
2016-03-03 14:40:58,514 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_17 tasks to pool default
2016-03-03 14:40:58,515 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 17.0 (TID 38, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:40:58,515 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 17.0 (TID 38)
2016-03-03 14:40:58,519 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_1 not found, computing it
2016-03-03 14:40:58,520 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(216) called with curMem=338709, maxMem=278302556
2016-03-03 14:40:58,521 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_1 stored as values in memory (estimated size 216.0 B, free 265.1 MB)
2016-03-03 14:40:58,522 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_1 in memory on localhost:10276 (size: 216.0 B, free: 265.4 MB)
2016-03-03 14:40:58,523 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 17.0 (TID 38). 1685 bytes result sent to driver
2016-03-03 14:40:58,524 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 17.0 (TID 38) in 9 ms on localhost (1/1)
2016-03-03 14:40:58,524 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 17 (hasNext at Streamer.scala:177) finished in 0.010 s
2016-03-03 14:40:58,525 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 17.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,525 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 16 finished: hasNext at Streamer.scala:177, took 0.023136 s
2016-03-03 14:40:58,530 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,532 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 17 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,532 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 18(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,532 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,537 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,538 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 18 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,539 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=338925, maxMem=278302556
2016-03-03 14:40:58,540 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_20 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,541 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=347789, maxMem=278302556
2016-03-03 14:40:58,542 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,544 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_20_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,545 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 20 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,545 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,545 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 18.0 with 1 tasks
2016-03-03 14:40:58,546 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_18 tasks to pool default
2016-03-03 14:40:58,547 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 18.0 (TID 39, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:40:58,548 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 18.0 (TID 39)
2016-03-03 14:40:58,552 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_2 not found, computing it
2016-03-03 14:40:58,553 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(16) called with curMem=352092, maxMem=278302556
2016-03-03 14:40:58,554 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_2 stored as values in memory (estimated size 16.0 B, free 265.1 MB)
2016-03-03 14:40:58,556 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_2 in memory on localhost:10276 (size: 16.0 B, free: 265.4 MB)
2016-03-03 14:40:58,557 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 18.0 (TID 39). 1427 bytes result sent to driver
2016-03-03 14:40:58,558 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 18.0 (TID 39) in 12 ms on localhost (1/1)
2016-03-03 14:40:58,558 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 18 (hasNext at Streamer.scala:177) finished in 0.012 s
2016-03-03 14:40:58,559 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 18.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,559 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 17 finished: hasNext at Streamer.scala:177, took 0.028259 s
2016-03-03 14:40:58,565 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,566 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 18 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,566 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 19(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,567 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,572 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,573 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 19 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,577 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=352108, maxMem=278302556
2016-03-03 14:40:58,578 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_21 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,579 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=360972, maxMem=278302556
2016-03-03 14:40:58,580 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,583 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_21_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,585 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 21 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,586 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,586 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 19.0 with 1 tasks
2016-03-03 14:40:58,587 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_19 tasks to pool default
2016-03-03 14:40:58,588 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 19.0 (TID 40, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:40:58,590 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 19.0 (TID 40)
2016-03-03 14:40:58,600 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_3 not found, computing it
2016-03-03 14:40:58,604 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(232) called with curMem=365275, maxMem=278302556
2016-03-03 14:40:58,605 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_3 stored as values in memory (estimated size 232.0 B, free 265.1 MB)
2016-03-03 14:40:58,606 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_3 in memory on localhost:10276 (size: 232.0 B, free: 265.3 MB)
2016-03-03 14:40:58,607 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 19.0 (TID 40). 1690 bytes result sent to driver
2016-03-03 14:40:58,609 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 19.0 (TID 40) in 20 ms on localhost (1/1)
2016-03-03 14:40:58,610 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 19.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,609 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 19 (hasNext at Streamer.scala:177) finished in 0.022 s
2016-03-03 14:40:58,611 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 18 finished: hasNext at Streamer.scala:177, took 0.045319 s
2016-03-03 14:40:58,617 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,619 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 19 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,619 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 20(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,619 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,624 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,624 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 20 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,626 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=365507, maxMem=278302556
2016-03-03 14:40:58,626 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_22 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,627 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=374371, maxMem=278302556
2016-03-03 14:40:58,628 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.0 MB)
2016-03-03 14:40:58,629 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_22_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,630 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 22 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,630 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,630 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 20.0 with 1 tasks
2016-03-03 14:40:58,631 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_20 tasks to pool default
2016-03-03 14:40:58,632 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 20.0 (TID 41, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:40:58,632 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 20.0 (TID 41)
2016-03-03 14:40:58,636 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_4 not found, computing it
2016-03-03 14:40:58,637 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(16) called with curMem=378674, maxMem=278302556
2016-03-03 14:40:58,637 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_4 stored as values in memory (estimated size 16.0 B, free 265.0 MB)
2016-03-03 14:40:58,639 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_4 in memory on localhost:10276 (size: 16.0 B, free: 265.3 MB)
2016-03-03 14:40:58,640 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 20.0 (TID 41). 1427 bytes result sent to driver
2016-03-03 14:40:58,641 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 20.0 (TID 41) in 10 ms on localhost (1/1)
2016-03-03 14:40:58,641 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 20 (hasNext at Streamer.scala:177) finished in 0.010 s
2016-03-03 14:40:58,642 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 20.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,642 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 19 finished: hasNext at Streamer.scala:177, took 0.024811 s
2016-03-03 14:40:58,650 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,652 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 20 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,652 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 21(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,652 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,656 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,657 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 21 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,659 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=378690, maxMem=278302556
2016-03-03 14:40:58,659 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_23 stored as values in memory (estimated size 8.7 KB, free 265.0 MB)
2016-03-03 14:40:58,660 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=387554, maxMem=278302556
2016-03-03 14:40:58,661 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.0 MB)
2016-03-03 14:40:58,663 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_23_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,663 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 23 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,664 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,664 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 21.0 with 1 tasks
2016-03-03 14:40:58,665 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_21 tasks to pool default
2016-03-03 14:40:58,666 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 21.0 (TID 42, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:40:58,666 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 21.0 (TID 42)
2016-03-03 14:40:58,672 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_5 not found, computing it
2016-03-03 14:40:58,674 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(232) called with curMem=391857, maxMem=278302556
2016-03-03 14:40:58,674 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_5 stored as values in memory (estimated size 232.0 B, free 265.0 MB)
2016-03-03 14:40:58,675 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_5 in memory on localhost:10276 (size: 232.0 B, free: 265.3 MB)
2016-03-03 14:40:58,676 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 21.0 (TID 42). 1690 bytes result sent to driver
2016-03-03 14:40:58,677 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 21.0 (TID 42) in 12 ms on localhost (1/1)
2016-03-03 14:40:58,678 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 21.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,677 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 21 (hasNext at Streamer.scala:177) finished in 0.012 s
2016-03-03 14:40:58,678 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 20 finished: hasNext at Streamer.scala:177, took 0.027270 s
2016-03-03 14:40:58,684 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,685 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 21 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,685 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 22(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,685 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,690 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,691 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 22 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,692 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=392089, maxMem=278302556
2016-03-03 14:40:58,693 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_24 stored as values in memory (estimated size 8.7 KB, free 265.0 MB)
2016-03-03 14:40:58,721 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=400953, maxMem=278302556
2016-03-03 14:40:58,722 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_23_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,722 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.0 MB)
2016-03-03 14:40:58,725 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_24_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,725 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 24 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,726 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_22_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,726 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,727 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 22.0 with 1 tasks
2016-03-03 14:40:58,727 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_22 tasks to pool default
2016-03-03 14:40:58,729 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 22.0 (TID 43, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:40:58,729 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_21_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.3 MB)
2016-03-03 14:40:58,729 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 22.0 (TID 43)
2016-03-03 14:40:58,732 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_20_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,734 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_6 not found, computing it
2016-03-03 14:40:58,735 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(16) called with curMem=339421, maxMem=278302556
2016-03-03 14:40:58,735 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_19_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,736 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_6 stored as values in memory (estimated size 16.0 B, free 265.1 MB)
2016-03-03 14:40:58,737 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_6 in memory on localhost:10276 (size: 16.0 B, free: 265.4 MB)
2016-03-03 14:40:58,738 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 22.0 (TID 43). 1427 bytes result sent to driver
2016-03-03 14:40:58,739 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_18_piece0 on localhost:10276 in memory (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,739 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 22 (hasNext at Streamer.scala:177) finished in 0.011 s
2016-03-03 14:40:58,739 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 22.0 (TID 43) in 11 ms on localhost (1/1)
2016-03-03 14:40:58,740 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 21 finished: hasNext at Streamer.scala:177, took 0.055849 s
2016-03-03 14:40:58,740 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 22.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,741 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_17_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:58,744 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_16_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:58,746 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_15_piece0 on localhost:10276 in memory (size: 5.1 KB, free: 265.4 MB)
2016-03-03 14:40:58,747 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,748 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 22 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,748 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 23(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,748 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,752 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,753 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 23 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,755 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=274397, maxMem=278302556
2016-03-03 14:40:58,755 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_25 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,757 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=283261, maxMem=278302556
2016-03-03 14:40:58,758 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,759 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_25_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,759 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 25 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,760 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,760 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 23.0 with 1 tasks
2016-03-03 14:40:58,761 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_23 tasks to pool default
2016-03-03 14:40:58,762 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 23.0 (TID 44, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:40:58,762 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 23.0 (TID 44)
2016-03-03 14:40:58,766 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_7 not found, computing it
2016-03-03 14:40:58,766 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(232) called with curMem=287564, maxMem=278302556
2016-03-03 14:40:58,767 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_7 stored as values in memory (estimated size 232.0 B, free 265.1 MB)
2016-03-03 14:40:58,768 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_7 in memory on localhost:10276 (size: 232.0 B, free: 265.4 MB)
2016-03-03 14:40:58,769 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 23.0 (TID 44). 1690 bytes result sent to driver
2016-03-03 14:40:58,771 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 23.0 (TID 44) in 10 ms on localhost (1/1)
2016-03-03 14:40:58,771 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 23.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,771 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 23 (hasNext at Streamer.scala:177) finished in 0.010 s
2016-03-03 14:40:58,772 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 22 finished: hasNext at Streamer.scala:177, took 0.024972 s
2016-03-03 14:40:58,780 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,781 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 23 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,781 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 24(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,781 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,786 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,787 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 24 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,789 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=287796, maxMem=278302556
2016-03-03 14:40:58,789 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_26 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,790 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=296660, maxMem=278302556
2016-03-03 14:40:58,791 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_26_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,793 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_26_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,794 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 26 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,794 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,795 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 24.0 with 1 tasks
2016-03-03 14:40:58,795 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_24 tasks to pool default
2016-03-03 14:40:58,796 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 24.0 (TID 45, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:40:58,797 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 24.0 (TID 45)
2016-03-03 14:40:58,801 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_8 not found, computing it
2016-03-03 14:40:58,802 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(16) called with curMem=300963, maxMem=278302556
2016-03-03 14:40:58,803 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_8 stored as values in memory (estimated size 16.0 B, free 265.1 MB)
2016-03-03 14:40:58,804 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_8 in memory on localhost:10276 (size: 16.0 B, free: 265.4 MB)
2016-03-03 14:40:58,805 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 24.0 (TID 45). 1427 bytes result sent to driver
2016-03-03 14:40:58,806 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 24.0 (TID 45) in 9 ms on localhost (1/1)
2016-03-03 14:40:58,806 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 24 (hasNext at Streamer.scala:177) finished in 0.011 s
2016-03-03 14:40:58,806 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 24.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,806 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 23 finished: hasNext at Streamer.scala:177, took 0.026053 s
2016-03-03 14:40:58,812 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 14:40:58,813 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 24 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 14:40:58,813 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 25(hasNext at Streamer.scala:177)
2016-03-03 14:40:58,814 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:40:58,818 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:40:58,819 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 25 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 14:40:58,821 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(8864) called with curMem=300979, maxMem=278302556
2016-03-03 14:40:58,821 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_27 stored as values in memory (estimated size 8.7 KB, free 265.1 MB)
2016-03-03 14:40:58,823 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(4303) called with curMem=309843, maxMem=278302556
2016-03-03 14:40:58,824 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.2 KB, free 265.1 MB)
2016-03-03 14:40:58,825 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_27_piece0 in memory on localhost:10276 (size: 4.2 KB, free: 265.4 MB)
2016-03-03 14:40:58,825 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 27 from broadcast at DAGScheduler.scala:874
2016-03-03 14:40:58,826 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[21] at mapPartitions at Streamer.scala:135)
2016-03-03 14:40:58,826 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 25.0 with 1 tasks
2016-03-03 14:40:58,826 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_25 tasks to pool default
2016-03-03 14:40:58,827 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 25.0 (TID 46, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:40:58,828 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Running task 0.0 in stage 25.0 (TID 46)
2016-03-03 14:40:58,831 INFO  [Executor task launch worker-4][org.apache.spark.CacheManager] Partition rdd_4_9 not found, computing it
2016-03-03 14:40:58,832 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] ensureFreeSpace(232) called with curMem=314146, maxMem=278302556
2016-03-03 14:40:58,832 INFO  [Executor task launch worker-4][org.apache.spark.storage.MemoryStore] Block rdd_4_9 stored as values in memory (estimated size 232.0 B, free 265.1 MB)
2016-03-03 14:40:58,833 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_4_9 in memory on localhost:10276 (size: 232.0 B, free: 265.4 MB)
2016-03-03 14:40:58,834 INFO  [Executor task launch worker-4][org.apache.spark.executor.Executor] Finished task 0.0 in stage 25.0 (TID 46). 1690 bytes result sent to driver
2016-03-03 14:40:58,836 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 25.0 (TID 46) in 9 ms on localhost (1/1)
2016-03-03 14:40:58,836 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 25 (hasNext at Streamer.scala:177) finished in 0.010 s
2016-03-03 14:40:58,836 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 25.0, whose tasks have all completed, from pool default
2016-03-03 14:40:58,837 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 24 finished: hasNext at Streamer.scala:177, took 0.024442 s
2016-03-03 14:40:58,839 INFO  [sbd-akka.actor.default-dispatcher-9][org.apache.spark.scheduler.DAGScheduler] Asked to cancel job group streamer-$b
2016-03-03 14:44:02,875 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:44:02,892 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:44:02,892 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:44:02,893 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:44:02,894 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:44:02,894 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:44:02,894 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:44:02,895 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:44:02,895 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:44:02,896 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:44:02,896 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:44:02,897 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:44:02,897 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:44:02,898 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:44:02,899 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:44:02,899 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:44:02,900 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:44:02,901 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:44:02,903 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:44:02,904 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:44:02,905 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:44:02,905 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:44:02,906 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:44:02,906 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:44:02,907 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:44:02,907 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:44:02,958 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:44:02,960 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:44:03,030 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:44:03,040 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a\blockmgr-fc1f3171-ec8f-412f-ab77-c8f2428345bf, already present as root for deletion.
2016-03-03 14:44:03,046 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:44:03,049 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:44:03,051 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:44:03,059 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:44:03,061 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:44:03,064 INFO  [sparkDriver-akka.actor.default-dispatcher-14][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:44:03,064 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:44:03,066 INFO  [sparkDriver-akka.actor.default-dispatcher-14][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:44:03,066 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-10f3dbd7-ff0c-4823-85fd-f32634f6723a
2016-03-03 14:44:03,106 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:44:03,191 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-a118bc7b-4dfd-49ae-9253-746d71e43bc2
2016-03-03 14:44:03,404 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-a118bc7b-4dfd-49ae-9253-746d71e43bc2
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-a118bc7b-4dfd-49ae-9253-746d71e43bc2
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:44:31,961 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:44:34,878 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:44:34,879 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:44:34,880 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:44:35,605 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:44:35,654 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Starting remoting
2016-03-03 14:44:35,823 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:10524]
2016-03-03 14:44:35,828 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 10524.
2016-03-03 14:44:35,847 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:44:35,861 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:44:35,885 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-493d5b54-3c11-4221-9950-6a27eec21a53\blockmgr-869ce47a-b829-4ff3-a041-362feb1ec793
2016-03-03 14:44:35,891 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:44:35,962 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-493d5b54-3c11-4221-9950-6a27eec21a53\httpd-f0d05ccd-7783-462a-96bc-1f98b54ea017
2016-03-03 14:44:35,967 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:44:36,029 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:44:36,048 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:10525
2016-03-03 14:44:36,050 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 10525.
2016-03-03 14:44:36,071 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:44:36,201 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:44:36,247 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:44:36,281 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:44:36,291 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:44:36,440 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:10525/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987476440
2016-03-03 14:44:36,504 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:44:36,505 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:44:36,506 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:44:36,521 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:44:37,648 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10544.
2016-03-03 14:44:37,649 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 10544
2016-03-03 14:44:37,650 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:44:37,654 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:10544 with 265.4 MB RAM, BlockManagerId(driver, localhost, 10544)
2016-03-03 14:44:37,657 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:44:38,516 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:44:38,904 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:44:38,927 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:44:39,112 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:44:39,113 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:44:39,233 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:44:39,474 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:44:59,311 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:44:59,372 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:45:01,316 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:01,318 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:21,590 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:21,591 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:28,396 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:45:28,873 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:45:30,649 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:45:30,653 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:45:31,598 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:45:31,739 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:45:31,808 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:45:35,326 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:45:35,385 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:45:35,406 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:45:35,564 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:45:35,565 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:45:35,640 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:45:35,826 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:45:37,376 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:45:37,423 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:45:38,181 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:38,182 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:38,541 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:38,542 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:45:38,661 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:45:38,663 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:45:38,885 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:45:38,886 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:45:38,974 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:45:39,136 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:45:39,517 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 14:45:39,535 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 14:45:39,536 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 14:45:39,537 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:45:39,542 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:45:39,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 14:45:39,659 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:45:39,661 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:45:39,671 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:45:39,671 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:45:39,674 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:10544 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:45:39,677 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:39,683 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 14:45:39,685 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:45:39,693 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:45:39,723 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:45:39,726 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:45:39,727 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:45:39,728 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:45:39,733 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:45:39,733 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:45:39,733 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:45:39,733 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:45:39,739 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:10525/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987476440
2016-03-03 14:45:39,819 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:10525/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-493d5b54-3c11-4221-9950-6a27eec21a53\userFiles-3c1f357c-8adb-488e-b401-92064fc12a6e\fetchFileTemp7720940066927993293.tmp
2016-03-03 14:45:43,258 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-493d5b54-3c11-4221-9950-6a27eec21a53/userFiles-3c1f357c-8adb-488e-b401-92064fc12a6e/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:45:43,278 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:45:43,279 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:45:43,279 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:45:43,280 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:45:43,282 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:45:43,282 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:45:43,285 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:45:43,285 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:45:43,291 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:45:43,292 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:45:43,294 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:45:43,295 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:45:43,295 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:45:43,297 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:45:43,298 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:45:43,299 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3591 ms on localhost (1/10)
2016-03-03 14:45:43,300 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:45:43,300 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3574 ms on localhost (2/10)
2016-03-03 14:45:43,302 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:45:43,302 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:45:43,303 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3575 ms on localhost (3/10)
2016-03-03 14:45:43,303 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 22 ms on localhost (4/10)
2016-03-03 14:45:43,305 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:45:43,306 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:45:43,306 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:45:43,306 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3581 ms on localhost (5/10)
2016-03-03 14:45:43,309 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:45:43,310 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 27 ms on localhost (6/10)
2016-03-03 14:45:43,311 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 19 ms on localhost (7/10)
2016-03-03 14:45:43,313 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 19 ms on localhost (8/10)
2016-03-03 14:45:43,314 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 10 ms on localhost (9/10)
2016-03-03 14:45:43,315 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 14 ms on localhost (10/10)
2016-03-03 14:45:43,315 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 3.621 s
2016-03-03 14:45:43,316 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:45:43,321 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 3.804073 s
2016-03-03 14:45:43,647 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:45:43,648 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:45:43,681 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:45:43,682 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:45:43,684 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:10544 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:45:43,685 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 14:45:43,731 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:45:43,741 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 14:45:43,743 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 14:45:43,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 14:45:43,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:45:43,745 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:45:43,746 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:45:43,752 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:45:43,753 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:45:43,755 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 14:45:43,756 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 14:45:43,758 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:10544 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:45:43,759 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:43,762 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:45:43,763 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:45:43,764 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:45:43,767 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:45:43,768 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:45:43,769 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:45:43,769 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:45:43,782 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:45:43,783 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:45:43,793 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:45:43,794 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:45:43,794 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:45:43,794 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:45:43,795 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:45:43,823 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:45:43,823 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:45:43,828 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 59 ms on localhost (1/2)
2016-03-03 14:45:43,829 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 64 ms on localhost (2/2)
2016-03-03 14:45:43,829 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.064 s
2016-03-03 14:45:43,830 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:45:43,830 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.088646 s
2016-03-03 14:45:43,844 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 14:45:43,846 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 14:45:43,846 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 14:45:43,846 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:45:43,848 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:45:43,849 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:45:43,851 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=257007, maxMem=278302556
2016-03-03 14:45:43,852 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:45:44,505 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=260143, maxMem=278302556
2016-03-03 14:45:44,507 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 14:45:44,511 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:10544 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:45:44,515 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:44,517 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:45:44,518 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:45:44,521 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:45:44,527 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:45:44,530 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:45:44,537 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:45:44,545 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 14:45:44,549 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 24 ms on localhost (1/1)
2016-03-03 14:45:44,550 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:45:44,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.027 s
2016-03-03 14:45:44,553 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.707558 s
2016-03-03 14:45:44,594 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 14:45:44,598 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 14:45:44,599 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 14:45:44,599 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 14:45:44,599 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:45:44,600 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:45:44,603 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 14:45:44,607 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=261960, maxMem=278302556
2016-03-03 14:45:44,608 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:45:44,609 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=265912, maxMem=278302556
2016-03-03 14:45:44,610 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:45:44,612 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:10544 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:45:44,613 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:44,615 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 14:45:44,616 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:45:44,616 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:45:44,618 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:45:44,619 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:45:44,672 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:45:44,672 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:45:44,681 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:45:44,681 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:45:44,688 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:10544 in memory (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:45:44,696 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_2_piece0 on localhost:10544 in memory (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:45:44,699 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:10544 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:45:44,799 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:45:44,799 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:45:44,802 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 183 ms on localhost (1/2)
2016-03-03 14:45:44,803 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.186 s
2016-03-03 14:45:44,803 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 186 ms on localhost (2/2)
2016-03-03 14:45:44,804 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:45:44,804 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:45:44,804 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:45:44,805 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:45:44,805 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:45:44,807 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:45:44,810 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 14:45:44,811 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=256614, maxMem=278302556
2016-03-03 14:45:44,812 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:45:44,814 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=258854, maxMem=278302556
2016-03-03 14:45:44,815 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:45:44,816 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:10544 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:45:44,817 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:44,818 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 14:45:44,819 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:45:44,819 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:45:44,821 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,822 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,823 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,824 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,824 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:45:44,824 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:45:44,824 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:45:44,824 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:45:44,839 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,839 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,839 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,839 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,842 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:45:44,842 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:45:44,842 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:45:44,842 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:45:44,862 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 14:45:44,862 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:45:44,862 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 14:45:44,862 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 14:45:44,864 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,864 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:45:44,865 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,866 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:45:44,867 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,868 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:45:44,868 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,868 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:45:44,869 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,870 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:45:44,872 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,872 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:45:44,873 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 52 ms on localhost (1/10)
2016-03-03 14:45:44,872 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,872 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 14:45:44,875 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:45:44,874 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,874 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 51 ms on localhost (2/10)
2016-03-03 14:45:44,876 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:45:44,877 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 14:45:44,877 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 54 ms on localhost (3/10)
2016-03-03 14:45:44,880 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 14:45:44,880 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:45:44,881 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,881 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:45:44,882 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 60 ms on localhost (4/10)
2016-03-03 14:45:44,882 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 19 ms on localhost (5/10)
2016-03-03 14:45:44,884 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:45:44,885 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,886 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:45:44,886 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 20 ms on localhost (6/10)
2016-03-03 14:45:44,885 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:45:44,888 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 20 ms on localhost (7/10)
2016-03-03 14:45:44,890 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 24 ms on localhost (8/10)
2016-03-03 14:45:44,892 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:45:44,892 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:45:44,893 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:45:44,896 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 15 ms on localhost (9/10)
2016-03-03 14:45:44,898 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 14:45:44,900 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 17 ms on localhost (10/10)
2016-03-03 14:45:44,900 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.081 s
2016-03-03 14:45:44,901 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:45:44,901 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.306596 s
2016-03-03 14:45:44,922 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=260216, maxMem=278302556
2016-03-03 14:45:44,923 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 14:45:44,957 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=490904, maxMem=278302556
2016-03-03 14:45:44,958 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 14:45:44,960 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:10544 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:45:44,962 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 14:45:44,975 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:45:44,985 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 14:45:44,986 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 14:45:44,987 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 14:45:44,987 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:45:44,989 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:45:44,990 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 14:45:44,992 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=510654, maxMem=278302556
2016-03-03 14:45:44,994 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 14:45:44,996 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=513902, maxMem=278302556
2016-03-03 14:45:44,997 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 14:45:45,000 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:10544 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:45:45,001 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 14:45:45,002 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 14:45:45,003 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 14:45:45,004 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:45:45,005 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:45:45,006 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:45:45,007 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 14:45:45,008 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 14:45:45,013 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:45:45,013 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:45:45,022 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 14:45:45,023 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 14:45:45,027 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 22 ms on localhost (1/2)
2016-03-03 14:45:45,028 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 22 ms on localhost (2/2)
2016-03-03 14:45:45,028 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.024 s
2016-03-03 14:45:45,028 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:45:45,029 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.043574 s
2016-03-03 14:48:31,312 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:48:31,330 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:48:31,332 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:48:31,333 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:48:31,335 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:48:31,335 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:48:31,336 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:48:31,336 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:48:31,337 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:48:31,338 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:48:31,339 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:48:31,339 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:48:31,340 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:48:31,340 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:48:31,341 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:48:31,341 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:48:31,342 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:48:31,342 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:48:31,342 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:48:31,343 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:48:31,343 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:48:31,344 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:48:31,344 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:48:31,344 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:48:31,345 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:48:31,345 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:48:31,397 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:48:31,400 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:48:31,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:48:31,474 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-493d5b54-3c11-4221-9950-6a27eec21a53\blockmgr-869ce47a-b829-4ff3-a041-362feb1ec793, already present as root for deletion.
2016-03-03 14:48:31,476 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:48:31,476 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:48:31,478 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:48:31,481 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:48:31,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:48:31,486 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:48:31,488 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:48:31,490 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:48:31,491 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d1e666de-ec00-4b8b-9117-aff711a14a78
2016-03-03 14:48:31,524 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:48:31,680 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d1e666de-ec00-4b8b-9117-aff711a14a78
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d1e666de-ec00-4b8b-9117-aff711a14a78
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:48:31,682 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-493d5b54-3c11-4221-9950-6a27eec21a53
2016-03-03 14:49:15,337 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:49:19,537 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:49:19,539 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:49:19,541 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:49:20,277 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:49:20,326 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 14:49:20,495 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:10702]
2016-03-03 14:49:20,501 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 10702.
2016-03-03 14:49:20,520 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:49:20,541 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:49:20,570 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c877f87-e779-4aa1-a7a0-5dfba4677290\blockmgr-017f2b10-ea1f-4afa-9341-b0ee23981002
2016-03-03 14:49:20,579 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:49:20,655 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c877f87-e779-4aa1-a7a0-5dfba4677290\httpd-43724948-5294-4843-991a-57d613dcbc82
2016-03-03 14:49:20,661 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:49:20,723 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:49:20,742 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:10703
2016-03-03 14:49:20,745 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 10703.
2016-03-03 14:49:20,770 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:49:20,922 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:49:20,942 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:49:20,943 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:49:20,945 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:49:21,151 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:10703/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987761150
2016-03-03 14:49:21,232 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:49:21,233 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:49:21,234 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:49:21,250 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:49:22,422 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10722.
2016-03-03 14:49:22,423 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 10722
2016-03-03 14:49:22,424 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:49:22,429 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:10722 with 265.4 MB RAM, BlockManagerId(driver, localhost, 10722)
2016-03-03 14:49:22,431 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:49:23,326 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:49:31,679 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:49:31,715 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:49:31,964 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:49:31,966 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:49:32,107 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:49:32,403 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:49:50,415 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:49:50,472 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:49:53,166 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:49:53,167 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:07,959 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:07,960 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:11,840 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:50:13,190 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:50:15,206 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:50:15,211 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:50:16,153 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:50:16,291 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:50:16,367 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:50:18,344 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:50:18,376 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:50:18,399 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:50:18,569 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:50:18,570 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:50:18,666 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:50:18,860 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:50:20,089 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:50:20,122 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:50:20,891 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:20,891 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:21,211 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:21,212 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:50:21,335 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:50:21,337 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:50:21,595 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:50:21,597 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:50:21,674 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:50:21,825 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:50:22,297 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 14:50:22,316 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 14:50:22,317 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 14:50:22,318 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:50:22,323 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:50:22,329 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 14:50:22,468 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:50:22,471 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:50:22,480 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:50:22,481 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:50:22,484 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:10722 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:50:22,486 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:22,491 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 14:50:22,493 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:50:22,501 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:50:22,530 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:50:22,533 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:50:22,534 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:50:22,535 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:50:22,539 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:50:22,539 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:50:22,539 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:50:22,540 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:50:22,547 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:10703/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456987761150
2016-03-03 14:50:22,629 INFO  [Executor task launch worker-1][org.apache.spark.util.Utils] Fetching http://169.254.236.187:10703/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c877f87-e779-4aa1-a7a0-5dfba4677290\userFiles-b0e3f1d5-f783-4efb-8d39-acdfcc575581\fetchFileTemp2584380954787087443.tmp
2016-03-03 14:50:26,808 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-8c877f87-e779-4aa1-a7a0-5dfba4677290/userFiles-b0e3f1d5-f783-4efb-8d39-acdfcc575581/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:50:26,830 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:50:26,830 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:50:26,831 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:50:26,830 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:50:26,833 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:50:26,834 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:50:26,835 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:50:26,838 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:50:26,839 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:50:26,840 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:50:26,843 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:50:26,844 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:50:26,845 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:50:26,845 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:50:26,847 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:50:26,848 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:50:26,849 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:50:26,849 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:50:26,850 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4315 ms on localhost (1/10)
2016-03-03 14:50:26,852 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 19 ms on localhost (2/10)
2016-03-03 14:50:26,853 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4320 ms on localhost (3/10)
2016-03-03 14:50:26,853 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4319 ms on localhost (4/10)
2016-03-03 14:50:26,854 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:50:26,855 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:50:26,856 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:50:26,861 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:50:26,863 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4349 ms on localhost (5/10)
2016-03-03 14:50:26,868 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 34 ms on localhost (6/10)
2016-03-03 14:50:26,869 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 15 ms on localhost (7/10)
2016-03-03 14:50:26,870 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 30 ms on localhost (8/10)
2016-03-03 14:50:26,872 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 24 ms on localhost (9/10)
2016-03-03 14:50:26,873 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 28 ms on localhost (10/10)
2016-03-03 14:50:26,874 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 4.371 s
2016-03-03 14:50:26,875 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:50:26,882 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.584346 s
2016-03-03 14:50:27,241 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:50:27,242 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:50:27,280 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:50:27,281 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:50:27,282 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:10722 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:50:27,284 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 14:50:27,331 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:50:27,342 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 14:50:27,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 14:50:27,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 14:50:27,345 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:50:27,346 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:50:27,347 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:50:27,353 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:50:27,354 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:50:27,357 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 14:50:27,358 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 14:50:27,360 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:10722 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:50:27,361 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:27,364 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:50:27,364 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:50:27,365 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:50:27,367 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:50:27,369 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:50:27,370 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:50:27,370 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:50:27,381 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:50:27,381 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:50:27,391 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:50:27,391 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:50:27,392 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:50:27,392 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:50:27,393 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:50:27,497 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:50:27,498 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:50:27,505 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 139 ms on localhost (1/2)
2016-03-03 14:50:27,507 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 138 ms on localhost (2/2)
2016-03-03 14:50:27,507 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.142 s
2016-03-03 14:50:27,508 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:50:27,508 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.165989 s
2016-03-03 14:50:27,509 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:10722 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:50:27,523 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 14:50:27,524 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 14:50:27,525 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 14:50:27,525 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:50:27,526 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:50:27,527 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:50:27,529 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 14:50:27,530 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:50:27,546 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 14:50:27,546 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 14:50:27,547 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:10722 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:50:27,548 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:27,549 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:50:27,549 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:50:27,550 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:50:27,552 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:50:27,553 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:50:27,556 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:50:27,561 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 14:50:27,563 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 12 ms on localhost (1/1)
2016-03-03 14:50:27,563 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.013 s
2016-03-03 14:50:27,563 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:50:27,564 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.040276 s
2016-03-03 14:50:27,604 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 14:50:27,608 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 14:50:27,609 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 14:50:27,610 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 14:50:27,610 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:50:27,611 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:50:27,614 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 14:50:27,621 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 14:50:27,622 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:50:27,625 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264012, maxMem=278302556
2016-03-03 14:50:27,625 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:50:27,628 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:10722 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:50:27,629 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:27,632 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 14:50:27,632 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:50:27,633 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:50:27,635 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:50:27,637 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:50:27,637 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:50:27,637 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:50:27,645 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:50:27,646 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:50:27,818 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:50:27,820 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:50:27,827 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 192 ms on localhost (1/2)
2016-03-03 14:50:27,828 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 191 ms on localhost (2/2)
2016-03-03 14:50:27,828 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:50:27,828 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.195 s
2016-03-03 14:50:27,830 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:50:27,831 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:50:27,832 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:50:27,833 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:50:27,839 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:50:27,844 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 14:50:27,847 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266292, maxMem=278302556
2016-03-03 14:50:27,848 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:50:27,851 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268532, maxMem=278302556
2016-03-03 14:50:27,852 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:50:27,855 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:10722 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:50:27,857 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:27,858 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 14:50:27,858 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:50:27,859 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:50:27,861 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,862 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,863 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,864 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,865 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:50:27,865 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:50:27,865 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:50:27,865 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:50:27,887 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,887 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,887 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,887 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,889 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:50:27,889 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:50:27,889 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:50:27,889 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 14:50:27,913 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 14:50:27,913 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:50:27,913 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 14:50:27,913 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 14:50:27,915 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,915 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:50:27,917 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,919 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:50:27,919 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,923 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:50:27,928 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,928 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:50:27,929 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,930 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:50:27,931 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 69 ms on localhost (1/10)
2016-03-03 14:50:27,922 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,932 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 10 ms
2016-03-03 14:50:27,934 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 14:50:27,930 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,935 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 14:50:27,935 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,936 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:50:27,938 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 14:50:27,933 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 72 ms on localhost (2/10)
2016-03-03 14:50:27,939 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 14:50:27,939 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 75 ms on localhost (3/10)
2016-03-03 14:50:27,939 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:50:27,941 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,941 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:50:27,943 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:50:27,944 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:50:27,944 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 81 ms on localhost (4/10)
2016-03-03 14:50:27,946 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,947 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:50:27,948 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:50:27,948 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 14:50:27,952 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:50:27,953 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 14:50:27,955 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 39 ms on localhost (5/10)
2016-03-03 14:50:27,957 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 38 ms on localhost (6/10)
2016-03-03 14:50:27,958 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 44 ms on localhost (7/10)
2016-03-03 14:50:27,960 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 36 ms on localhost (8/10)
2016-03-03 14:50:27,963 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 22 ms on localhost (9/10)
2016-03-03 14:50:27,963 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 21 ms on localhost (10/10)
2016-03-03 14:50:27,963 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.103 s
2016-03-03 14:50:27,964 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:50:27,965 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.360001 s
2016-03-03 14:50:27,991 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269894, maxMem=278302556
2016-03-03 14:50:27,992 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 14:50:28,029 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500582, maxMem=278302556
2016-03-03 14:50:28,030 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 14:50:28,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:10722 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:50:28,034 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 14:50:28,055 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:50:28,065 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 14:50:28,067 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 14:50:28,067 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 14:50:28,067 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:50:28,070 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:50:28,071 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 14:50:28,073 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520332, maxMem=278302556
2016-03-03 14:50:28,074 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 14:50:28,076 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523580, maxMem=278302556
2016-03-03 14:50:28,077 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 14:50:28,079 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:10722 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:50:28,080 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 14:50:28,081 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 14:50:28,081 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 14:50:28,082 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:50:28,083 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:50:28,085 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:50:28,085 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 14:50:28,085 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 14:50:28,090 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:50:28,090 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:50:28,102 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 14:50:28,104 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 14:50:28,106 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 22 ms on localhost (1/2)
2016-03-03 14:50:28,107 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 24 ms on localhost (2/2)
2016-03-03 14:50:28,108 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.024 s
2016-03-03 14:50:28,108 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:50:28,109 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.042971 s
2016-03-03 14:50:54,001 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:50:54,014 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:50:54,015 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:50:54,016 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:50:54,017 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:50:54,018 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:50:54,018 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:50:54,019 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:50:54,019 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:50:54,020 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:50:54,020 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:50:54,021 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:50:54,022 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:50:54,022 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:50:54,023 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:50:54,024 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:50:54,025 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:50:54,026 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:50:54,027 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:50:54,027 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:50:54,029 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:50:54,030 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:50:54,030 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:50:54,031 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:50:54,031 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:50:54,031 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:50:54,086 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:50:54,092 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:50:54,175 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:50:54,185 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c877f87-e779-4aa1-a7a0-5dfba4677290\blockmgr-017f2b10-ea1f-4afa-9341-b0ee23981002, already present as root for deletion.
2016-03-03 14:50:54,187 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:50:54,191 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:50:54,193 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:50:54,196 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:50:54,201 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:50:54,202 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:50:54,203 INFO  [sparkDriver-akka.actor.default-dispatcher-15][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:50:54,203 INFO  [sparkDriver-akka.actor.default-dispatcher-15][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:50:54,205 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c877f87-e779-4aa1-a7a0-5dfba4677290
2016-03-03 14:50:54,239 INFO  [sparkDriver-akka.actor.default-dispatcher-15][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:50:54,249 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d6647294-d365-4c1e-bdf1-3a2e61ebd4fb
2016-03-03 14:50:54,461 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d6647294-d365-4c1e-bdf1-3a2e61ebd4fb
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-d6647294-d365-4c1e-bdf1-3a2e61ebd4fb
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 14:55:33,979 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 14:55:36,948 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 14:55:36,950 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 14:55:36,951 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 14:55:37,668 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 14:55:37,715 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 14:55:37,886 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:11041]
2016-03-03 14:55:37,892 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 11041.
2016-03-03 14:55:37,911 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 14:55:37,926 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 14:55:37,952 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-6317d3e9-b3de-429a-b809-a06ef7265a70\blockmgr-88960321-235b-47a1-b323-0e7fd1a069b6
2016-03-03 14:55:37,959 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 14:55:38,050 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-6317d3e9-b3de-429a-b809-a06ef7265a70\httpd-20e91351-eef8-4872-80d2-2a9d61adfe1c
2016-03-03 14:55:38,057 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 14:55:38,119 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:55:38,139 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:11042
2016-03-03 14:55:38,142 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 11042.
2016-03-03 14:55:38,163 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 14:55:38,295 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 14:55:38,317 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 14:55:38,318 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 14:55:38,348 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 14:55:38,487 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:11042/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456988138487
2016-03-03 14:55:38,549 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:55:38,550 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:55:38,551 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 14:55:38,567 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 14:55:39,679 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11061.
2016-03-03 14:55:39,681 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 11061
2016-03-03 14:55:39,684 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 14:55:39,688 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:11061 with 265.4 MB RAM, BlockManagerId(driver, localhost, 11061)
2016-03-03 14:55:39,691 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 14:55:40,532 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 14:55:40,909 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:55:40,932 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:55:41,122 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:55:41,123 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:55:41,252 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:55:41,486 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:55:52,265 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:55:52,320 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:55:53,701 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:55:53,702 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:02,073 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:02,074 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:03,859 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:56:04,225 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 14:56:05,283 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:56:05,287 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:56:05,721 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:56:05,841 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:56:05,970 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 14:56:07,843 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 14:56:07,885 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 14:56:07,907 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 14:56:08,063 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 14:56:08,064 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 14:56:08,175 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:56:08,377 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 14:56:09,804 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 14:56:09,879 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 14:56:10,794 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:10,795 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:11,107 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:11,107 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 14:56:11,238 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 14:56:11,240 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 14:56:11,489 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 14:56:11,491 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 14:56:11,579 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 14:56:11,738 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 14:56:12,304 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 14:56:12,331 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 14:56:12,332 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 14:56:12,334 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:56:12,340 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:56:12,349 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 14:56:12,554 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 14:56:12,556 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 14:56:12,567 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 14:56:12,567 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 14:56:12,571 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:11061 (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:56:12,574 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:12,580 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 14:56:12,583 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 14:56:12,594 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 14:56:12,625 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:56:12,627 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:56:12,629 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:56:12,630 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:56:12,636 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 14:56:12,636 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 14:56:12,636 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 14:56:12,636 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 14:56:12,643 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:11042/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456988138487
2016-03-03 14:56:12,727 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:11042/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-6317d3e9-b3de-429a-b809-a06ef7265a70\userFiles-e5b2284f-013c-44dc-988d-3ffe2ad266c7\fetchFileTemp326677865630893844.tmp
2016-03-03 14:56:16,690 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-6317d3e9-b3de-429a-b809-a06ef7265a70/userFiles-e5b2284f-013c-44dc-988d-3ffe2ad266c7/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 14:56:16,712 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 14:56:16,713 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 14:56:16,712 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 14:56:16,713 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 14:56:16,716 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:56:16,718 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 14:56:16,720 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:56:16,721 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 14:56:16,724 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:56:16,726 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 14:56:16,729 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:56:16,731 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 14:56:16,732 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 14:56:16,733 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 14:56:16,734 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 14:56:16,736 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4102 ms on localhost (1/10)
2016-03-03 14:56:16,737 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 14:56:16,738 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4129 ms on localhost (2/10)
2016-03-03 14:56:16,740 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 14:56:16,742 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 14:56:16,743 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 14:56:16,744 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 14:56:16,747 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 14:56:16,747 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 14:56:16,748 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4121 ms on localhost (3/10)
2016-03-03 14:56:16,751 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4123 ms on localhost (4/10)
2016-03-03 14:56:16,753 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 33 ms on localhost (5/10)
2016-03-03 14:56:16,753 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 38 ms on localhost (6/10)
2016-03-03 14:56:16,755 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 26 ms on localhost (7/10)
2016-03-03 14:56:16,755 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 32 ms on localhost (8/10)
2016-03-03 14:56:16,756 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 17 ms on localhost (9/10)
2016-03-03 14:56:16,757 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 15 ms on localhost (10/10)
2016-03-03 14:56:16,758 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 4.162 s
2016-03-03 14:56:16,760 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 14:56:16,767 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.460886 s
2016-03-03 14:56:17,084 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 14:56:17,085 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 14:56:17,120 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 14:56:17,121 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 14:56:17,122 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:11061 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:56:17,124 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 14:56:17,172 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:56:17,182 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 14:56:17,184 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 14:56:17,185 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 14:56:17,185 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:56:17,187 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:56:17,188 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:56:17,196 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 14:56:17,197 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 14:56:17,266 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 14:56:17,270 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 14:56:17,275 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:11061 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 14:56:17,277 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:17,280 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:56:17,280 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 14:56:17,282 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 14:56:17,286 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:56:17,287 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:56:17,288 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 14:56:17,289 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 14:56:17,288 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:11061 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 14:56:17,302 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:56:17,302 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:56:17,314 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 14:56:17,315 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 14:56:17,315 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 14:56:17,316 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 14:56:17,314 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 14:56:17,348 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 14:56:17,348 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 14:56:17,352 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 67 ms on localhost (1/2)
2016-03-03 14:56:17,353 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 67 ms on localhost (2/2)
2016-03-03 14:56:17,354 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 14:56:17,354 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.070 s
2016-03-03 14:56:17,357 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.174194 s
2016-03-03 14:56:17,372 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 14:56:17,374 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 14:56:17,374 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 14:56:17,375 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:56:17,377 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:56:17,378 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 14:56:17,381 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 14:56:17,382 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 14:56:17,417 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 14:56:17,419 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 14:56:17,420 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:11061 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 14:56:17,422 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:17,424 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 14:56:17,425 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 14:56:17,427 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 14:56:17,429 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:56:17,431 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 14:56:17,435 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:56:17,439 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 14:56:17,452 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 22 ms on localhost (1/1)
2016-03-03 14:56:17,453 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 14:56:17,452 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.024 s
2016-03-03 14:56:17,456 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.082888 s
2016-03-03 14:56:17,491 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 14:56:17,496 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 14:56:17,498 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 14:56:17,498 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 14:56:17,498 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 14:56:17,501 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 14:56:17,505 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 14:56:17,511 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 14:56:17,514 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 14:56:17,520 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264012, maxMem=278302556
2016-03-03 14:56:17,523 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:56:17,528 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:11061 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 14:56:17,532 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:17,537 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 14:56:17,539 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 14:56:17,540 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 14:56:17,544 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:56:17,545 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 14:56:17,546 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 14:56:17,547 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 14:56:17,564 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:56:17,555 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:56:17,724 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 14:56:17,725 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 14:56:17,735 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 193 ms on localhost (1/2)
2016-03-03 14:56:17,737 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 192 ms on localhost (2/2)
2016-03-03 14:56:17,738 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 14:56:17,738 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.195 s
2016-03-03 14:56:17,741 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 14:56:17,742 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 14:56:17,743 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 14:56:17,744 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 14:56:17,748 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 14:56:17,751 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 14:56:17,753 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266292, maxMem=278302556
2016-03-03 14:56:17,754 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 14:56:17,757 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268532, maxMem=278302556
2016-03-03 14:56:17,758 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 14:56:17,761 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:11061 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 14:56:17,762 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:17,763 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 14:56:17,764 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 14:56:17,765 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 14:56:17,767 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,768 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,770 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,771 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,772 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 14:56:17,772 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 14:56:17,772 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 14:56:17,772 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 14:56:17,791 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,791 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,791 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,794 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:56:17,791 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,795 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 14:56:17,794 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:56:17,794 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 14:56:17,821 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 14:56:17,821 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 14:56:17,821 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 14:56:17,821 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 14:56:17,822 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,826 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 14:56:17,826 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,830 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,831 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,831 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 14:56:17,834 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 14:56:17,839 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,839 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 14:56:17,840 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 14:56:17,833 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,833 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 14:56:17,844 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,843 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 14:56:17,847 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 77 ms on localhost (1/10)
2016-03-03 14:56:17,843 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 14:56:17,849 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 81 ms on localhost (2/10)
2016-03-03 14:56:17,848 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,852 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 4 ms
2016-03-03 14:56:17,845 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 14:56:17,852 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 30 ms on localhost (3/10)
2016-03-03 14:56:17,850 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,858 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 14:56:17,858 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 14:56:17,859 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 14:56:17,863 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 14:56:17,858 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,866 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 14:56:17,865 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 98 ms on localhost (4/10)
2016-03-03 14:56:17,865 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 14:56:17,871 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 41 ms on localhost (5/10)
2016-03-03 14:56:17,873 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 14:56:17,875 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 14:56:17,876 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 43 ms on localhost (6/10)
2016-03-03 14:56:17,881 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 14:56:17,880 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 14:56:17,881 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 112 ms on localhost (7/10)
2016-03-03 14:56:17,886 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 61 ms on localhost (8/10)
2016-03-03 14:56:17,888 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 44 ms on localhost (9/10)
2016-03-03 14:56:17,889 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 31 ms on localhost (10/10)
2016-03-03 14:56:17,892 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 14:56:17,890 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.125 s
2016-03-03 14:56:17,895 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.402454 s
2016-03-03 14:56:17,921 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269894, maxMem=278302556
2016-03-03 14:56:17,922 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 14:56:17,961 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500582, maxMem=278302556
2016-03-03 14:56:17,962 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 14:56:17,965 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:11061 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 14:56:17,967 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 14:56:17,984 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 14:56:17,994 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 14:56:17,996 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 14:56:17,996 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 14:56:17,997 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 14:56:18,000 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 14:56:18,001 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 14:56:18,004 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520332, maxMem=278302556
2016-03-03 14:56:18,005 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 14:56:18,008 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523580, maxMem=278302556
2016-03-03 14:56:18,010 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 14:56:18,013 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:11061 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 14:56:18,016 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 14:56:18,017 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 14:56:18,018 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 14:56:18,020 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 14:56:18,022 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:56:18,023 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 14:56:18,024 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 14:56:18,029 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 14:56:18,039 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 14:56:18,041 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 14:56:18,046 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 14:56:18,057 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 14:56:18,048 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 26 ms on localhost (1/2)
2016-03-03 14:56:18,067 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 46 ms on localhost (2/2)
2016-03-03 14:56:18,070 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 14:56:18,069 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.049 s
2016-03-03 14:56:18,072 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.077285 s
2016-03-03 14:59:01,339 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 14:59:01,354 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 14:59:01,355 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 14:59:01,356 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 14:59:01,358 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 14:59:01,359 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 14:59:01,360 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 14:59:01,361 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 14:59:01,362 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 14:59:01,363 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 14:59:01,363 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 14:59:01,365 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 14:59:01,365 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 14:59:01,366 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 14:59:01,367 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 14:59:01,368 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 14:59:01,369 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 14:59:01,369 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 14:59:01,370 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 14:59:01,372 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 14:59:01,373 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 14:59:01,373 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 14:59:01,374 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 14:59:01,375 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 14:59:01,376 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 14:59:01,376 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 14:59:01,429 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 14:59:01,432 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 14:59:01,519 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 14:59:01,586 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-6317d3e9-b3de-429a-b809-a06ef7265a70\blockmgr-88960321-235b-47a1-b323-0e7fd1a069b6, already present as root for deletion.
2016-03-03 14:59:01,587 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 14:59:01,588 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 14:59:01,590 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 14:59:01,594 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 14:59:01,601 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 14:59:01,601 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 14:59:01,603 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 14:59:01,603 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 14:59:01,606 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-6317d3e9-b3de-429a-b809-a06ef7265a70
2016-03-03 14:59:01,641 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 14:59:01,707 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e3f4c0b9-2dc7-42ce-bd26-1174aaa99123
2016-03-03 14:59:01,894 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e3f4c0b9-2dc7-42ce-bd26-1174aaa99123
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e3f4c0b9-2dc7-42ce-bd26-1174aaa99123
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 15:04:18,327 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 15:04:22,820 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 15:04:22,821 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 15:04:22,823 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 15:04:23,641 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 15:04:23,690 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 15:04:23,873 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:11616]
2016-03-03 15:04:23,881 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 11616.
2016-03-03 15:04:23,901 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 15:04:23,917 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 15:04:23,949 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15\blockmgr-dc30a32e-033a-4bed-9ae2-0d9702329d2a
2016-03-03 15:04:23,955 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 15:04:24,058 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15\httpd-fd4e13f6-16f0-4671-bfba-db9c506f51cc
2016-03-03 15:04:24,065 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 15:04:24,132 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:04:24,154 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:11617
2016-03-03 15:04:24,157 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 11617.
2016-03-03 15:04:24,186 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 15:04:24,330 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:04:24,355 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 15:04:24,385 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 15:04:24,402 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 15:04:24,542 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:11617/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456988664541
2016-03-03 15:04:24,634 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:04:24,635 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:04:24,636 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:04:24,673 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 15:04:26,224 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11636.
2016-03-03 15:04:26,226 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 11636
2016-03-03 15:04:26,228 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 15:04:26,234 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:11636 with 265.4 MB RAM, BlockManagerId(driver, localhost, 11636)
2016-03-03 15:04:26,239 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 15:04:27,315 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 15:04:27,728 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:04:27,762 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:04:27,996 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:04:27,998 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:04:28,154 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:04:28,443 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:04:36,443 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:04:36,508 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:04:37,771 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:37,772 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:45,722 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:45,723 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:48,140 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:04:48,559 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 15:04:49,860 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:04:49,863 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:04:50,282 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:04:50,375 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:04:50,452 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 15:04:53,041 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 15:04:53,088 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:04:53,109 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:04:53,251 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:04:53,252 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:04:53,350 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:04:53,548 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:04:54,725 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:04:54,770 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:04:55,590 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:55,591 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:55,901 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:55,902 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:04:56,021 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 15:04:56,023 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:04:56,261 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:04:56,263 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:04:56,349 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:04:56,516 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:04:56,943 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 15:04:56,959 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 15:04:56,960 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 15:04:56,961 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:04:56,965 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:04:56,971 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 15:04:57,101 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 15:04:57,104 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 15:04:57,114 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(805) called with curMem=1096, maxMem=278302556
2016-03-03 15:04:57,115 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 805.0 B, free 265.4 MB)
2016-03-03 15:04:57,118 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:11636 (size: 805.0 B, free: 265.4 MB)
2016-03-03 15:04:57,121 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 15:04:57,127 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 15:04:57,129 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 15:04:57,137 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 15:04:57,165 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:04:57,168 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:04:57,169 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:04:57,171 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:04:57,176 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 15:04:57,176 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 15:04:57,176 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 15:04:57,176 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 15:04:57,183 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:11617/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456988664541
2016-03-03 15:04:57,260 INFO  [Executor task launch worker-1][org.apache.spark.util.Utils] Fetching http://169.254.236.187:11617/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15\userFiles-4c652181-5e46-487f-a260-ee532608d7aa\fetchFileTemp7370393264364248252.tmp
2016-03-03 15:05:01,016 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15/userFiles-4c652181-5e46-487f-a260-ee532608d7aa/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 15:05:01,036 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 15:05:01,037 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 15:05:01,036 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 15:05:01,039 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:05:01,041 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 15:05:01,046 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:05:01,048 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:05:01,051 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 15:05:01,038 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 15:05:01,057 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 15:05:01,054 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3881 ms on localhost (1/10)
2016-03-03 15:05:01,050 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 15:05:01,050 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 15:05:01,061 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:05:01,065 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:05:01,066 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 15:05:01,068 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 15:05:01,070 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 15:05:01,070 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:05:01,069 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 15:05:01,073 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 15:05:01,074 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 15:05:01,077 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3910 ms on localhost (2/10)
2016-03-03 15:05:01,079 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 15:05:01,079 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 41 ms on localhost (3/10)
2016-03-03 15:05:01,081 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 41 ms on localhost (4/10)
2016-03-03 15:05:01,084 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 19 ms on localhost (5/10)
2016-03-03 15:05:01,089 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3919 ms on localhost (6/10)
2016-03-03 15:05:01,089 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3940 ms on localhost (7/10)
2016-03-03 15:05:01,091 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 44 ms on localhost (8/10)
2016-03-03 15:05:01,093 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 33 ms on localhost (9/10)
2016-03-03 15:05:01,095 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 25 ms on localhost (10/10)
2016-03-03 15:05:01,096 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 3.957 s
2016-03-03 15:05:01,096 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 15:05:01,104 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.159698 s
2016-03-03 15:05:01,417 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1901, maxMem=278302556
2016-03-03 15:05:01,417 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 15:05:01,458 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232533, maxMem=278302556
2016-03-03 15:05:01,459 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 15:05:01,460 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:11636 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:05:01,462 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 15:05:01,511 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:05:01,520 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 15:05:01,523 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 15:05:01,523 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 15:05:01,524 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:05:01,526 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:05:01,527 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:05:01,533 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252283, maxMem=278302556
2016-03-03 15:05:01,534 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 15:05:01,620 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255251, maxMem=278302556
2016-03-03 15:05:01,621 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 15:05:01,626 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:11636 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 15:05:01,628 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 15:05:01,631 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:05:01,631 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 15:05:01,633 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 15:05:01,635 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:11636 in memory (size: 805.0 B, free: 265.4 MB)
2016-03-03 15:05:01,636 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:05:01,639 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:05:01,641 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 15:05:01,645 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 15:05:01,655 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:05:01,655 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:05:01,668 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 15:05:01,669 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 15:05:01,670 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 15:05:01,668 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 15:05:01,671 INFO  [Executor task launch worker-3][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 15:05:01,696 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 15:05:01,696 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 15:05:01,700 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 62 ms on localhost (1/2)
2016-03-03 15:05:01,701 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 67 ms on localhost (2/2)
2016-03-03 15:05:01,702 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.068 s
2016-03-03 15:05:01,702 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 15:05:01,703 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.181818 s
2016-03-03 15:05:01,715 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 15:05:01,717 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 15:05:01,718 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 15:05:01,718 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:05:01,721 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:05:01,722 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:05:01,725 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 15:05:01,726 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 15:05:01,751 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 15:05:01,754 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 15:05:01,756 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:11636 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 15:05:01,757 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 15:05:01,758 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:05:01,758 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 15:05:01,760 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 15:05:01,762 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:05:01,764 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 15:05:01,767 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:05:01,771 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 15:05:01,775 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 14 ms on localhost (1/1)
2016-03-03 15:05:01,776 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 15:05:01,775 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.014 s
2016-03-03 15:05:01,778 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.061497 s
2016-03-03 15:05:01,814 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 15:05:01,821 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 15:05:01,823 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 15:05:01,823 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 15:05:01,824 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 15:05:01,827 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 15:05:01,831 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 15:05:01,837 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 15:05:01,838 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 15:05:01,841 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264012, maxMem=278302556
2016-03-03 15:05:01,841 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:05:01,844 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:11636 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 15:05:01,845 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 15:05:01,847 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 15:05:01,848 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 15:05:01,849 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 15:05:01,852 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:05:01,853 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:05:01,854 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 15:05:01,854 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 15:05:01,862 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:05:01,862 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:05:02,035 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 15:05:02,035 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 15:05:02,042 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 191 ms on localhost (1/2)
2016-03-03 15:05:02,044 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 191 ms on localhost (2/2)
2016-03-03 15:05:02,046 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 15:05:02,045 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.195 s
2016-03-03 15:05:02,048 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 15:05:02,049 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 15:05:02,050 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 15:05:02,052 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 15:05:02,056 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 15:05:02,061 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 15:05:02,064 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266292, maxMem=278302556
2016-03-03 15:05:02,065 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:05:02,069 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268532, maxMem=278302556
2016-03-03 15:05:02,070 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 15:05:02,074 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:11636 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 15:05:02,076 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 15:05:02,077 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 15:05:02,078 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 15:05:02,079 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 15:05:02,082 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,084 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,085 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,086 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,087 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 15:05:02,088 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 15:05:02,088 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 15:05:02,088 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 15:05:02,107 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,107 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,107 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,109 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:05:02,107 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,111 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 15:05:02,109 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:05:02,109 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:05:02,134 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 15:05:02,134 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 15:05:02,134 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 15:05:02,134 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 15:05:02,136 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,139 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 15:05:02,140 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,142 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 15:05:02,144 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,144 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,146 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 15:05:02,146 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,149 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 15:05:02,148 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 15:05:02,148 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,153 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:05:02,155 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,156 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:05:02,159 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 15:05:02,152 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 15:05:02,151 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 66 ms on localhost (1/10)
2016-03-03 15:05:02,163 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 80 ms on localhost (2/10)
2016-03-03 15:05:02,162 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 15:05:02,165 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 79 ms on localhost (3/10)
2016-03-03 15:05:02,160 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,167 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,167 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 15:05:02,170 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 15:05:02,170 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:05:02,175 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 15:05:02,178 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 32 ms on localhost (4/10)
2016-03-03 15:05:02,175 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,180 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 15:05:02,175 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 15:05:02,180 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 99 ms on localhost (5/10)
2016-03-03 15:05:02,180 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:05:02,186 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:05:02,188 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 49 ms on localhost (6/10)
2016-03-03 15:05:02,192 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 15:05:02,185 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 15:05:02,192 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 57 ms on localhost (7/10)
2016-03-03 15:05:02,195 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 53 ms on localhost (8/10)
2016-03-03 15:05:02,199 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 29 ms on localhost (9/10)
2016-03-03 15:05:02,201 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 34 ms on localhost (10/10)
2016-03-03 15:05:02,203 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 15:05:02,202 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.121 s
2016-03-03 15:05:02,205 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.387850 s
2016-03-03 15:05:02,232 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269894, maxMem=278302556
2016-03-03 15:05:02,233 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 15:05:02,271 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500582, maxMem=278302556
2016-03-03 15:05:02,272 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 15:05:02,275 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:11636 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:05:02,277 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 15:05:02,295 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:05:02,305 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 15:05:02,307 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 15:05:02,308 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 15:05:02,309 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:05:02,312 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:05:02,313 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 15:05:02,316 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520332, maxMem=278302556
2016-03-03 15:05:02,317 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 15:05:02,320 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523580, maxMem=278302556
2016-03-03 15:05:02,321 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 15:05:02,324 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:11636 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 15:05:02,326 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 15:05:02,327 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 15:05:02,328 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 15:05:02,329 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 15:05:02,331 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:05:02,333 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:05:02,334 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 15:05:02,334 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 15:05:02,340 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:05:02,342 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:05:02,354 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 15:05:02,354 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 15:05:02,358 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 26 ms on localhost (1/2)
2016-03-03 15:05:02,363 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 32 ms on localhost (2/2)
2016-03-03 15:05:02,363 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 15:05:02,363 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.032 s
2016-03-03 15:05:02,365 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.059038 s
2016-03-03 15:07:35,858 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 15:07:35,874 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 15:07:35,874 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 15:07:35,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 15:07:35,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 15:07:35,875 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 15:07:35,876 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 15:07:35,876 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 15:07:35,877 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 15:07:35,878 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 15:07:35,879 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 15:07:35,882 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 15:07:35,884 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 15:07:35,885 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 15:07:35,887 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 15:07:35,888 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 15:07:35,889 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 15:07:35,892 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 15:07:35,893 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 15:07:35,895 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 15:07:35,896 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 15:07:35,897 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 15:07:35,898 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 15:07:35,898 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 15:07:35,900 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 15:07:35,901 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 15:07:35,960 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 15:07:35,967 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 15:07:36,109 WARN  [Thread-0][org.spark-project.jetty.util.thread.QueuedThreadPool] 2 threads could not be stopped
2016-03-03 15:07:36,122 INFO  [sparkDriver-akka.actor.default-dispatcher-14][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 15:07:36,128 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15\blockmgr-dc30a32e-033a-4bed-9ae2-0d9702329d2a, already present as root for deletion.
2016-03-03 15:07:36,129 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 15:07:36,130 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 15:07:36,132 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 15:07:36,136 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 15:07:36,138 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 15:07:36,140 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 15:07:36,144 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 15:07:36,146 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 15:07:36,147 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9436dd1d-8bec-4d7a-b2ba-57021c87ec2e
2016-03-03 15:07:36,171 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 15:07:36,405 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9436dd1d-8bec-4d7a-b2ba-57021c87ec2e
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-9436dd1d-8bec-4d7a-b2ba-57021c87ec2e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 15:07:36,408 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-7b7b15e8-33f2-41b9-ab81-18af0c2d5c15
2016-03-03 15:11:21,103 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 15:11:25,632 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 15:11:25,635 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 15:11:25,637 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 15:11:26,431 INFO  [sparkDriver-akka.actor.default-dispatcher-4][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 15:11:26,482 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Starting remoting
2016-03-03 15:11:26,689 INFO  [sparkDriver-akka.actor.default-dispatcher-4][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:12346]
2016-03-03 15:11:26,698 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 12346.
2016-03-03 15:11:26,723 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 15:11:26,746 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 15:11:26,773 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-06af3369-dc00-40a5-8950-e3dd992cdcff\blockmgr-ef9011bf-45f7-4d04-a090-199733ee6c14
2016-03-03 15:11:26,780 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 15:11:26,860 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-06af3369-dc00-40a5-8950-e3dd992cdcff\httpd-1cc4e2d2-7282-4558-99a3-5dc417fa8025
2016-03-03 15:11:26,869 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 15:11:26,939 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:11:26,962 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:12347
2016-03-03 15:11:26,964 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 12347.
2016-03-03 15:11:26,985 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 15:11:27,136 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:11:27,199 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 15:11:27,200 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 15:11:27,204 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 15:11:27,346 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:12347/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989087345
2016-03-03 15:11:27,418 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:11:27,419 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:11:27,420 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:11:27,437 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 15:11:28,671 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12366.
2016-03-03 15:11:28,673 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 12366
2016-03-03 15:11:28,675 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 15:11:28,684 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:12366 with 265.4 MB RAM, BlockManagerId(driver, localhost, 12366)
2016-03-03 15:11:28,688 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 15:11:29,707 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 15:11:30,091 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:11:30,115 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:11:30,295 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:11:30,297 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:11:30,440 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:11:30,713 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:11:38,469 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:11:38,530 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:11:39,965 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:39,966 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:47,885 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:47,887 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:50,136 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:11:51,090 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 15:11:52,020 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:11:52,023 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:11:52,321 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:11:52,423 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:11:52,518 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 15:11:54,949 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 15:11:54,993 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:11:55,021 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:11:55,200 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:11:55,200 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:11:55,301 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:11:55,498 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:11:56,696 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:11:56,731 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:11:57,531 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:57,532 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:57,856 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:57,857 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:11:57,983 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 15:11:57,985 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:11:58,241 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:11:58,243 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:11:58,313 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:11:58,492 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:11:58,984 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 15:11:59,009 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 15:11:59,010 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 15:11:59,011 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:11:59,016 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:11:59,025 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 15:11:59,322 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 15:11:59,325 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 15:11:59,338 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 15:11:59,339 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 15:11:59,343 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:12366 (size: 804.0 B, free: 265.4 MB)
2016-03-03 15:11:59,347 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 15:11:59,355 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 15:11:59,357 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 15:11:59,369 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 15:11:59,401 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:11:59,404 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:11:59,406 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:11:59,407 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:11:59,413 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 15:11:59,413 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 15:11:59,413 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 15:11:59,413 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 15:11:59,421 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:12347/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989087345
2016-03-03 15:11:59,481 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:12347/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-06af3369-dc00-40a5-8950-e3dd992cdcff\userFiles-3c113284-d58f-4aa5-8363-2d6702965328\fetchFileTemp1479900563849871022.tmp
2016-03-03 15:12:03,106 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-06af3369-dc00-40a5-8950-e3dd992cdcff/userFiles-3c113284-d58f-4aa5-8363-2d6702965328/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 15:12:03,133 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 15:12:03,134 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 15:12:03,133 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 15:12:03,137 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:12:03,134 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 15:12:03,140 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 15:12:03,141 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:12:03,145 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 15:12:03,148 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:12:03,149 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 15:12:03,153 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:12:03,155 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 15:12:03,158 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 15:12:03,159 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 15:12:03,158 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:12:03,162 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 15:12:03,163 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 15:12:03,164 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 15:12:03,162 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:12:03,167 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 15:12:03,169 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 15:12:03,172 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 15:12:03,174 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3766 ms on localhost (1/10)
2016-03-03 15:12:03,176 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 36 ms on localhost (2/10)
2016-03-03 15:12:03,178 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3794 ms on localhost (3/10)
2016-03-03 15:12:03,179 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3776 ms on localhost (4/10)
2016-03-03 15:12:03,183 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 36 ms on localhost (5/10)
2016-03-03 15:12:03,184 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3778 ms on localhost (6/10)
2016-03-03 15:12:03,185 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 33 ms on localhost (7/10)
2016-03-03 15:12:03,189 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 28 ms on localhost (8/10)
2016-03-03 15:12:03,193 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 36 ms on localhost (9/10)
2016-03-03 15:12:03,195 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 59 ms on localhost (10/10)
2016-03-03 15:12:03,197 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 15:12:03,198 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 3.826 s
2016-03-03 15:12:03,207 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.222035 s
2016-03-03 15:12:03,520 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 15:12:03,522 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 15:12:03,561 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 15:12:03,562 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 15:12:03,563 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:12366 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:12:03,565 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 15:12:03,605 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:12:03,616 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 15:12:03,618 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 15:12:03,619 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 15:12:03,620 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:12:03,622 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:12:03,623 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:12:03,630 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 15:12:03,631 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 15:12:03,698 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 15:12:03,700 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 15:12:03,706 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:12366 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 15:12:03,708 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 15:12:03,711 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:12:03,712 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 15:12:03,713 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 15:12:03,715 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:12366 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 15:12:03,716 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:12:03,718 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:12:03,719 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 15:12:03,719 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 15:12:03,731 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:12:03,731 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:12:03,742 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 15:12:03,742 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 15:12:03,742 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 15:12:03,743 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 15:12:03,744 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 15:12:03,769 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 15:12:03,769 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 15:12:03,776 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 58 ms on localhost (1/2)
2016-03-03 15:12:03,777 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.063 s
2016-03-03 15:12:03,777 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 63 ms on localhost (2/2)
2016-03-03 15:12:03,779 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 15:12:03,778 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.161925 s
2016-03-03 15:12:03,793 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 15:12:03,794 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 15:12:03,795 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 15:12:03,795 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:12:03,797 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:12:03,798 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:12:03,801 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 15:12:03,802 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 15:12:03,805 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 15:12:03,806 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 15:12:03,807 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:12366 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 15:12:03,808 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 15:12:03,809 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:12:03,810 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 15:12:03,811 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 15:12:03,813 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:12:03,814 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 15:12:03,818 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:12:03,823 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 15:12:03,825 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.013 s
2016-03-03 15:12:03,825 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 13 ms on localhost (1/1)
2016-03-03 15:12:03,826 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.033005 s
2016-03-03 15:12:03,827 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 15:12:03,872 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 15:12:03,876 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 15:12:03,877 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 15:12:03,877 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 15:12:03,878 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 15:12:03,880 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 15:12:03,883 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 15:12:03,889 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 15:12:03,889 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 15:12:03,892 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264012, maxMem=278302556
2016-03-03 15:12:03,893 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:12:03,896 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:12366 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 15:12:03,898 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 15:12:03,900 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 15:12:03,901 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 15:12:03,902 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 15:12:03,905 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:12:03,907 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:12:03,908 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 15:12:03,908 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 15:12:03,916 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:12:03,916 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:12:04,077 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 15:12:04,077 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 15:12:04,086 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 182 ms on localhost (1/2)
2016-03-03 15:12:04,088 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 181 ms on localhost (2/2)
2016-03-03 15:12:04,092 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 15:12:04,091 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.188 s
2016-03-03 15:12:04,095 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 15:12:04,096 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 15:12:04,097 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 15:12:04,098 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 15:12:04,103 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 15:12:04,107 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 15:12:04,109 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266292, maxMem=278302556
2016-03-03 15:12:04,110 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:12:04,113 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268532, maxMem=278302556
2016-03-03 15:12:04,114 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 15:12:04,116 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:12366 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 15:12:04,118 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 15:12:04,119 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 15:12:04,120 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 15:12:04,121 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 15:12:04,124 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,125 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,127 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,128 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,129 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 15:12:04,129 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 15:12:04,129 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 15:12:04,129 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 15:12:04,152 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,153 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,156 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 15:12:04,156 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 15:12:04,152 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,162 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 15 ms
2016-03-03 15:12:04,153 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,174 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 26 ms
2016-03-03 15:12:04,186 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 15:12:04,186 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 15:12:04,186 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 15:12:04,191 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,192 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 15:12:04,193 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,195 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 15:12:04,194 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 15:12:04,196 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,196 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,201 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,199 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 15:12:04,202 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:12:04,206 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,206 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 15:12:04,207 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 15:12:04,210 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 83 ms on localhost (1/10)
2016-03-03 15:12:04,212 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 15:12:04,212 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,213 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,214 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:12:04,213 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 15:12:04,215 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:12:04,218 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 15:12:04,214 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 15:12:04,222 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,220 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 15:12:04,220 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 92 ms on localhost (2/10)
2016-03-03 15:12:04,228 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 105 ms on localhost (3/10)
2016-03-03 15:12:04,229 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 36 ms on localhost (4/10)
2016-03-03 15:12:04,231 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 105 ms on localhost (5/10)
2016-03-03 15:12:04,225 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 15:12:04,225 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,223 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:12:04,236 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 13 ms
2016-03-03 15:12:04,237 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 15:12:04,236 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 12 ms
2016-03-03 15:12:04,242 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 15:12:04,233 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 42 ms on localhost (6/10)
2016-03-03 15:12:04,245 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 15:12:04,245 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 44 ms on localhost (7/10)
2016-03-03 15:12:04,247 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 51 ms on localhost (8/10)
2016-03-03 15:12:04,249 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 35 ms on localhost (9/10)
2016-03-03 15:12:04,251 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.129 s
2016-03-03 15:12:04,251 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 39 ms on localhost (10/10)
2016-03-03 15:12:04,252 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.379997 s
2016-03-03 15:12:04,253 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 15:12:04,273 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269894, maxMem=278302556
2016-03-03 15:12:04,274 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 15:12:04,315 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500582, maxMem=278302556
2016-03-03 15:12:04,316 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 15:12:04,319 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:12366 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:12:04,321 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 15:12:04,341 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:12:04,351 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 15:12:04,353 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 15:12:04,354 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 15:12:04,355 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:12:04,360 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:12:04,361 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 15:12:04,363 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520332, maxMem=278302556
2016-03-03 15:12:04,364 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 15:12:04,368 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523580, maxMem=278302556
2016-03-03 15:12:04,369 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 15:12:04,372 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:12366 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 15:12:04,374 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 15:12:04,375 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 15:12:04,376 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 15:12:04,377 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 15:12:04,379 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:12:04,381 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:12:04,382 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 15:12:04,382 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 15:12:04,387 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:12:04,387 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:12:04,397 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 15:12:04,397 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 15:12:04,404 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 23 ms on localhost (1/2)
2016-03-03 15:12:04,405 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 26 ms on localhost (2/2)
2016-03-03 15:12:04,405 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 15:12:04,407 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.029 s
2016-03-03 15:12:04,409 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.056540 s
2016-03-03 15:17:11,940 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 15:17:11,956 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 15:17:11,957 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 15:17:11,957 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 15:17:11,958 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 15:17:11,959 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 15:17:11,960 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 15:17:11,961 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 15:17:11,961 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 15:17:11,961 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 15:17:11,962 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 15:17:11,962 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 15:17:11,963 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 15:17:11,964 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 15:17:11,965 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 15:17:11,966 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 15:17:11,967 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 15:17:11,968 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 15:17:11,969 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 15:17:11,970 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 15:17:11,971 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 15:17:11,972 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 15:17:11,973 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 15:17:11,974 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 15:17:11,975 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 15:17:11,975 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 15:17:12,028 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 15:17:12,031 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 15:17:12,144 WARN  [Thread-0][org.spark-project.jetty.util.thread.QueuedThreadPool] 1 threads could not be stopped
2016-03-03 15:17:12,160 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 15:17:12,167 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-06af3369-dc00-40a5-8950-e3dd992cdcff\blockmgr-ef9011bf-45f7-4d04-a090-199733ee6c14, already present as root for deletion.
2016-03-03 15:17:12,168 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 15:17:12,169 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 15:17:12,171 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 15:17:12,175 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 15:17:12,178 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 15:17:12,182 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 15:17:12,182 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 15:17:12,184 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 15:17:12,187 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-06af3369-dc00-40a5-8950-e3dd992cdcff
2016-03-03 15:17:12,234 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 15:17:12,241 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e7b017b7-bd5e-4514-893e-a3e373845776
2016-03-03 15:17:12,460 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e7b017b7-bd5e-4514-893e-a3e373845776
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-e7b017b7-bd5e-4514-893e-a3e373845776
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 15:18:09,089 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 15:18:11,457 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 15:18:11,458 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 15:18:11,459 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 15:18:12,179 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 15:18:12,223 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 15:18:12,405 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:12650]
2016-03-03 15:18:12,413 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 12650.
2016-03-03 15:18:12,432 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 15:18:12,446 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 15:18:12,472 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f243d44b-9c3b-4037-ac86-f1459392cced\blockmgr-430efc15-7d22-41ec-9721-81de52fcfbbe
2016-03-03 15:18:12,479 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 15:18:12,627 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f243d44b-9c3b-4037-ac86-f1459392cced\httpd-a66316d1-4694-49ec-901c-d11ca6ae9872
2016-03-03 15:18:12,633 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 15:18:12,693 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:18:12,716 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:12651
2016-03-03 15:18:12,718 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 12651.
2016-03-03 15:18:12,741 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 15:18:12,864 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:18:12,880 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 15:18:12,880 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 15:18:12,913 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 15:18:13,069 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:12651/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989493068
2016-03-03 15:18:13,141 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:18:13,142 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:18:13,143 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:18:13,159 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 15:18:14,321 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12670.
2016-03-03 15:18:14,322 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 12670
2016-03-03 15:18:14,323 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 15:18:14,328 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:12670 with 265.4 MB RAM, BlockManagerId(driver, localhost, 12670)
2016-03-03 15:18:14,332 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 15:18:15,173 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 15:18:15,559 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:18:15,582 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:18:15,780 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:18:15,781 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:18:15,909 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:18:16,169 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:18:26,282 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:18:26,333 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:18:27,594 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:27,595 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:35,658 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:35,659 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:38,048 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:18:38,398 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 15:18:39,258 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:18:39,262 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:18:39,697 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:18:39,790 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:18:39,894 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 15:18:42,063 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 15:18:42,110 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:18:42,132 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:18:42,308 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:18:42,309 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:18:42,411 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:18:42,603 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:18:44,022 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:18:44,055 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:18:44,836 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:44,837 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:45,170 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:45,171 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:18:45,302 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 15:18:45,305 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:18:45,571 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:18:45,573 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:18:45,658 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:18:45,849 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:18:46,268 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 15:18:46,287 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 15:18:46,288 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 15:18:46,289 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:18:46,294 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:18:46,302 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 15:18:46,421 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 15:18:46,423 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 15:18:46,433 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(805) called with curMem=1096, maxMem=278302556
2016-03-03 15:18:46,434 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 805.0 B, free 265.4 MB)
2016-03-03 15:18:46,438 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:12670 (size: 805.0 B, free: 265.4 MB)
2016-03-03 15:18:46,440 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:46,448 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 15:18:46,450 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 15:18:46,459 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 15:18:46,487 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:18:46,490 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:18:46,492 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:18:46,494 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:18:46,499 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 15:18:46,499 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 15:18:46,499 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 15:18:46,499 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 15:18:46,505 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:12651/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989493068
2016-03-03 15:18:46,566 INFO  [Executor task launch worker-0][org.apache.spark.util.Utils] Fetching http://169.254.236.187:12651/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f243d44b-9c3b-4037-ac86-f1459392cced\userFiles-3a8ee831-f014-4bf4-af55-22e5c1a7e1e1\fetchFileTemp4650193188304447716.tmp
2016-03-03 15:18:50,088 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-f243d44b-9c3b-4037-ac86-f1459392cced/userFiles-3a8ee831-f014-4bf4-af55-22e5c1a7e1e1/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 15:18:50,111 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 15:18:50,111 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 15:18:50,111 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 15:18:50,111 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 15:18:50,114 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:18:50,115 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 15:18:50,117 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:18:50,119 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 15:18:50,121 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:18:50,124 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 15:18:50,125 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 15:18:50,125 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 15:18:50,127 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:18:50,129 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 15:18:50,130 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:18:50,132 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 15:18:50,133 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 15:18:50,132 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 15:18:50,133 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:18:50,136 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 15:18:50,136 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 15:18:50,141 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 15:18:50,142 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 3647 ms on localhost (1/10)
2016-03-03 15:18:50,144 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 3672 ms on localhost (2/10)
2016-03-03 15:18:50,145 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 3655 ms on localhost (3/10)
2016-03-03 15:18:50,147 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 3656 ms on localhost (4/10)
2016-03-03 15:18:50,148 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 32 ms on localhost (5/10)
2016-03-03 15:18:50,149 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 36 ms on localhost (6/10)
2016-03-03 15:18:50,151 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 25 ms on localhost (7/10)
2016-03-03 15:18:50,152 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 20 ms on localhost (8/10)
2016-03-03 15:18:50,153 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 24 ms on localhost (9/10)
2016-03-03 15:18:50,154 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 34 ms on localhost (10/10)
2016-03-03 15:18:50,155 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 3.694 s
2016-03-03 15:18:50,155 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 15:18:50,162 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 3.893192 s
2016-03-03 15:18:50,483 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1901, maxMem=278302556
2016-03-03 15:18:50,484 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 15:18:50,528 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232533, maxMem=278302556
2016-03-03 15:18:50,529 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 15:18:50,531 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:12670 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:18:50,533 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 15:18:50,581 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:18:50,593 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 15:18:50,596 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 15:18:50,596 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 15:18:50,597 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:18:50,600 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:18:50,601 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:18:50,609 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252283, maxMem=278302556
2016-03-03 15:18:50,610 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 15:18:50,679 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255251, maxMem=278302556
2016-03-03 15:18:50,681 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 15:18:50,686 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:12670 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 15:18:50,688 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:50,691 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:18:50,691 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 15:18:50,693 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 15:18:50,696 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:18:50,696 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:12670 in memory (size: 805.0 B, free: 265.4 MB)
2016-03-03 15:18:50,698 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:18:50,699 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 15:18:50,699 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 15:18:50,713 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:18:50,713 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:18:50,722 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 15:18:50,722 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 15:18:50,722 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 15:18:50,723 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 15:18:50,724 INFO  [Executor task launch worker-0][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 15:18:50,750 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 15:18:50,750 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 15:18:50,756 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 61 ms on localhost (1/2)
2016-03-03 15:18:50,758 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 60 ms on localhost (2/2)
2016-03-03 15:18:50,758 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 15:18:50,759 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.065 s
2016-03-03 15:18:50,761 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.167521 s
2016-03-03 15:18:50,777 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 15:18:50,779 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 15:18:50,779 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 15:18:50,780 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:18:50,782 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:18:50,783 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:18:50,785 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 15:18:50,786 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 15:18:50,831 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1817) called with curMem=258243, maxMem=278302556
2016-03-03 15:18:50,833 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1817.0 B, free 265.2 MB)
2016-03-03 15:18:50,834 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:12670 (size: 1817.0 B, free: 265.4 MB)
2016-03-03 15:18:50,836 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:50,837 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:18:50,838 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 15:18:50,839 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 15:18:50,841 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:18:50,842 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 15:18:50,846 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:18:50,850 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 15:18:50,853 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 12 ms on localhost (1/1)
2016-03-03 15:18:50,853 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.013 s
2016-03-03 15:18:50,853 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 15:18:50,855 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.076708 s
2016-03-03 15:18:50,897 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 15:18:50,902 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 15:18:50,904 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 15:18:50,904 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 15:18:50,905 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 15:18:50,907 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 15:18:50,912 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 15:18:50,918 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260060, maxMem=278302556
2016-03-03 15:18:50,919 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 15:18:50,922 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2281) called with curMem=264012, maxMem=278302556
2016-03-03 15:18:50,923 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:18:50,926 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:12670 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 15:18:50,928 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:50,930 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 15:18:50,931 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 15:18:50,932 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 15:18:50,935 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:18:50,937 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:18:50,938 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 15:18:50,938 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 15:18:50,947 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:18:50,947 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:18:51,105 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 15:18:51,105 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 15:18:51,113 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 180 ms on localhost (1/2)
2016-03-03 15:18:51,118 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 182 ms on localhost (2/2)
2016-03-03 15:18:51,119 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.186 s
2016-03-03 15:18:51,123 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 15:18:51,120 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 15:18:51,125 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 15:18:51,128 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 15:18:51,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 15:18:51,135 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 15:18:51,137 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 15:18:51,140 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266293, maxMem=278302556
2016-03-03 15:18:51,141 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:18:51,144 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268533, maxMem=278302556
2016-03-03 15:18:51,145 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 15:18:51,147 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:12670 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 15:18:51,148 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:51,149 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 15:18:51,150 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 15:18:51,151 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 15:18:51,153 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,155 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,156 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,157 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,158 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 15:18:51,159 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 15:18:51,158 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 15:18:51,158 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 15:18:51,179 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,179 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,179 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,182 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 15:18:51,179 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,182 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 15:18:51,182 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 15:18:51,184 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 15:18:51,211 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 15:18:51,211 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 15:18:51,213 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,211 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 15:18:51,211 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 15:18:51,215 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 15:18:51,214 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,218 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 15:18:51,220 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,220 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 0 ms
2016-03-03 15:18:51,219 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,223 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,225 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 15:18:51,226 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 15:18:51,225 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,230 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 15:18:51,226 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 15:18:51,232 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,231 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 75 ms on localhost (1/10)
2016-03-03 15:18:51,230 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 15:18:51,233 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:18:51,236 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,238 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,240 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 15:18:51,241 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 15:18:51,239 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 15:18:51,245 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 15:18:51,240 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:18:51,248 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 15:18:51,247 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,249 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 15:18:51,252 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:18:51,254 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 2 ms
2016-03-03 15:18:51,253 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 99 ms on localhost (2/10)
2016-03-03 15:18:51,256 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 15:18:51,261 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 103 ms on localhost (3/10)
2016-03-03 15:18:51,262 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 15:18:51,263 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 49 ms on localhost (4/10)
2016-03-03 15:18:51,267 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 114 ms on localhost (5/10)
2016-03-03 15:18:51,268 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 43 ms on localhost (6/10)
2016-03-03 15:18:51,269 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 49 ms on localhost (7/10)
2016-03-03 15:18:51,272 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 59 ms on localhost (8/10)
2016-03-03 15:18:51,273 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 33 ms on localhost (9/10)
2016-03-03 15:18:51,274 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.122 s
2016-03-03 15:18:51,274 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 39 ms on localhost (10/10)
2016-03-03 15:18:51,275 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.377264 s
2016-03-03 15:18:51,276 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 15:18:51,305 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269895, maxMem=278302556
2016-03-03 15:18:51,306 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 15:18:51,347 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500583, maxMem=278302556
2016-03-03 15:18:51,348 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 15:18:51,351 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:12670 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:18:51,353 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 15:18:51,371 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:18:51,381 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 15:18:51,382 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 15:18:51,383 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 15:18:51,383 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:18:51,386 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:18:51,387 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 15:18:51,389 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520333, maxMem=278302556
2016-03-03 15:18:51,390 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 15:18:51,394 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523581, maxMem=278302556
2016-03-03 15:18:51,395 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 15:18:51,399 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:12670 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 15:18:51,402 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 15:18:51,404 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 15:18:51,405 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 15:18:51,407 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 15:18:51,409 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:18:51,410 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:18:51,411 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 15:18:51,411 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 15:18:51,417 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:18:51,420 INFO  [Executor task launch worker-0][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:18:51,429 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 15:18:51,431 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 15:18:51,434 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 25 ms on localhost (1/2)
2016-03-03 15:18:51,437 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 28 ms on localhost (2/2)
2016-03-03 15:18:51,437 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 15:18:51,437 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.030 s
2016-03-03 15:18:51,440 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.058761 s
2016-03-03 15:20:08,897 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 15:20:08,956 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 15:20:08,957 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 15:20:08,961 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 15:20:08,962 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 15:20:08,962 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 15:20:08,963 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 15:20:08,964 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 15:20:08,964 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 15:20:08,965 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 15:20:08,966 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 15:20:08,967 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 15:20:08,968 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 15:20:08,970 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 15:20:08,971 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 15:20:08,972 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 15:20:08,972 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 15:20:08,974 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 15:20:08,974 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 15:20:08,975 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 15:20:08,976 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 15:20:08,978 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 15:20:08,979 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 15:20:08,979 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 15:20:08,981 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 15:20:08,982 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 15:20:09,034 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 15:20:09,036 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 15:20:09,109 INFO  [sparkDriver-akka.actor.default-dispatcher-16][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 15:20:09,116 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f243d44b-9c3b-4037-ac86-f1459392cced\blockmgr-430efc15-7d22-41ec-9721-81de52fcfbbe, already present as root for deletion.
2016-03-03 15:20:09,118 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 15:20:09,132 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 15:20:09,134 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 15:20:09,145 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 15:20:09,147 INFO  [sparkDriver-akka.actor.default-dispatcher-4][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 15:20:09,149 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 15:20:09,160 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c0a467d-992a-4de6-acfa-32d95cb19ae0
2016-03-03 15:20:09,172 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 15:20:09,173 INFO  [sparkDriver-akka.actor.default-dispatcher-17][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 15:20:09,297 INFO  [sparkDriver-akka.actor.default-dispatcher-3][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 15:20:09,490 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c0a467d-992a-4de6-acfa-32d95cb19ae0
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-8c0a467d-992a-4de6-acfa-32d95cb19ae0
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2016-03-03 15:20:09,496 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f243d44b-9c3b-4037-ac86-f1459392cced
2016-03-03 15:21:50,032 INFO  [main][org.apache.spark.SparkContext] Running Spark version 1.4.1
2016-03-03 15:21:54,348 INFO  [main][org.apache.spark.SecurityManager] Changing view acls to: admin
2016-03-03 15:21:54,349 INFO  [main][org.apache.spark.SecurityManager] Changing modify acls to: admin
2016-03-03 15:21:54,350 INFO  [main][org.apache.spark.SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
2016-03-03 15:21:55,076 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.event.slf4j.Slf4jLogger] Slf4jLogger started
2016-03-03 15:21:55,122 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Starting remoting
2016-03-03 15:21:55,288 INFO  [sparkDriver-akka.actor.default-dispatcher-2][Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@169.254.236.187:12808]
2016-03-03 15:21:55,295 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'sparkDriver' on port 12808.
2016-03-03 15:21:55,314 INFO  [main][org.apache.spark.SparkEnv] Registering MapOutputTracker
2016-03-03 15:21:55,328 INFO  [main][org.apache.spark.SparkEnv] Registering BlockManagerMaster
2016-03-03 15:21:55,353 INFO  [main][org.apache.spark.storage.DiskBlockManager] Created local directory at C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b\blockmgr-5bd0945a-c894-49fd-82c9-07dc6ef6dfcf
2016-03-03 15:21:55,366 INFO  [main][org.apache.spark.storage.MemoryStore] MemoryStore started with capacity 265.4 MB
2016-03-03 15:21:55,448 INFO  [main][org.apache.spark.HttpFileServer] HTTP File server directory is C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b\httpd-159d528b-dec2-4edd-802c-985dfe70daa6
2016-03-03 15:21:55,454 INFO  [main][org.apache.spark.HttpServer] Starting HTTP Server
2016-03-03 15:21:55,519 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:21:55,535 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SocketConnector@0.0.0.0:12809
2016-03-03 15:21:55,538 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'HTTP file server' on port 12809.
2016-03-03 15:21:55,559 INFO  [main][org.apache.spark.SparkEnv] Registering OutputCommitCoordinator
2016-03-03 15:21:55,689 INFO  [main][org.spark-project.jetty.server.Server] jetty-8.y.z-SNAPSHOT
2016-03-03 15:21:55,708 INFO  [main][org.spark-project.jetty.server.AbstractConnector] Started SelectChannelConnector@0.0.0.0:4040
2016-03-03 15:21:55,737 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'SparkUI' on port 4040.
2016-03-03 15:21:55,740 INFO  [main][org.apache.spark.ui.SparkUI] Started SparkUI at http://169.254.236.187:4040
2016-03-03 15:21:55,880 INFO  [main][org.apache.spark.SparkContext] Added JAR file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/target/scala-2.10/sbdsample-assembly-0.1-SNAPSHOT.jar at http://169.254.236.187:12809/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989715880
2016-03-03 15:21:55,946 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool worker, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:21:55,947 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool warmup, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:21:55,948 INFO  [main][org.apache.spark.scheduler.FairSchedulableBuilder] Created pool default, schedulingMode: FAIR, minShare: 0, weight: 1
2016-03-03 15:21:55,963 INFO  [main][org.apache.spark.executor.Executor] Starting executor ID driver on host localhost
2016-03-03 15:21:57,123 INFO  [main][org.apache.spark.util.Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12828.
2016-03-03 15:21:57,125 INFO  [main][org.apache.spark.network.netty.NettyBlockTransferService] Server created on 12828
2016-03-03 15:21:57,126 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Trying to register BlockManager
2016-03-03 15:21:57,131 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerMasterEndpoint] Registering block manager localhost:12828 with 265.4 MB RAM, BlockManagerId(driver, localhost, 12828)
2016-03-03 15:21:57,134 INFO  [main][org.apache.spark.storage.BlockManagerMaster] Registered BlockManager
2016-03-03 15:21:58,001 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing execution hive, version 0.13.1
2016-03-03 15:21:58,376 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:21:58,402 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:21:58,600 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:21:58,602 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:21:58,732 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:21:58,990 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:22:07,994 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:22:08,045 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:22:09,320 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:09,322 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:17,703 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:17,704 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:19,980 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:22:20,378 WARN  [main][org.apache.hadoop.hive.metastore.ObjectStore] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
2016-03-03 15:22:21,309 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:22:21,314 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:22:21,769 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:22:21,872 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:22:21,967 INFO  [main][org.apache.spark.sql.hive.HiveContext] Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
2016-03-03 15:22:24,264 WARN  [main][org.apache.hadoop.util.NativeCodeLoader] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-03-03 15:22:24,310 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-03-03 15:22:24,332 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] ObjectStore, initialize called
2016-03-03 15:22:24,498 INFO  [main][DataNucleus.Persistence] Property datanucleus.cache.level2 unknown - will be ignored
2016-03-03 15:22:24,498 INFO  [main][DataNucleus.Persistence] Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2016-03-03 15:22:24,585 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:22:24,784 WARN  [main][DataNucleus.Connection] BoneCP specified but not present in CLASSPATH (or one of dependencies)
2016-03-03 15:22:25,861 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-03-03 15:22:25,903 INFO  [main][org.apache.hadoop.hive.metastore.MetaStoreDirectSql] MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
2016-03-03 15:22:26,707 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:26,709 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:27,039 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:27,040 INFO  [main][DataNucleus.Datastore] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2016-03-03 15:22:27,162 INFO  [main][DataNucleus.Query] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2016-03-03 15:22:27,165 INFO  [main][org.apache.hadoop.hive.metastore.ObjectStore] Initialized ObjectStore
2016-03-03 15:22:27,431 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added admin role in metastore
2016-03-03 15:22:27,433 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] Added public role in metastore
2016-03-03 15:22:27,523 INFO  [main][org.apache.hadoop.hive.metastore.HiveMetaStore] No user is added in admin role, since config is empty
2016-03-03 15:22:27,690 INFO  [main][org.apache.hadoop.hive.ql.session.SessionState] No Tez session required at this point. hive.execution.engine=mr.
2016-03-03 15:22:28,160 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:89
2016-03-03 15:22:28,177 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 0 (count at Main.scala:89) with 10 output partitions (allowLocal=false)
2016-03-03 15:22:28,178 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 0(count at Main.scala:89)
2016-03-03 15:22:28,179 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:28,183 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:28,190 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88), which has no missing parents
2016-03-03 15:22:28,296 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1096) called with curMem=0, maxMem=278302556
2016-03-03 15:22:28,299 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0 stored as values in memory (estimated size 1096.0 B, free 265.4 MB)
2016-03-03 15:22:28,309 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(804) called with curMem=1096, maxMem=278302556
2016-03-03 15:22:28,310 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 804.0 B, free 265.4 MB)
2016-03-03 15:22:28,316 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:12828 (size: 804.0 B, free: 265.4 MB)
2016-03-03 15:22:28,331 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:28,339 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:88)
2016-03-03 15:22:28,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 0.0 with 10 tasks
2016-03-03 15:22:28,354 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_0 tasks to pool default
2016-03-03 15:22:28,384 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:22:28,387 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:22:28,389 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:22:28,390 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:22:28,396 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 2.0 in stage 0.0 (TID 2)
2016-03-03 15:22:28,396 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 0.0 (TID 1)
2016-03-03 15:22:28,396 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 0.0 in stage 0.0 (TID 0)
2016-03-03 15:22:28,396 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 3.0 in stage 0.0 (TID 3)
2016-03-03 15:22:28,405 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Fetching http://169.254.236.187:12809/jars/sbdsample-assembly-0.1-SNAPSHOT.jar with timestamp 1456989715880
2016-03-03 15:22:28,464 INFO  [Executor task launch worker-3][org.apache.spark.util.Utils] Fetching http://169.254.236.187:12809/jars/sbdsample-assembly-0.1-SNAPSHOT.jar to C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b\userFiles-ff539193-93e7-44f3-b5ad-df56e6dfed88\fetchFileTemp3883325805691475145.tmp
2016-03-03 15:22:32,343 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Adding file:/C:/Users/IBM_ADMIN/AppData/Local/Temp/spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b/userFiles-ff539193-93e7-44f3-b5ad-df56e6dfed88/sbdsample-assembly-0.1-SNAPSHOT.jar to class loader
2016-03-03 15:22:32,366 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 0.0 in stage 0.0 (TID 0). 658 bytes result sent to driver
2016-03-03 15:22:32,366 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 2.0 in stage 0.0 (TID 2). 658 bytes result sent to driver
2016-03-03 15:22:32,366 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 0.0 (TID 1). 658 bytes result sent to driver
2016-03-03 15:22:32,366 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 3.0 in stage 0.0 (TID 3). 658 bytes result sent to driver
2016-03-03 15:22:32,369 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:22:32,371 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 4.0 in stage 0.0 (TID 4)
2016-03-03 15:22:32,373 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:22:32,375 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 4.0 in stage 0.0 (TID 4). 658 bytes result sent to driver
2016-03-03 15:22:32,377 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 5.0 in stage 0.0 (TID 5)
2016-03-03 15:22:32,376 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:22:32,380 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 6.0 in stage 0.0 (TID 6)
2016-03-03 15:22:32,382 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:22:32,383 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 5.0 in stage 0.0 (TID 5). 658 bytes result sent to driver
2016-03-03 15:22:32,386 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1388 bytes)
2016-03-03 15:22:32,385 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 6.0 in stage 0.0 (TID 6). 658 bytes result sent to driver
2016-03-03 15:22:32,385 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 7.0 in stage 0.0 (TID 7)
2016-03-03 15:22:32,387 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 0.0 (TID 8)
2016-03-03 15:22:32,389 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 4020 ms on localhost (1/10)
2016-03-03 15:22:32,392 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 0.0 (TID 4) in 24 ms on localhost (2/10)
2016-03-03 15:22:32,393 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 7.0 in stage 0.0 (TID 7). 658 bytes result sent to driver
2016-03-03 15:22:32,393 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 0.0 (TID 8). 658 bytes result sent to driver
2016-03-03 15:22:32,394 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1392 bytes)
2016-03-03 15:22:32,396 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 9.0 in stage 0.0 (TID 9)
2016-03-03 15:22:32,401 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 9.0 in stage 0.0 (TID 9). 658 bytes result sent to driver
2016-03-03 15:22:32,401 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 0.0 (TID 1) in 4015 ms on localhost (3/10)
2016-03-03 15:22:32,402 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 0.0 (TID 3) in 4013 ms on localhost (4/10)
2016-03-03 15:22:32,403 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 0.0 (TID 5) in 31 ms on localhost (5/10)
2016-03-03 15:22:32,405 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 0.0 (TID 8) in 20 ms on localhost (6/10)
2016-03-03 15:22:32,407 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 0.0 (TID 6) in 31 ms on localhost (7/10)
2016-03-03 15:22:32,409 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 0.0 (TID 9) in 16 ms on localhost (8/10)
2016-03-03 15:22:32,415 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 0.0 (TID 2) in 4026 ms on localhost (9/10)
2016-03-03 15:22:32,417 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 0.0 (TID 7) in 35 ms on localhost (10/10)
2016-03-03 15:22:32,418 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 0 (count at Main.scala:89) finished in 4.061 s
2016-03-03 15:22:32,419 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool default
2016-03-03 15:22:32,427 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 0 finished: count at Main.scala:89, took 4.266326 s
2016-03-03 15:22:32,753 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230632) called with curMem=1900, maxMem=278302556
2016-03-03 15:22:32,754 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1 stored as values in memory (estimated size 225.2 KB, free 265.2 MB)
2016-03-03 15:22:32,791 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=232532, maxMem=278302556
2016-03-03 15:22:32,792 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 265.2 MB)
2016-03-03 15:22:32,794 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:12828 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:22:32,796 INFO  [main][org.apache.spark.SparkContext] Created broadcast 1 from textFile at Main.scala:94
2016-03-03 15:22:32,844 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:22:32,856 INFO  [main][org.apache.spark.SparkContext] Starting job: count at Main.scala:95
2016-03-03 15:22:32,858 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 1 (count at Main.scala:95) with 2 output partitions (allowLocal=false)
2016-03-03 15:22:32,858 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 1(count at Main.scala:95)
2016-03-03 15:22:32,859 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:32,861 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:32,862 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:22:32,868 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2968) called with curMem=252282, maxMem=278302556
2016-03-03 15:22:32,869 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.2 MB)
2016-03-03 15:22:32,953 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1757) called with curMem=255250, maxMem=278302556
2016-03-03 15:22:32,954 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1757.0 B, free 265.2 MB)
2016-03-03 15:22:32,957 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:12828 (size: 1757.0 B, free: 265.4 MB)
2016-03-03 15:22:32,958 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:32,962 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:22:32,962 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 1.0 with 2 tasks
2016-03-03 15:22:32,964 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_1 tasks to pool default
2016-03-03 15:22:32,968 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:22:32,969 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:22:32,970 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 1.0 (TID 11)
2016-03-03 15:22:32,970 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 1.0 (TID 10)
2016-03-03 15:22:32,971 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_0_piece0 on localhost:12828 in memory (size: 804.0 B, free: 265.4 MB)
2016-03-03 15:22:32,983 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:22:32,983 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:22:32,992 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-03-03 15:22:32,993 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-03-03 15:22:32,993 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-03-03 15:22:32,994 INFO  [Executor task launch worker-1][org.apache.hadoop.conf.Configuration.deprecation] mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-03-03 15:22:32,994 INFO  [Executor task launch worker-2][org.apache.hadoop.conf.Configuration.deprecation] mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-03-03 15:22:33,019 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 1.0 (TID 11). 1830 bytes result sent to driver
2016-03-03 15:22:33,019 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 1.0 (TID 10). 1830 bytes result sent to driver
2016-03-03 15:22:33,023 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 1.0 (TID 10) in 57 ms on localhost (1/2)
2016-03-03 15:22:33,024 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 1.0 (TID 11) in 56 ms on localhost (2/2)
2016-03-03 15:22:33,025 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 1 (count at Main.scala:95) finished in 0.060 s
2016-03-03 15:22:33,025 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool default
2016-03-03 15:22:33,026 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 1 finished: count at Main.scala:95, took 0.170200 s
2016-03-03 15:22:33,039 INFO  [main][org.apache.spark.SparkContext] Starting job: first at Main.scala:96
2016-03-03 15:22:33,040 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 2 (first at Main.scala:96) with 1 output partitions (allowLocal=true)
2016-03-03 15:22:33,040 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 2(first at Main.scala:96)
2016-03-03 15:22:33,041 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:33,043 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:33,045 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94), which has no missing parents
2016-03-03 15:22:33,048 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3136) called with curMem=255107, maxMem=278302556
2016-03-03 15:22:33,049 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 265.2 MB)
2016-03-03 15:22:33,053 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1819) called with curMem=258243, maxMem=278302556
2016-03-03 15:22:33,054 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_3_piece0 stored as bytes in memory (estimated size 1819.0 B, free 265.2 MB)
2016-03-03 15:22:33,056 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_3_piece0 in memory on localhost:12828 (size: 1819.0 B, free: 265.4 MB)
2016-03-03 15:22:33,057 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 3 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:33,058 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at textFile at Main.scala:94)
2016-03-03 15:22:33,059 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 2.0 with 1 tasks
2016-03-03 15:22:33,060 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_2 tasks to pool default
2016-03-03 15:22:33,062 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 2.0 (TID 12, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:22:33,064 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 2.0 (TID 12)
2016-03-03 15:22:33,068 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:22:33,072 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 2.0 (TID 12). 1860 bytes result sent to driver
2016-03-03 15:22:33,074 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 2 (first at Main.scala:96) finished in 0.014 s
2016-03-03 15:22:33,074 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 2.0 (TID 12) in 13 ms on localhost (1/1)
2016-03-03 15:22:33,075 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 2 finished: first at Main.scala:96, took 0.035792 s
2016-03-03 15:22:33,076 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool default
2016-03-03 15:22:33,116 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:98
2016-03-03 15:22:33,121 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Registering RDD 8 (map at Main.scala:97)
2016-03-03 15:22:33,123 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 3 (collect at Main.scala:98) with 10 output partitions (allowLocal=false)
2016-03-03 15:22:33,123 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 4(collect at Main.scala:98)
2016-03-03 15:22:33,124 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List(ShuffleMapStage 3)
2016-03-03 15:22:33,126 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List(ShuffleMapStage 3)
2016-03-03 15:22:33,130 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97), which has no missing parents
2016-03-03 15:22:33,135 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3952) called with curMem=260062, maxMem=278302556
2016-03-03 15:22:33,136 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 265.2 MB)
2016-03-03 15:22:33,139 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2280) called with curMem=264014, maxMem=278302556
2016-03-03 15:22:33,140 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:22:33,143 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_4_piece0 in memory on localhost:12828 (size: 2.2 KB, free: 265.4 MB)
2016-03-03 15:22:33,144 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 4 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:33,147 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[8] at map at Main.scala:97)
2016-03-03 15:22:33,148 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 3.0 with 2 tasks
2016-03-03 15:22:33,149 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_3 tasks to pool default
2016-03-03 15:22:33,152 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:22:33,153 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1524 bytes)
2016-03-03 15:22:33,154 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 3.0 (TID 14)
2016-03-03 15:22:33,154 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 3.0 (TID 13)
2016-03-03 15:22:33,161 INFO  [Executor task launch worker-2][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:22:33,161 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:22:33,326 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 3.0 (TID 14). 2010 bytes result sent to driver
2016-03-03 15:22:33,326 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 3.0 (TID 13). 2010 bytes result sent to driver
2016-03-03 15:22:33,332 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 3.0 (TID 14) in 180 ms on localhost (1/2)
2016-03-03 15:22:33,336 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 3.0 (TID 13) in 185 ms on localhost (2/2)
2016-03-03 15:22:33,337 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 3.0, whose tasks have all completed, from pool default
2016-03-03 15:22:33,336 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ShuffleMapStage 3 (map at Main.scala:97) finished in 0.187 s
2016-03-03 15:22:33,340 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] looking for newly runnable stages
2016-03-03 15:22:33,341 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] running: Set()
2016-03-03 15:22:33,342 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] waiting: Set(ResultStage 4)
2016-03-03 15:22:33,344 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] failed: Set()
2016-03-03 15:22:33,349 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents for ResultStage 4: List()
2016-03-03 15:22:33,353 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97), which is now runnable
2016-03-03 15:22:33,355 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2240) called with curMem=266294, maxMem=278302556
2016-03-03 15:22:33,357 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5 stored as values in memory (estimated size 2.2 KB, free 265.2 MB)
2016-03-03 15:22:33,360 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1362) called with curMem=268534, maxMem=278302556
2016-03-03 15:22:33,361 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1362.0 B, free 265.2 MB)
2016-03-03 15:22:33,364 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_5_piece0 in memory on localhost:12828 (size: 1362.0 B, free: 265.4 MB)
2016-03-03 15:22:33,365 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 5 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:33,366 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 10 missing tasks from ResultStage 4 (ShuffledRDD[9] at reduceByKey at Main.scala:97)
2016-03-03 15:22:33,367 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 4.0 with 10 tasks
2016-03-03 15:22:33,368 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_4 tasks to pool default
2016-03-03 15:22:33,370 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 4.0 (TID 15, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,372 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,373 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 2.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,374 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 3.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,375 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 3.0 in stage 4.0 (TID 18)
2016-03-03 15:22:33,376 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 2.0 in stage 4.0 (TID 17)
2016-03-03 15:22:33,375 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 0.0 in stage 4.0 (TID 15)
2016-03-03 15:22:33,375 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 1.0 in stage 4.0 (TID 16)
2016-03-03 15:22:33,396 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,396 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,396 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,396 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,399 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 7 ms
2016-03-03 15:22:33,400 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 9 ms
2016-03-03 15:22:33,399 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 15:22:33,399 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 8 ms
2016-03-03 15:22:33,424 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 2.0 in stage 4.0 (TID 17). 1333 bytes result sent to driver
2016-03-03 15:22:33,424 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 0.0 in stage 4.0 (TID 15). 1296 bytes result sent to driver
2016-03-03 15:22:33,424 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 3.0 in stage 4.0 (TID 18). 1277 bytes result sent to driver
2016-03-03 15:22:33,424 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 1.0 in stage 4.0 (TID 16). 1319 bytes result sent to driver
2016-03-03 15:22:33,426 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 4.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,430 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Running task 4.0 in stage 4.0 (TID 19)
2016-03-03 15:22:33,432 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 5.0 in stage 4.0 (TID 20, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,434 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,435 INFO  [Executor task launch worker-2][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:22:33,434 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 6.0 in stage 4.0 (TID 21, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,439 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 7.0 in stage 4.0 (TID 22, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,435 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Running task 5.0 in stage 4.0 (TID 20)
2016-03-03 15:22:33,441 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 7.0 in stage 4.0 (TID 22)
2016-03-03 15:22:33,441 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 3.0 in stage 4.0 (TID 18) in 67 ms on localhost (1/10)
2016-03-03 15:22:33,440 INFO  [Executor task launch worker-2][org.apache.spark.executor.Executor] Finished task 4.0 in stage 4.0 (TID 19). 1296 bytes result sent to driver
2016-03-03 15:22:33,449 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 2.0 in stage 4.0 (TID 17) in 77 ms on localhost (2/10)
2016-03-03 15:22:33,445 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,451 INFO  [Executor task launch worker-0][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 6 ms
2016-03-03 15:22:33,444 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 6.0 in stage 4.0 (TID 21)
2016-03-03 15:22:33,451 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,456 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 5 ms
2016-03-03 15:22:33,457 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,458 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:22:33,451 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 4.0 (TID 16) in 80 ms on localhost (3/10)
2016-03-03 15:22:33,456 INFO  [Executor task launch worker-0][org.apache.spark.executor.Executor] Finished task 5.0 in stage 4.0 (TID 20). 1275 bytes result sent to driver
2016-03-03 15:22:33,461 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 7.0 in stage 4.0 (TID 22). 1275 bytes result sent to driver
2016-03-03 15:22:33,464 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 6.0 in stage 4.0 (TID 21). 1300 bytes result sent to driver
2016-03-03 15:22:33,463 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 8.0 in stage 4.0 (TID 23, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,466 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 8.0 in stage 4.0 (TID 23)
2016-03-03 15:22:33,467 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 9.0 in stage 4.0 (TID 24, localhost, PROCESS_LOCAL, 1244 bytes)
2016-03-03 15:22:33,470 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 9.0 in stage 4.0 (TID 24)
2016-03-03 15:22:33,471 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 4.0 (TID 15) in 101 ms on localhost (4/10)
2016-03-03 15:22:33,475 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,474 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Getting 2 non-empty blocks out of 2 blocks
2016-03-03 15:22:33,477 INFO  [Executor task launch worker-1][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 3 ms
2016-03-03 15:22:33,476 INFO  [Executor task launch worker-3][org.apache.spark.storage.ShuffleBlockFetcherIterator] Started 0 remote fetches in 1 ms
2016-03-03 15:22:33,483 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 8.0 in stage 4.0 (TID 23). 1319 bytes result sent to driver
2016-03-03 15:22:33,476 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 6.0 in stage 4.0 (TID 21) in 41 ms on localhost (5/10)
2016-03-03 15:22:33,486 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 9.0 in stage 4.0 (TID 24). 1361 bytes result sent to driver
2016-03-03 15:22:33,487 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 7.0 in stage 4.0 (TID 22) in 48 ms on localhost (6/10)
2016-03-03 15:22:33,491 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 5.0 in stage 4.0 (TID 20) in 60 ms on localhost (7/10)
2016-03-03 15:22:33,493 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 4.0 in stage 4.0 (TID 19) in 68 ms on localhost (8/10)
2016-03-03 15:22:33,498 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 8.0 in stage 4.0 (TID 23) in 36 ms on localhost (9/10)
2016-03-03 15:22:33,499 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 9.0 in stage 4.0 (TID 24) in 33 ms on localhost (10/10)
2016-03-03 15:22:33,500 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 4.0, whose tasks have all completed, from pool default
2016-03-03 15:22:33,500 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 4 (collect at Main.scala:98) finished in 0.132 s
2016-03-03 15:22:33,502 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 3 finished: collect at Main.scala:98, took 0.385224 s
2016-03-03 15:22:33,529 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(230688) called with curMem=269896, maxMem=278302556
2016-03-03 15:22:33,530 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6 stored as values in memory (estimated size 225.3 KB, free 264.9 MB)
2016-03-03 15:22:33,571 INFO  [main][org.apache.spark.storage.MemoryStore] ensureFreeSpace(19750) called with curMem=500584, maxMem=278302556
2016-03-03 15:22:33,572 INFO  [main][org.apache.spark.storage.MemoryStore] Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 264.9 MB)
2016-03-03 15:22:33,575 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_6_piece0 in memory on localhost:12828 (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:22:33,577 INFO  [main][org.apache.spark.SparkContext] Created broadcast 6 from textFile at Main.scala:48
2016-03-03 15:22:33,595 INFO  [main][org.apache.hadoop.mapred.FileInputFormat] Total input paths to process : 1
2016-03-03 15:22:33,605 INFO  [main][org.apache.spark.SparkContext] Starting job: collect at Main.scala:48
2016-03-03 15:22:33,607 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 4 (collect at Main.scala:48) with 2 output partitions (allowLocal=false)
2016-03-03 15:22:33,608 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 5(collect at Main.scala:48)
2016-03-03 15:22:33,608 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:33,611 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:33,612 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48), which has no missing parents
2016-03-03 15:22:33,615 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(3248) called with curMem=520334, maxMem=278302556
2016-03-03 15:22:33,617 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 264.9 MB)
2016-03-03 15:22:33,620 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(1867) called with curMem=523582, maxMem=278302556
2016-03-03 15:22:33,621 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1867.0 B, free 264.9 MB)
2016-03-03 15:22:33,624 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_7_piece0 in memory on localhost:12828 (size: 1867.0 B, free: 265.4 MB)
2016-03-03 15:22:33,625 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 7 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:33,626 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at map at Main.scala:48)
2016-03-03 15:22:33,627 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 5.0 with 2 tasks
2016-03-03 15:22:33,629 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_5 tasks to pool default
2016-03-03 15:22:33,631 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 5.0 (TID 25, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:22:33,633 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.TaskSetManager] Starting task 1.0 in stage 5.0 (TID 26, localhost, PROCESS_LOCAL, 1535 bytes)
2016-03-03 15:22:33,634 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Running task 1.0 in stage 5.0 (TID 26)
2016-03-03 15:22:33,634 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 5.0 (TID 25)
2016-03-03 15:22:33,639 INFO  [Executor task launch worker-1][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:1544+1545
2016-03-03 15:22:33,641 INFO  [Executor task launch worker-3][org.apache.spark.rdd.HadoopRDD] Input split: file:/E:/eclipse/workspace_new/workspace_scala/sbdsample/server/data/attributes.txt:0+1544
2016-03-03 15:22:33,651 INFO  [Executor task launch worker-1][org.apache.spark.executor.Executor] Finished task 1.0 in stage 5.0 (TID 26). 4201 bytes result sent to driver
2016-03-03 15:22:33,652 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 5.0 (TID 25). 4197 bytes result sent to driver
2016-03-03 15:22:33,656 INFO  [task-result-getter-2][org.apache.spark.scheduler.TaskSetManager] Finished task 1.0 in stage 5.0 (TID 26) in 24 ms on localhost (1/2)
2016-03-03 15:22:33,658 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 5.0 (TID 25) in 27 ms on localhost (2/2)
2016-03-03 15:22:33,659 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 5 (collect at Main.scala:48) finished in 0.029 s
2016-03-03 15:22:33,661 INFO  [main][org.apache.spark.scheduler.DAGScheduler] Job 4 finished: collect at Main.scala:48, took 0.055086 s
2016-03-03 15:22:33,660 INFO  [task-result-getter-3][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 5.0, whose tasks have all completed, from pool default
2016-03-03 15:22:45,392 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_7_piece0 on localhost:12828 in memory (size: 1867.0 B, free: 265.4 MB)
2016-03-03 15:22:45,396 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_6_piece0 on localhost:12828 in memory (size: 19.3 KB, free: 265.4 MB)
2016-03-03 15:22:45,412 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.SparkContext] Starting job: collect at Streamer.scala:223
2016-03-03 15:22:45,413 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_5_piece0 on localhost:12828 in memory (size: 1362.0 B, free: 265.4 MB)
2016-03-03 15:22:45,414 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 5 (collect at Streamer.scala:223) with 1 output partitions (allowLocal=false)
2016-03-03 15:22:45,415 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 6(collect at Streamer.scala:223)
2016-03-03 15:22:45,417 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:45,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:45,421 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at Streamer.scala:223), which has no missing parents
2016-03-03 15:22:45,422 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_4_piece0 on localhost:12828 in memory (size: 2.2 KB, free: 265.4 MB)
2016-03-03 15:22:45,426 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5080) called with curMem=260062, maxMem=278302556
2016-03-03 15:22:45,426 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 265.2 MB)
2016-03-03 15:22:45,429 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(2665) called with curMem=265142, maxMem=278302556
2016-03-03 15:22:45,430 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KB, free 265.2 MB)
2016-03-03 15:22:45,433 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Added broadcast_8_piece0 in memory on localhost:12828 (size: 2.6 KB, free: 265.4 MB)
2016-03-03 15:22:45,434 INFO  [Spark Context Cleaner][org.apache.spark.ContextCleaner] Cleaned shuffle 0
2016-03-03 15:22:45,441 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 8 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:45,443 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at Streamer.scala:223)
2016-03-03 15:22:45,446 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 6.0 with 1 tasks
2016-03-03 15:22:45,448 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_6 tasks to pool default
2016-03-03 15:22:45,444 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.storage.BlockManagerInfo] Removed broadcast_3_piece0 on localhost:12828 in memory (size: 1819.0 B, free: 265.4 MB)
2016-03-03 15:22:45,463 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 6.0 (TID 27, localhost, PROCESS_LOCAL, 1250 bytes)
2016-03-03 15:22:45,465 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 6.0 (TID 27)
2016-03-03 15:22:45,691 INFO  [Executor task launch worker-3][org.apache.spark.sql.jdbc.JDBCRDD] closed connection
2016-03-03 15:22:45,698 INFO  [Executor task launch worker-3][org.apache.spark.sql.jdbc.JDBCRDD] closed connection
2016-03-03 15:22:45,702 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 6.0 (TID 27). 2113 bytes result sent to driver
2016-03-03 15:22:45,706 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 6 (collect at Streamer.scala:223) finished in 0.246 s
2016-03-03 15:22:45,706 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 6.0 (TID 27) in 243 ms on localhost (1/1)
2016-03-03 15:22:45,706 INFO  [ForkJoinPool-4-worker-7][org.apache.spark.scheduler.DAGScheduler] Job 5 finished: collect at Streamer.scala:223, took 0.293643 s
2016-03-03 15:22:45,707 INFO  [task-result-getter-1][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 6.0, whose tasks have all completed, from pool default
2016-03-03 15:22:45,761 INFO  [sbd-akka.actor.default-dispatcher-8][org.apache.spark.scheduler.DAGScheduler] Asked to cancel job group sender-$a
2016-03-03 15:22:58,618 INFO  [sbd-akka.actor.default-dispatcher-5][hive.ql.parse.ParseDriver] Parsing command: select * from user
2016-03-03 15:22:59,024 INFO  [sbd-akka.actor.default-dispatcher-5][hive.ql.parse.ParseDriver] Parse Completed
2016-03-03 15:22:59,350 INFO  [ForkJoinPool-4-worker-5][org.apache.spark.SparkContext] Starting job: hasNext at Streamer.scala:177
2016-03-03 15:22:59,351 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Got job 6 (hasNext at Streamer.scala:177) with 1 output partitions (allowLocal=false)
2016-03-03 15:22:59,353 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Final stage: ResultStage 7(hasNext at Streamer.scala:177)
2016-03-03 15:22:59,354 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Parents of final stage: List()
2016-03-03 15:22:59,358 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Missing parents: List()
2016-03-03 15:22:59,359 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting ResultStage 7 (MapPartitionsRDD[30] at mapPartitions at Streamer.scala:135), which has no missing parents
2016-03-03 15:22:59,364 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(11144) called with curMem=262852, maxMem=278302556
2016-03-03 15:22:59,365 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_9 stored as values in memory (estimated size 10.9 KB, free 265.1 MB)
2016-03-03 15:22:59,367 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] ensureFreeSpace(5193) called with curMem=273996, maxMem=278302556
2016-03-03 15:22:59,368 INFO  [dag-scheduler-event-loop][org.apache.spark.storage.MemoryStore] Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KB, free 265.1 MB)
2016-03-03 15:22:59,372 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added broadcast_9_piece0 in memory on localhost:12828 (size: 5.1 KB, free: 265.4 MB)
2016-03-03 15:22:59,373 INFO  [dag-scheduler-event-loop][org.apache.spark.SparkContext] Created broadcast 9 from broadcast at DAGScheduler.scala:874
2016-03-03 15:22:59,374 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at mapPartitions at Streamer.scala:135)
2016-03-03 15:22:59,374 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.TaskSchedulerImpl] Adding task set 7.0 with 1 tasks
2016-03-03 15:22:59,376 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.FairSchedulableBuilder] Added task set TaskSet_7 tasks to pool default
2016-03-03 15:22:59,377 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.scheduler.TaskSetManager] Starting task 0.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1250 bytes)
2016-03-03 15:22:59,378 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Running task 0.0 in stage 7.0 (TID 28)
2016-03-03 15:22:59,389 INFO  [Executor task launch worker-3][org.apache.spark.CacheManager] Partition rdd_25_0 not found, computing it
2016-03-03 15:22:59,432 INFO  [Executor task launch worker-3][org.apache.spark.sql.jdbc.JDBCRDD] closed connection
2016-03-03 15:22:59,437 INFO  [Executor task launch worker-3][org.apache.spark.storage.MemoryStore] ensureFreeSpace(928) called with curMem=279189, maxMem=278302556
2016-03-03 15:22:59,438 INFO  [Executor task launch worker-3][org.apache.spark.storage.MemoryStore] Block rdd_25_0 stored as values in memory (estimated size 928.0 B, free 265.1 MB)
2016-03-03 15:22:59,440 INFO  [sparkDriver-akka.actor.default-dispatcher-3][org.apache.spark.storage.BlockManagerInfo] Added rdd_25_0 in memory on localhost:12828 (size: 928.0 B, free: 265.4 MB)
2016-03-03 15:22:59,451 INFO  [Executor task launch worker-3][org.apache.spark.sql.jdbc.JDBCRDD] closed connection
2016-03-03 15:22:59,454 INFO  [Executor task launch worker-3][org.apache.spark.executor.Executor] Finished task 0.0 in stage 7.0 (TID 28). 2395 bytes result sent to driver
2016-03-03 15:22:59,457 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSetManager] Finished task 0.0 in stage 7.0 (TID 28) in 79 ms on localhost (1/1)
2016-03-03 15:22:59,457 INFO  [task-result-getter-0][org.apache.spark.scheduler.TaskSchedulerImpl] Removed TaskSet 7.0, whose tasks have all completed, from pool default
2016-03-03 15:22:59,472 INFO  [dag-scheduler-event-loop][org.apache.spark.scheduler.DAGScheduler] ResultStage 7 (hasNext at Streamer.scala:177) finished in 0.096 s
2016-03-03 15:22:59,473 INFO  [ForkJoinPool-4-worker-5][org.apache.spark.scheduler.DAGScheduler] Job 6 finished: hasNext at Streamer.scala:177, took 0.122274 s
2016-03-03 15:22:59,498 INFO  [sbd-akka.actor.default-dispatcher-2][org.apache.spark.scheduler.DAGScheduler] Asked to cancel job group streamer-$b
2016-03-03 15:23:11,205 INFO  [Thread-0][org.apache.spark.SparkContext] Invoking stop() from shutdown hook
2016-03-03 15:23:11,243 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2016-03-03 15:23:11,244 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2016-03-03 15:23:11,245 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/api,null}
2016-03-03 15:23:11,247 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/,null}
2016-03-03 15:23:11,248 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/static,null}
2016-03-03 15:23:11,251 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2016-03-03 15:23:11,252 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2016-03-03 15:23:11,253 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2016-03-03 15:23:11,255 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/executors,null}
2016-03-03 15:23:11,256 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2016-03-03 15:23:11,258 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/environment,null}
2016-03-03 15:23:11,258 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2016-03-03 15:23:11,259 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2016-03-03 15:23:11,260 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2016-03-03 15:23:11,262 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/storage,null}
2016-03-03 15:23:11,263 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2016-03-03 15:23:11,264 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2016-03-03 15:23:11,264 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2016-03-03 15:23:11,266 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2016-03-03 15:23:11,267 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2016-03-03 15:23:11,268 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/stages,null}
2016-03-03 15:23:11,269 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2016-03-03 15:23:11,270 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2016-03-03 15:23:11,270 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2016-03-03 15:23:11,272 INFO  [Thread-0][org.spark-project.jetty.server.handler.ContextHandler] stopped o.s.j.s.ServletContextHandler{/jobs,null}
2016-03-03 15:23:11,325 INFO  [Thread-0][org.apache.spark.ui.SparkUI] Stopped Spark web UI at http://169.254.236.187:4040
2016-03-03 15:23:11,328 INFO  [Thread-0][org.apache.spark.scheduler.DAGScheduler] Stopping DAGScheduler
2016-03-03 15:23:11,399 INFO  [sparkDriver-akka.actor.default-dispatcher-2][org.apache.spark.MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
2016-03-03 15:23:11,408 INFO  [Thread-0][org.apache.spark.util.Utils] path = C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b\blockmgr-5bd0945a-c894-49fd-82c9-07dc6ef6dfcf, already present as root for deletion.
2016-03-03 15:23:11,410 INFO  [Thread-0][org.apache.spark.storage.MemoryStore] MemoryStore cleared
2016-03-03 15:23:11,412 INFO  [Thread-0][org.apache.spark.storage.BlockManager] BlockManager stopped
2016-03-03 15:23:11,414 INFO  [Thread-0][org.apache.spark.storage.BlockManagerMaster] BlockManagerMaster stopped
2016-03-03 15:23:11,418 INFO  [sparkDriver-akka.actor.default-dispatcher-15][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
2016-03-03 15:23:11,425 INFO  [Thread-0][org.apache.spark.SparkContext] Successfully stopped SparkContext
2016-03-03 15:23:11,428 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Shutting down remote daemon.
2016-03-03 15:23:11,433 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remote daemon shut down; proceeding with flushing remote transports.
2016-03-03 15:23:11,428 INFO  [Thread-0][org.apache.spark.util.Utils] Shutdown hook called
2016-03-03 15:23:11,436 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f1fb38aa-8798-4a8d-9371-bc290fb3ae2b
2016-03-03 15:23:11,459 INFO  [sparkDriver-akka.actor.default-dispatcher-2][akka.remote.RemoteActorRefProvider$RemotingTerminator] Remoting shut down.
2016-03-03 15:23:11,481 INFO  [Thread-0][org.apache.spark.util.Utils] Deleting directory C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f023e723-2ae2-47b5-9cc3-0b808cd747c9
2016-03-03 15:23:11,647 ERROR [Thread-0][org.apache.spark.util.Utils] Exception while deleting Spark temp dir: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f023e723-2ae2-47b5-9cc3-0b808cd747c9
java.io.IOException: Failed to delete: C:\Users\IBM_ADMIN\AppData\Local\Temp\spark-f023e723-2ae2-47b5-9cc3-0b808cd747c9
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:963)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(Utils.scala:201)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anonfun$1.apply$mcV$sp(Utils.scala:201)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2308)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2278)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2278)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2278)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2260)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
